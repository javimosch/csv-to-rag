This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
docs/
  chunking-strategy-for-docs.md
  db-health.md
  generic-csv-format.md
  list-route.md
  md-to-csv.md
  openai-like-api.md
  pinecone-singleton.md
  process-flow.md
  ui-namespace-required.md
scripts/
  deno-ui/
    app/
      app.js
      backend.js
      files.js
      logs.js
      query.js
      sections.js
      tabs.js
    ui/
      backend.js
      fileList.js
      logs.js
      query.js
      styles.js
      template.js
      upload.js
    ui-old.js
    ui.js
  bundle.js
  clear-db.js
  db-health-repair-from-log.js
  db-health.js
  deno-ui.js
  md-to-csv.js
  mysql-schemas-to-csv.js
  scripts.md
src/
  cli/
    index.js
  config/
    mongodb.js
    openai.js
    pinecone.js
  middleware/
    error.middleware.js
    validation.middleware.js
  models/
    document.model.js
  routes/
    csv.routes.js
    log.routes.js
    query.routes.js
  services/
    csv.service.js
    embedding.service.js
    log.service.js
    query.service.js
  utils/
    logger.js
  server.js
.commiat
.env.example
.gitignore
compose.coolify.yml
compose.prod.yml
compose.yml
deno.lock
Dockerfile.backend
LICENSE
package.json
README.md
spec.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="docs/generic-csv-format.md">
# Generic CSV Format for Uploads

This document describes the required CSV format for uploads, as inferred from the code in `scripts/mysql-schemas-to-csv.js` and `scripts/md-to-csv.js`.

## Required Columns (Header)

For MySQL schemas:
```
code,metadata_small,metadata_big_1,metadata_big_2
```

For Markdown conversion:
```
code,metadata_small,metadata_big_1,metadata_big_2,metadata_big_3
```

- **code**: Unique identifier for the row (e.g., table name, content chunk ID).
- **metadata_small**: Short summary or snippet (JSON-encoded or plain text, often a preview or summary).
- **metadata_big_1**: Main content or DDL (JSON-encoded or plain text, e.g., full content, table DDL).
- **metadata_big_2**: Additional metadata (JSON-encoded or plain text, e.g., foreign keys, indexes, or empty).
- **metadata_big_3**: (Optional, for markdown) More metadata (often empty for MySQL schema CSVs).

## Example (MySQL Schema)
```
code,metadata_small,metadata_big_1,metadata_big_2
users,"{\"columns\":[\"id\",\"name\"]}","{\"ddl\":\"CREATE TABLE ...\"}","{\"foreignKeys\":[],\"indexes\":[]}" 
```

## Example (Markdown)
```
code,metadata_small,metadata_big_1,metadata_big_2,metadata_big_3
content_intro_12345678,"This is a summary...","Full content here...",,
```

## Format Rules
- The file must start with the correct header row.
- Fields must be separated by the chosen delimiter (default: `;` but often `,` for uploads).
- All required columns must be present, even if some values are empty.
- JSON fields must be stringified and escaped properly.
- No extra columns or missing columns are allowed.

## Validation
Files not matching this structure will be rejected by the upload UI/API.

_Last updated: 2025-04-25_
</file>

<file path="docs/pinecone-singleton.md">
# Pinecone Client Singleton Initialization

## Problem
Previously, the Pinecone client and index were being initialized every time `initPinecone()` was called. This caused repeated log entries and unnecessary client creation, especially during batch processing of CSV files.

## Solution
The Pinecone index is now cached at the module level in `src/config/pinecone.js`. The first call to `initPinecone()` creates and stores the index instance; subsequent calls return the same instance. This ensures only a single Pinecone client/index per process, improving performance and reducing log noise.

## Implementation
```js
// src/config/pinecone.js
let pineconeIndexInstance = null;
export async function initPinecone() {
  if (pineconeIndexInstance) return pineconeIndexInstance;
  const pinecone = new Pinecone({ apiKey: process.env.PINECONE_API_KEY });
  pineconeIndexInstance = pinecone.index(process.env.PINECONE_INDEX, process.env.PINECONE_HOST);
  return pineconeIndexInstance;
}
```

## Environment Variables
Ensure `.env` or `.env.example` contains:
- `PINECONE_API_KEY`
- `PINECONE_INDEX`
- `PINECONE_HOST`

## Impact
- No repeated client initialization logs
- More efficient resource usage
- Safe for concurrent batch operations

## Related files
- `src/config/pinecone.js`
- `.env.example`
</file>

<file path="docs/ui-namespace-required.md">
# UI Change: Namespace Field Required for File Upload

## Summary
The upload UI for embedding files now requires the "namespace" field to be filled. This is visually indicated with a red asterisk and helper text. If the field is left empty, the upload is prevented and an error message is shown.

## Implementation Details
- The namespace input now has a required attribute and a red asterisk in the label.
- Helper text and an error message placeholder are present.
- Validation is performed in JavaScript before submitting the upload. If empty, an error is shown and upload is blocked.
- Logging is added before upload and in the catch block for debugging (see user rules).

## Accessibility
- The required field is visually and programmatically marked (`aria-required`).

## Related Files
- `scripts/deno-ui/ui/upload.js`
- `scripts/deno-ui/app/files.js`

---

_Last updated: 2025-04-25_
</file>

<file path="scripts/db-health-repair-from-log.js">
import fs from 'fs';
import { fileURLToPath } from 'url';
import { dirname, join } from 'path';
import dotenv from 'dotenv';
import { Pinecone } from '@pinecone-database/pinecone';
import mongoose from 'mongoose';
import { Document } from '../src/models/document.model.js';
import { embedDocument } from '../src/services/embedding.service.js';
import { logger } from '../src/utils/logger.js';

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);

// Load environment variables
dotenv.config();

async function initServices() {
    try {
        // Connect to MongoDB
        await mongoose.connect(process.env.MONGODB_URI);
        logger.info('Connected to MongoDB');

        // Initialize Pinecone
        const pinecone = new Pinecone({
            apiKey: process.env.PINECONE_API_KEY
        });
        const index = pinecone.index(process.env.PINECONE_INDEX);
        logger.info('Connected to Pinecone');

        return index;
    } catch (error) {
        logger.error('Error initializing services:', error);
        throw error;
    }
}

async function closeServices() {
    await mongoose.disconnect();
    logger.info('Disconnected from MongoDB');
}

async function readRepairLog(logPath) {
    try {
        const content = fs.readFileSync(logPath, 'utf-8');
        const sections = content.split('=== Extra MongoDB Documents for ');
        
        const documents = [];
        for (const section of sections) {
            if (!section.trim()) continue;
            
            // Split into filename and content
            const [fileName, jsonContent] = section.split('===\n\n');
            if (!jsonContent) continue;

            // Split content into JSON objects
            const jsonObjects = jsonContent.split('\n}\n')
                .map(chunk => chunk.trim())
                .filter(chunk => chunk)
                .map(chunk => {
                    // Add closing brace if it's missing
                    const jsonStr = chunk.endsWith('}') ? chunk : chunk + '}';
                    try {
                        return JSON.parse(jsonStr);
                    } catch (e) {
                        logger.warn(`Failed to parse JSON object:`, {
                            chunk: jsonStr,
                            error: e.message
                        });
                        return null;
                    }
                })
                .filter(doc => doc !== null);
            
            documents.push(...jsonObjects);
        }
        
        logger.info(`Successfully parsed ${documents.length} documents from log`);
        if (documents.length > 0) {
            logger.debug('First document:', documents[0]);
        }
        
        return documents;
    } catch (error) {
        logger.error('Error reading repair log:', error);
        throw error;
    }
}

async function repairVectors(pineconeIndex, documents) {
    try {
        logger.info(`Repairing ${documents.length} vectors`);
        
        for (const doc of documents) {
            try {
                // Re-embed the document
                const embedding = await embedDocument(doc.code, doc.metadata_small);
                if (!embedding) {
                    logger.error(`Failed to generate embedding for ${doc.code}`);
                    continue;
                }

                // Prepare metadata
                const metadata = {
                    code: doc.code,
                    fileName: doc.fileName,
                    metadata_small: doc.metadata_small
                };

                // Upsert to Pinecone
                await pineconeIndex.upsert([{
                    id: doc.code,
                    values: embedding,
                    metadata
                }]);

                logger.info(`Successfully repaired vector for ${doc.code}`);

                // Add small delay to avoid rate limiting
                await new Promise(resolve => setTimeout(resolve, 100));
            } catch (error) {
                logger.error(`Failed to repair vector for ${doc.code}:`, {
                    error: {
                        name: error.name,
                        message: error.message,
                        stack: error.stack
                    },
                    document: doc
                });
            }
        }
    } catch (error) {
        logger.error('Error repairing vectors:', error);
        throw error;
    }
}

async function main() {
    try {
        // Initialize services
        const pineconeIndex = await initServices();

        // Read repair log
        const logPath = join(__dirname, '..', 'repair.log');
        const documents = await readRepairLog(logPath);
        logger.info(`Found ${documents.length} documents to repair`);

        // Repair vectors
        await repairVectors(pineconeIndex, documents);

        logger.info('Repair completed');
    } catch (error) {
        logger.error('Error in main:', error);
        process.exit(1);
    } finally {
        await closeServices();
    }
}

main();
</file>

<file path=".commiat">
{
  "format": "{type}: {msg}",
  "variables": {}
}
</file>

<file path="compose.coolify.yml">
version: '3.8'

services:
  backend:
    image: javimosch/csv-to-rag-backend:1.2
    working_dir: /app
    expose:
      - 3000
    volumes:
      - ./src:/app/src
      - ./scripts:/app/scripts
      - ./package.json:/app/package.json
      - ./package-lock.json:/app/package-lock.json
    restart: unless-stopped

  deno-ui:
    image: denoland/deno:alpine
    working_dir: /app
    expose:
      - 3001
    environment:
      - PORT=3001
      - INTERNAL_BACKEND=0
    volumes:
      - ./:/app
    command: ["run", "--allow-net", "--allow-env", "--allow-read", "--allow-run", "scripts/deno-ui.js"]
    restart: unless-stopped
    depends_on:
      - backend
</file>

<file path="docs/chunking-strategy-for-docs.md">
# Chunking Text to Vector

In this article I discuss the reason why chunking is needed and selecting a splitting strategy that matches the needs of the source knowledge base

## Text Splitting Strategy

While the concept of the chunking strategy is easy to understand, there are nuances that affect the accuracy of vector query responses, and there are various chunking strategies that can be used.

## Naïvely splitting with fixed chunk sizes

The simplest approach to creating text chunks is to split a source document into blocks with an appropriate maximum number of bytes in each chunk.  For example, we might use a text splitter that breaks a long document into chunks where each chunk is a fixed number of characters.

The embedding process would then create a single embedding vector for each chunk.  All the chunks would be stored in the vector database, and as users asked about Lincoln's speeches, the vector query would return the original text (and other metadata) for vectors matching the meaning of the user's prompt.

But there's a serious problem with this naïve strategy--do you see it? It's the way text was split into chunk 1 and 2.

Consider the following prompt:

💡
"Has Abraham Lincoln talked about Liberty in any of his speeaches?"
Would the vector search realize the Gettysburg speech does, in fact, include discussion of "Liberty"?  

Probably not. Vector 1 includes the word "Lib", and vector 2 includes the word "erty".  Neither of these are likely to be considered close to the concept "Liberty" in the original prompt.

## Context Overlap strategy 

Clearly, we need a better strategy. There are several approaches, but one that's easy to understand is using context overlap.

In this approach, rather than splitting chunks at exactly the fixed number of characters, we could instead use chunks that overlap from one to the next. In this way, a chunk still may begin or end with a partial word, but the chunk adjacent to it will contain the entire word.

For example, in the following document chunked with overlap, the word "testing" has been split in chunk 3 (to "ing"), but it is fully contained in chunk number 2.  In a search for "testing" vector number 3 wouldn't be returned, but vector 2 would be returned.

While this is a simple example, it illustrates the idea that we need to devise chunking strategies carefully.
</file>

<file path="docs/list-route.md">
# List Route Flow

## Overview
This document outlines the flow of the `/csv/list` route in the application, which is responsible for retrieving and displaying a summary of uploaded CSV files and their associated metadata.

## Process Flow Steps

1. **Route Definition**  
   The `/csv/list` route is defined in the `csv.routes.js` file. It handles GET requests to retrieve the list of uploaded files.

2. **Request Handling**  
   When a request is made to this route, the following actions occur:
   - The request is logged for debugging purposes.

3. **Database Aggregation**  
   The application performs an aggregation query on the MongoDB database to:
   - Group documents by `fileName`
   - Calculate statistics for each file:
     - Total number of rows
     - Last update timestamp
     - Sample metadata from the first row

4. **Response Preparation**  
   The response includes:
   - Total number of unique files
   - For each file:
     - File name
     - Number of rows processed
     - Last update timestamp
     - Sample metadata from one row

5. **Response Sending**  
   - The application sends the JSON response back to the client
   - The response status is set to 200 OK, indicating a successful retrieval of data

## How We Track Uploaded Files

- Each row from a CSV file is stored as a separate document in MongoDB
- Each document contains:
  - `fileName`: Identifies which file the row came from
  - `code`: Unique identifier for the row
  - `metadata_small`: Small metadata field
  - `metadata_big_1`, `metadata_big_2`, `metadata_big_3`: Larger metadata fields
  - `timestamp`: When the row was processed

- When listing files, we:
  1. Group all documents by `fileName`
  2. Calculate statistics (row count, last update)
  3. Include a sample of the data for preview purposes

## Example Response
```json
{
  "totalFiles": 2,
  "files": [
    {
      "fileName": "mysql-schemas.csv",
      "rowCount": 1000,
      "lastUpdated": "2024-12-30T21:36:31+01:00",
      "sampleMetadata": {
        "code": "example_code",
        "metadata_small": "example_metadata"
      }
    }
  ]
}
```

## Conclusion
This route provides a high-level overview of uploaded CSV files, showing meaningful statistics about each file's contents while maintaining the ability to track individual rows within the system.
</file>

<file path="docs/md-to-csv.md">
# Markdown to CSV Conversion Script

## Overview

The `md-to-csv.js` script is designed to convert any markdown file into the generic CSV format used by the `csv-to-rag` project. This allows markdown content to be integrated into the existing data processing pipeline, enabling seamless storage and retrieval through the system's database integrations.

## CSV Format

The CSV format expected by the project includes the following columns:

- `code`: Unique identifier for each entry.
- `metadata_small`: A brief metadata string extracted from the markdown.
- `metadata_big_1`: JSON-stringified data, potentially containing more detailed metadata or content.
- `metadata_big_2`: JSON-stringified data for additional content or metadata.
- `metadata_big_3`: JSON-stringified data for further content or metadata.

## Script Functionality

- **Input**: A markdown file.
- **Output**: A CSV file conforming to the project's format.

### Steps

1. **Parse Markdown**: Extract headings, paragraphs, lists, and other relevant content from the markdown file.
2. **Generate Unique Code**: Create a unique identifier for each entry based on the content or a hash of the file.
3. **Extract Metadata**: Identify key pieces of information to populate `metadata_small`. This could be the title or a summary.
4. **Structure Content**: Convert sections of the markdown into JSON-stringified objects for `metadata_big_1`, `metadata_big_2`, and `metadata_big_3`.
5. **Write CSV**: Output the processed data into a CSV file with the specified columns.

## Usage Example

```bash
node scripts/md-to-csv.js -i /path/to/markdown.md -o /path/to/output.csv
```

- `-i, --input <file>`: Path to the input markdown file.
- `-o, --output <file>`: Path to the output CSV file.

## Considerations

- Ensure markdown is well-structured to facilitate accurate parsing.
- Customize metadata extraction logic to suit specific project needs.
</file>

<file path="docs/process-flow.md">
# CSV Upload Process Flow

## Overview
This document outlines the process flow for handling CSV file uploads in the application, including data validation, processing, and storage in MongoDB and Pinecone.

## Process Flow Steps

1. **File Upload**  
   The user uploads a CSV file through the application interface.

2. **Initial Request Handling**  
   - The request headers and files are logged for debugging purposes.
   - The application extracts the original file name from the uploaded file.

3. **Asynchronous Processing Initiation**  
   - The application responds immediately with a success message and a job ID, indicating that processing has started in the background.
   - The processing includes:
     - **File Size**: Logged for monitoring.

4. **Cleanup Existing Data**  
   - The application checks for existing MongoDB documents and Pinecone vectors associated with the same `fileName`.
   - If existing data is found, it is deleted from both MongoDB and Pinecone to avoid duplicates.
   - The number of deleted records is logged.

5. **CSV Parsing**  
   - The uploaded CSV file is parsed to extract records.
   - Each record is validated, and the total number of records processed is logged.
   - If no valid records are found, an error is thrown, and processing is halted.

6. **Generate Embeddings**  
   - OpenAI embeddings are generated for the valid records.
   - The application logs the start of the embedding generation process.
   - If embedding generation fails, an error is thrown, and processing is halted.

7. **Save to MongoDB**  
   - The valid records are saved to MongoDB within a transaction to ensure data integrity.
   - The application logs the number of records saved.

8. **Save to Pinecone**  
   - The generated embeddings are saved to Pinecone, with the associated `fileName` included in the metadata.
   - The application logs the successful saving of embeddings.

9. **Transaction Commit**  
   - If all operations succeed, the transaction is committed, and the application logs the successful completion of the background processing.

10. **Error Handling**  
   - If any step fails during processing, the transaction is rolled back to maintain data integrity.
   - Relevant error messages are logged, and any necessary cleanup is performed.

## Conclusion
This process flow ensures that CSV uploads are handled efficiently, with proper validation, error handling, and data integrity across MongoDB and Pinecone.
</file>

<file path="scripts/deno-ui/app/tabs.js">
// All files under /deno-ui/app are compiled and combined into main.js (All functions are available globally)

// Tab switching functionality
function switchTab(tabName) {
    // scripts/deno-ui/app/tabs.js switchTab switching to tab
    console.log('tabs.js switchTab switching to tab', {data: {tabName}});
    
    // scripts/deno-ui/app/tabs.js switchTab switching tabs
    console.log('tabs.js switchTab switching tabs', {data: {tabName}});
    try {
        // Hide all tab contents
        const tabContents = document.querySelectorAll('.tab-content');
        tabContents.forEach(content => {
            content.classList.add('hidden');
        });

        // Remove DaisyUI active class from all tab buttons
        const tabButtons = document.querySelectorAll('.tab');
        tabButtons.forEach(button => {
            button.classList.remove('tab-active');
        });

        // Show the selected tab content
        const selectedTab = document.getElementById(`${tabName}-tab`);
        if (selectedTab) {
            selectedTab.classList.remove('hidden');
        }

        // Add DaisyUI active class to the clicked tab button
        const selectedButton = document.getElementById(`tab-${tabName}`);
        if (selectedButton) {
            selectedButton.classList.add('tab-active');
        }

        // Special handling for logs section if needed
        if (tabName === 'logs') {
            if (typeof startLogFetching === 'function') startLogFetching();
        } else {
            // Optionally stop log fetching
            // if (typeof stopLogFetching === 'function') stopLogFetching();
        }
    } catch (err) {
        // scripts/deno-ui/app/tabs.js switchTab try/catch
        console.log('tabs.js switchTab try/catch', {message: err?.message, stack: err?.stack});
    }
}

// Initialize tabs on page load
function initializeTabs() {
    // Default to the upload tab on page load
    switchTab('upload');
    
    // scripts/deno-ui/app/tabs.js initializeTabs tabs initialized
    console.log('tabs.js initializeTabs tabs initialized', {data: {}});
}
</file>

<file path="scripts/deno-ui/ui/logs.js">
export const template = `
<!-- Logs Section (Fixed) -->
<div id="logsSection" class="w-full px-4 md:px-8">
    <div class="max-w-7xl mx-auto">
        <div class="flex items-center justify-between py-2">
            <button onclick="toggleSection('logs')" 
                    class="text-lg font-semibold flex items-center">
                <span id="logsToggle" class="section-toggle">▼</span>
                <span class="ml-2">Logs</span>
            </button>
            <div class="flex items-center space-x-4">
                <div class="flex items-center">
                    <input type="checkbox" 
                           id="autoScroll" 
                           checked
                           class="mr-2">
                    <label for="autoScroll">Auto-scroll</label>
                </div>
                <button onclick="clearLogs()" 
                        class="text-red-600 hover:text-red-800">
                    Clear Logs
                </button>
            </div>
        </div>
        <div id="logsContent" class="section-content bg-white border border-gray-300 rounded overflow-y-auto font-mono text-sm p-4"
             style="height: 400px;"
             onscroll="handleLogsScroll(event)">
        </div>
    </div>
</div>`;
</file>

<file path="scripts/deno-ui/ui-old.js">
export const template = `<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CSV to RAG UI</title>
    <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
    <style>
        .section-content {
            transition: max-height 0.3s ease-out;
            overflow: hidden;
            max-height: 500px;
            overflow-y: auto;
        }
        .section-content.collapsed {
            max-height: 0 !important;
        }
        .section-toggle {
            transition: transform 0.3s ease;
        }
        .section-toggle.collapsed {
            transform: rotate(-90deg);
        }
        #logsSection {
            position: fixed;
            bottom: 0;
            left: 0;
            right: 0;
            background: white;
            z-index: 50;
            box-shadow: 0 -2px 10px rgba(0, 0, 0, 0.1);
            max-height: 500px;
        }
        body {
            padding-bottom: 550px; /* Space for fixed logs section + margin */
        }
        .main-content {
            max-width: 1200px;
            margin: 0 auto;
        }
    </style>
</head>
<body class="bg-gray-100">
    <div class="container mx-auto p-8 main-content">
        <h1 class="text-3xl text-blue-600 mb-8">CSV to RAG UI</h1>
        
        <div class="mb-4">
            <label for="baseUrl" class="block mb-1">Base URL:</label>
            <input type="text" id="baseUrl" value="${Deno.env.get('UI_BACKEND_URL')||"http://localhost:3000"}" 
                   placeholder="Enter base URL (e.g., http://localhost:3000)"
                   class="w-full p-2 border border-gray-300 rounded">
        </div>

        <!-- Backend Control Section -->
<div id="backendSection" class="mb-6 bg-white rounded shadow hidden">
    <button onclick="toggleSection('backend')" 
            class="w-full p-4 text-left font-semibold flex items-center justify-between">
        <span>Backend Control</span>
        <span id="backendSectionToggle" class="section-toggle">▼</span>
    </button>
    <div id="backendContent" class="section-content p-4">
        <div class="flex items-center justify-between">
            <button id="backendToggle" 
                    onclick="toggleBackend()" 
                    class="px-4 py-2 bg-green-600 text-white rounded hover:bg-green-500">
                Start Backend
            </button>
            <div id="backendStatus" class="text-gray-600">Backend is stopped</div>
        </div>
    </div>
</div>

        <!-- Upload Section -->
        <div class="mb-6 bg-white rounded shadow">
            <button onclick="toggleSection('upload')" 
                    class="w-full p-4 text-left font-semibold flex items-center justify-between">
                <span>Upload CSV</span>
                <span id="uploadToggle" class="section-toggle">▼</span>
            </button>
            <div id="uploadContent" class="section-content">
                <div class="p-4 space-y-3">
                    <div>
                        <input type="file" 
                               id="csvFile" 
                               accept=".csv"
                               class="block w-full text-sm text-gray-500
                                      file:mr-4 file:py-2 file:px-4
                                      file:rounded file:border-0
                                      file:text-sm file:font-semibold
                                      file:bg-blue-50 file:text-blue-700
                                      hover:file:bg-blue-100">
                    </div>
                    <div>
                        <label for="delimiter" class="block text-sm font-medium text-gray-700 mb-1">Delimiter:</label>
                        <select id="delimiter" 
                                class="block w-full p-2 border border-gray-300 rounded">
                            <option value=",">Comma (,)</option>
                            <option value=";">Semicolon (;)</option>
                            <option value="|">Pipe (|)</option>
                            <option value="\\t">Tab</option>
                        </select>
                    </div>
                    <button onclick="uploadFile()" 
                            class="w-full bg-blue-600 text-white p-2 rounded hover:bg-blue-500">
                        Upload
                    </button>
                    <div id="uploadProgress" class="hidden">
                        <div class="w-full bg-gray-200 rounded-full h-2.5">
                            <div id="uploadProgressBar" 
                                 class="bg-blue-600 h-2.5 rounded-full" 
                                 style="width: 0%"></div>
                        </div>
                        <div id="uploadStatus" class="text-sm text-gray-600 mt-1"></div>
                    </div>
                </div>
            </div>
        </div>

        <!-- File List Section -->
        <div class="mb-6 bg-white rounded shadow">
            <button onclick="toggleSection('list')" 
                    class="w-full p-4 text-left font-semibold flex items-center justify-between">
                <span>File List</span>
                <span id="listToggle" class="section-toggle">▼</span>
            </button>
            <div id="listContent" class="section-content">
                <div class="p-4">
                    <button onclick="listFiles()" 
                            class="w-full bg-blue-600 text-white p-2 rounded hover:bg-blue-500 mb-4">
                        Refresh File List
                    </button>
                    <div id="fileList" class="break-words"></div>
                </div>
            </div>
        </div>

        <!-- Query Section -->
        <div class="mb-6 bg-white rounded shadow">
            <button onclick="toggleSection('query')" 
                    class="w-full p-4 text-left font-semibold flex items-center justify-between">
                <span>Query</span>
                <span id="queryToggle" class="section-toggle">▼</span>
            </button>
            <div id="queryContent" class="section-content">
                <div class="p-4 space-y-3">
                    <div>
                        <textarea id="queryInput" 
                                  placeholder="Enter your query here..."
                                  class="w-full p-2 border border-gray-300 rounded"
                                  rows="3"></textarea>
                    </div>
                    <div class="flex space-x-2">
                        <button onclick="submitQuery()" 
                                class="flex-1 bg-blue-600 text-white p-2 rounded hover:bg-blue-500">
                            Submit Query
                        </button>
                        <button onclick="computeCurl()" 
                                class="bg-gray-600 text-white p-2 rounded hover:bg-gray-500 flex items-center">
                            <span>Compute cURL</span>
                        </button>
                    </div>
                    <div id="curlCommand" class="hidden">
                        <div class="bg-gray-100 p-3 rounded-lg text-sm font-mono relative">
                            <pre class="whitespace-pre-wrap break-all"></pre>
                            <button onclick="copyToClipboard(this.previousElementSibling.textContent)" 
                                    class="absolute top-2 right-2 bg-gray-700 text-white px-2 py-1 rounded text-xs hover:bg-gray-600">
                                Copy
                            </button>
                        </div>
                    </div>
                    <div id="queryResult" class="mt-4"></div>
                </div>
            </div>
        </div>

        <div id="error" class="text-red-600 mt-4 mb-4"></div>
    </div>

    <!-- Logs Section (Fixed) -->
    <div id="logsSection" class="w-full px-4 md:px-8">
        <div class="max-w-7xl mx-auto">
            <div class="flex items-center justify-between py-2">
                <button onclick="toggleSection('logs')" 
                        class="text-lg font-semibold flex items-center">
                    <span id="logsToggle" class="section-toggle">▼</span>
                    <span class="ml-2">Logs</span>
                </button>
                <div class="flex items-center space-x-4">
                    <div class="flex items-center">
                        <input type="checkbox" 
                               id="autoScroll" 
                               checked
                               class="mr-2">
                        <label for="autoScroll">Auto-scroll</label>
                    </div>
                    <button onclick="clearLogs()" 
                            class="text-red-600 hover:text-red-800">
                        Clear Logs
                    </button>
                </div>
            </div>
            <div id="logsContent" class="section-content bg-white border border-gray-300 rounded overflow-y-auto font-mono text-sm p-4"
                 style="height: 400px;"
                 onscroll="handleLogsScroll(event)">
            </div>
        </div>
    </div>

    <script>
        // Initialize on page load
        document.addEventListener('DOMContentLoaded', async () => {
            // Check if internal backend is available
            try {
                const response = await fetch('/api/backend/available');
                if (response.ok) {
                    const data = await response.json();
                    if (data.available) {
                        document.getElementById('backendSection').classList.remove('hidden');
                        checkBackendState();
                    }
                }
            } catch (error) {
                console.error('Error checking backend availability:', error);
            }
        });
    </script>
    <script src="/static/main.js"></script>
</body>
</html>`;
</file>

<file path="scripts/bundle.js">
#!/usr/bin/env -S deno run --allow-read --allow-write

import { join } from "https://deno.land/std/path/mod.ts";

async function bundleAppFiles() {
  const appDir = join(Deno.cwd(), "scripts", "deno-ui", "app");
  const bundleContent = [];

  // Add a timestamp for cache busting
  bundleContent.push(`// Generated bundle ${new Date().toISOString()}`);
  
  for await (const entry of Deno.readDir(appDir)) {
    if (entry.isFile && entry.name.endsWith(".js")) {
      const content = await Deno.readTextFile(join(appDir, entry.name));
      bundleContent.push(`// File: ${entry.name}\n${content}`);
    }
  }

  await Deno.writeTextFile(
    join(Deno.cwd(), "scripts", "deno-ui", "app-bundle.js"),
    bundleContent.join("\n\n")
  );
  console.log("Bundle created successfully!");
}

await bundleAppFiles();
</file>

<file path="scripts/mysql-schemas-to-csv.js">
#!/usr/bin/env node

import mysql from 'mysql2/promise';
import fs from 'fs/promises';
import path from 'path';
import { program } from 'commander';

/**
 * Example:
 * node scripts/mysql-schemas-to-csv.js -h localhost -u guest -p password -d geonline -t utilisateur
 */

program
  .option('-h, --host <host>', 'MySQL host', 'localhost')
  .option('-P, --port <port>', 'MySQL port', '3306')
  .option('-u, --user <user>', 'MySQL user')
  .option('-p, --password <password>', 'MySQL password')
  .option('-d, --database <database>', 'MySQL database name')
  .option('-o, --output <file>', 'Output CSV file', 'mysql-schemas.csv')
  .option('-t, --tables <tables>', 'Comma-separated list of tables to process')
  .option('-D, --delimiter <delimiter>', 'CSV delimiter', ';')
  .parse();

const options = program.opts();

async function getTableColumns(connection, tableName) {
  const [columns] = await connection.query(
    'SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_SCHEMA = ? AND TABLE_NAME = ?',
    [options.database, tableName]
  );
  return columns.map(col => col.COLUMN_NAME);
}

async function getTableDDL(connection, tableName) {
  const [result] = await connection.query('SHOW CREATE TABLE ??', [tableName]);
  return result[0]['Create Table'];
}

async function getForeignKeysAndIndexes(connection, tableName) {
  // Get foreign keys
  const [foreignKeys] = await connection.query(`
    SELECT 
      COLUMN_NAME,
      REFERENCED_TABLE_NAME,
      REFERENCED_COLUMN_NAME,
      CONSTRAINT_NAME
    FROM INFORMATION_SCHEMA.KEY_COLUMN_USAGE
    WHERE TABLE_SCHEMA = ? 
      AND TABLE_NAME = ?
      AND REFERENCED_TABLE_NAME IS NOT NULL`,
    [options.database, tableName]
  );

  // Get indexes
  const [indexes] = await connection.query(
    'SHOW INDEX FROM ??',
    [tableName]
  );

  return {
    foreignKeys: foreignKeys.map(fk => ({
      column: fk.COLUMN_NAME,
      referencedTable: fk.REFERENCED_TABLE_NAME,
      referencedColumn: fk.REFERENCED_COLUMN_NAME,
      constraintName: fk.CONSTRAINT_NAME
    })),
    indexes: indexes.map(idx => ({
      keyName: idx.Key_name,
      columnName: idx.Column_name,
      isUnique: idx.Non_unique === 0,
      type: idx.Index_type
    }))
  };
}

async function loadExistingData(filePath) {
  try {
    const content = await fs.readFile(filePath, 'utf-8');
    const rows = content.trim().split('\n');
    
    // Skip header row and empty rows
    const dataRows = rows.slice(1).filter(row => row.trim());
    
    const existingData = {};
    for (const row of dataRows) {
      const [code, metadata_small, metadata_big_1, metadata_big_2] = row.split(options.delimiter);
      if (code && code.trim()) {  // Only add if code exists and is not empty
        existingData[code.trim()] = {
          metadata_small: metadata_small || '',
          metadata_big_1: metadata_big_1 || '',
          metadata_big_2: metadata_big_2 || ''
        };
      }
    }
    return existingData;
  } catch (error) {
    if (error.code === 'ENOENT') {
      // File doesn't exist yet, return empty object
      return {};
    }
    console.error('Error loading existing data:', error);
    return {};
  }
}

async function processTable(connection, tableName) {
  try {
    const columns = await getTableColumns(connection, tableName);
    const ddl = await getTableDDL(connection, tableName);
    const fkAndIndexes = await getForeignKeysAndIndexes(connection, tableName);
    
    // Load existing data
    const existingData = await loadExistingData(options.output);
    
    // Process table schema
    const schemaCode = tableName;
    const schemaData = {
      metadata_small: JSON.stringify({ columns }),
      metadata_big_1: JSON.stringify({ ddl }),
      metadata_big_2: JSON.stringify(fkAndIndexes)
    };
    
    // Update or add schema information
    existingData[schemaCode] = schemaData;

    // Write all data back to CSV
    const csvContent = ['code,metadata_small,metadata_big_1,metadata_big_2'];
    
    // Add all data (both updated and existing)
    Object.entries(existingData).forEach(([code, data]) => {
      csvContent.push(`${code}${options.delimiter}${data.metadata_small}${options.delimiter}${data.metadata_big_1}${options.delimiter}${data.metadata_big_2}`);
    });

    await fs.writeFile(options.output, csvContent.join('\n'));
    console.log(`Updated schema information for table ${tableName}`);

  } catch (error) {
    console.error(`Error processing table ${tableName}:`, error);
    throw error;
  }
}

async function main() {
  if (!options.database) {
    console.error('Database name is required');
    process.exit(1);
  }

  const connection = await mysql.createConnection({
    host: options.host,
    port: parseInt(options.port),
    user: options.user,
    password: options.password,
    database: options.database
  });

  try {
    // Get all tables or use whitelist
    let tables;
    if (options.tables) {
      tables = options.tables.split(',').map(t => t.trim());
    } else {
      const [rows] = await connection.query(
        'SELECT TABLE_NAME FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = ?',
        [options.database]
      );
      tables = rows.map(row => row.TABLE_NAME);
    }

    // Process each table
    for (const table of tables) {
      console.log(`Processing table: ${table}`);
      await processTable(connection, table);
    }

    console.log(`Successfully wrote schema information to ${options.output}`);

  } catch (error) {
    console.error('Error:', error);
    process.exit(1);
  } finally {
    await connection.end();
  }
}

main();
</file>

<file path="scripts/scripts.md">
## Scripts Descriptions

This document describes the purpose of each JavaScript file in the `scripts/` directory.

### `bundle.js`
This script uses Deno to bundle all the JavaScript files located in the `scripts/deno-ui/app` directory into a single file named `app-bundle.js`. This is likely done for optimization purposes in a production environment.

### `clear-db.js`
This script is used to clear data from both the Pinecone vector database and the MongoDB database. It prompts the user for confirmation before proceeding with the data deletion.

### `deno-ui.js`
This script is a Deno application that serves a user interface. It handles starting and stopping a backend process (likely a Node.js server) and serves static files for the UI. It also manages user authentication for accessing the UI.

### `mysql-schemas-to-csv.js`
This script connects to a MySQL database and extracts schema information for specified tables. The extracted information includes column names, the Data Definition Language (DDL) for creating the tables, and details about foreign keys and indexes. This information is then written to a CSV file, which can be useful for documentation or analysis purposes.
</file>

<file path="src/cli/index.js">
#!/usr/bin/env node

import { program } from 'commander';
import inquirer from 'inquirer';
import axios from 'axios';
import fs from 'fs';
import path from 'path';
import os from 'os';

const CONFIG_DIR = path.join(os.homedir(), '.csv-to-rag');
const CONFIG_FILE = path.join(CONFIG_DIR, 'config.json');

const defaultConfig = {
  API_URL: 'http://localhost:3000',
  API_KEY: '' // Added default API_KEY
};

// Ensure config directory exists
if (!fs.existsSync(CONFIG_DIR)) {
  fs.mkdirSync(CONFIG_DIR, { recursive: true });
}

// Load config
function loadConfig() {
  try {
    if (fs.existsSync(CONFIG_FILE)) {
      return JSON.parse(fs.readFileSync(CONFIG_FILE, 'utf8'));
    }
    return defaultConfig;
  } catch (error) {
    console.log('Error loading config:', { message: error.message, stack: error.stack });
    return defaultConfig;
  }
}

// Save config
function saveConfig(config) {
  try {
    console.log('cli/index.js saveConfig', { config });
    fs.writeFileSync(CONFIG_FILE, JSON.stringify(config, null, 2));
  } catch (error) {
    console.log('Error saving config:', { message: error.message, stack: error.stack });
    throw error;
  }
}

program
  .name('ctr')
  .description('CSV to RAG CLI')
  .version('1.0.0');

program
  .command('config')
  .description('Configure the CLI')
  .action(async () => {
    try {
      console.log('cli/index.js config command', { currentConfig: loadConfig() });
      const answers = await inquirer.prompt([
        {
          type: 'input',
          name: 'API_URL',
          message: 'Enter API URL:',
          default: loadConfig().API_URL
        },
        {
          type: 'input',
          name: 'API_KEY',
          message: 'Enter API Key:',
          default: loadConfig().API_KEY
        }
      ]);
      saveConfig(answers);
      console.log('Configuration saved successfully');
    } catch (error) {
      console.log('Error in config command:', { message: error.message, stack: error.stack });
    }
  });

program
  .command('query <text>')
  .description('Query the RAG system')
  .option('--ctx', 'Only return context without LLM completion')
  .action(async (text, options) => {
    try {
      console.log('cli/index.js query command', { text, options });
      const config = loadConfig();
      const response = await axios.post(`${config.API_URL}/api/query${options.ctx ? '?onlyContext=true' : ''}`, {
        query: text
      }, {
        headers: {
          'Authorization': `Bearer ${config.API_KEY}`
        }
      });
      
      if (options.ctx) {
        console.log('\nAnswer:','\n', response.data);
      } else {
        // Display answer and sources as before
        console.log('\nAnswer:', response.data.answer);
        console.log('\nSources:');
        response.data.sources.forEach(source => {
          console.log(`- ${source.fileName} (${source.namespace})`);
          console.log(`  Context: ${source.context}`);
        });
      }
    } catch (error) {
      if (error.isAxiosError) {
        console.log('Error in query command:', { 
          message: error.message, 
          stack: error.stack,
          data: error.response?.data 
        });
      } else {
        console.log('Error in query command:', { message: error.message, stack: error.stack });
      }
    }
  });

program.parse();
</file>

<file path="src/config/mongodb.js">
import mongoose from 'mongoose';
import { logger } from '../utils/logger.js';

export async function setupMongoDB() {
  try {
    await mongoose.connect(process.env.MONGODB_URI);
    logger.info('MongoDB connected successfully');
  } catch (error) {
    logger.error('MongoDB connection error:', error);
    throw error;
  }
}
</file>

<file path="src/middleware/error.middleware.js">
import { logger } from '../utils/logger.js';

export function errorHandler(err, req, res, next) {
  logger.error('Error:', err);

  if (err.name === 'ValidationError') {
    return res.status(400).json({
      error: 'Validation Error',
      details: err.message
    });
  }

  if (err.name === 'MongoError' && err.code === 11000) {
    return res.status(409).json({
      error: 'Duplicate Entry',
      details: 'A record with this code already exists'
    });
  }

  res.status(500).json({
    error: 'Internal Server Error',
    message: process.env.NODE_ENV === 'production' ? 
      'An unexpected error occurred' : 
      err.message
  });
}
</file>

<file path="src/routes/log.routes.js">
import express from 'express';
import { logService } from '../services/log.service.js';

const router = express.Router();

router.get('/', (req, res) => {
    try {
        const timestamp = req.query.timestamp ? parseInt(req.query.timestamp) : undefined;
        const logs = logService.getLogs(timestamp);
        res.json({
            logs,
            count: logs.length,
            oldestTimestamp: logs.length > 0 ? Math.min(...logs.map(log => log.timestamp)) : null,
            newestTimestamp: logs.length > 0 ? Math.max(...logs.map(log => log.timestamp)) : null
        });
    } catch (error) {
        res.status(500).json({ 
            error: 'Failed to retrieve logs',
            message: error.message 
        });
    }
});

export const logRoutes = router;
</file>

<file path="src/services/log.service.js">
class LogEntry {
    constructor(message, level = 'info') {
        this.timestamp = Date.now();
        this.message = message;
        this.level = level;
    }
}

class LogService {
    constructor(retentionMs = 60000) { // Default retention: 1 minute
        this.logs = [];
        this.retentionMs = retentionMs;
        this.pruneInterval = setInterval(() => this.pruneOldLogs(), 10000); // Prune every 10 seconds
    }

    log(message, level = 'info') {
        const entry = new LogEntry(message, level);
        this.logs.push(entry);
        this.pruneOldLogs(); // Prune on each write to maintain memory
        return entry;
    }

    info(message) {
        return this.log(message, 'info');
    }

    error(message) {
        return this.log(message, 'error');
    }

    warn(message) {
        return this.log(message, 'warn');
    }

    debug(message) {
        return this.log(message, 'debug');
    }

    pruneOldLogs() {
        const cutoffTime = Date.now() - this.retentionMs;
        this.logs = this.logs.filter(log => log.timestamp >= cutoffTime);
    }

    getLogs(since = Date.now() - 10000) { // Default: last 10 seconds
        return this.logs.filter(log => log.timestamp >= since);
    }

    destroy() {
        if (this.pruneInterval) {
            clearInterval(this.pruneInterval);
        }
    }
}

// Create a singleton instance
const logService = new LogService();

// Handle cleanup on process exit
process.on('exit', () => {
    logService.destroy();
});

export { logService };
</file>

<file path="compose.prod.yml">
version: '3.8'

services:
  backend:
    image: javimosch/csv-to-rag-backend:1.3
    build:
      context: .
      dockerfile: Dockerfile.backend
    working_dir: /app
    ports:
      - 3000:3000
    volumes:
      - ./src:/app/src
      - ./scripts:/app/scripts
      - ./package.json:/app/package.json
      - ./package-lock.json:/app/package-lock.json
    restart: unless-stopped

  deno-ui:
    image: denoland/deno:alpine
    working_dir: /app
    ports:
      - 3001:3001
    environment:
      - PORT=3001
      - INTERNAL_BACKEND=0
    volumes:
      - ./:/app
    command: ["run", "--allow-net", "--allow-env", "--allow-read", "--allow-run", "scripts/deno-ui.js"]
    restart: unless-stopped
    depends_on:
      - backend
</file>

<file path="Dockerfile.backend">
FROM node:20.17.0-alpine

WORKDIR /app

COPY package*.json ./
RUN npm ci

COPY . .

CMD ["node", "src/server.js"]
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2024 Javier Leandro Arancibia

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="docs/db-health.md">
# Database Health Check and Repair Tool

The `db-health.js` script is a utility tool designed to monitor and maintain the health of MongoDB documents and Pinecone vectors. It provides functionality to check the synchronization between MongoDB and Pinecone, and repair any inconsistencies in metadata. This can also be used to upload/embed new files to both databases by using the repair mode.

## Features

- Health check of MongoDB documents and Pinecone vectors
- Repair of missing or inconsistent metadata
- Automatic fixing of orphaned vectors (vectors without fileName)
- Batch processing with configurable sizes
- Interactive or automated repair mode

## Usage

### Basic Health Check

To run a health check without making any changes:

```bash
node scripts/db-health.js
```

This will:
1. Check MongoDB document counts by fileName
2. Check Pinecone vector counts and metadata
3. Report any discrepancies between the two databases
4. Identify orphaned vectors (vectors without fileName metadata)

### Repair Mode

To repair metadata using a CSV file:

```bash
node scripts/db-health.js --repair --file=./path/to/file.csv [--auto]
```

Options:
- `--repair`: Enables repair mode
- `--file`: Path to the CSV file containing the records to repair
- `--auto`: (Optional) Run in automatic mode without prompting for confirmation

The repair process will:
1. Extract fileName from the CSV path
2. Update MongoDB documents with correct fileName and metadata
3. Fix orphaned vectors by setting their fileName metadata
4. Update all vector metadata to ensure consistency

Note: This mode can be used to upload/embed new files to both databases even if no data is present in mongo/pinecone.

## Health Check Output

The script provides detailed health information:

```
=== MongoDB Health ===
Total Documents: XXX

Documents by File:
  file1.csv          XXX documents
  file2.csv          XXX documents

=== Pinecone Health ===
Total Vectors:          XXX
Vectors with fileName:  XXX
Orphaned Vectors:       XXX

Vectors by File:
  file1.csv          XXX vectors
  file2.csv          XXX vectors

=== Discrepancies ===
  file1.csv:
    MongoDB:       XXX documents
    Pinecone:      XXX vectors
    Difference:    XXX missing in Pinecone
```

## Data Model

### MongoDB Document Schema
```javascript
{
  code: String,          // Required, indexed
  fileName: String,      // Required, indexed
  metadata_small: String // Required
  source: String        // Optional
}
```

### Pinecone Vector Metadata
```javascript
{
  code: String,
  fileName: String,
  metadata_small: String
}
```

## Error Handling

The script includes comprehensive error handling:
- Connection errors for both MongoDB and Pinecone
- CSV parsing errors
- Invalid vector dimensions
- Failed metadata updates
- Batch processing errors

## Implementation Details

### Batch Processing
- Records are processed in batches to optimize performance
- Default batch size is calculated to ensure max 5 parallel batches
- Each batch can be manually confirmed in interactive mode

### Orphan Vector Repair
- Identifies vectors missing fileName metadata
- Uses Pinecone's update API to fix metadata without re-embedding
- Maintains vector values while updating metadata

### Metadata Synchronization
- Ensures consistency between MongoDB documents and Pinecone vectors
- Updates both databases in a single operation
- Preserves existing vector embeddings when possible

## Best Practices

1. Always run a health check before repairs
2. Use `--auto` mode for large datasets
3. Keep CSV files organized by domain/purpose
4. Monitor logs for any failed updates
5. Verify health check after repairs

## Troubleshooting

Common issues and solutions:

1. **MongoDB Connection Errors**
   - Verify MongoDB connection string
   - Check database permissions

2. **Pinecone API Errors**
   - Verify API key and environment
   - Check rate limits

3. **Missing Files**
   - Ensure CSV file paths are correct
   - Check file permissions

4. **Orphaned Vectors**
   - Run repair with correct CSV file
   - Check fileName extraction logic

## Logging

The script uses a structured logging system with different levels:
- INFO: General progress and statistics
- DEBUG: Detailed operation information
- ERROR: Failed operations and exceptions

Logs include:
- Operation timestamps
- Batch processing progress
- Vector metadata updates
- Error details with stack traces
</file>

<file path="scripts/deno-ui/app/backend.js">
// All files under /deno-ui/app are compiled and combined into main.js (All functions are available globally)

// Backend state
let backendRunning = false;

// Initialize backend settings from localStorage
window.addEventListener('load', () => {
    const savedApiKey = localStorage.getItem('apiKey');
    const savedBaseUrl = localStorage.getItem('baseUrl');
    
    if (savedApiKey) {
        document.getElementById('apiKey').value = savedApiKey;
    }
    
    if (savedBaseUrl) {
        document.getElementById('baseUrl').value = savedBaseUrl;
    }
});

function handleApiKeyChange(value) {
    localStorage.setItem('apiKey', value);
}

function handleBaseUrlChange(value) {
    localStorage.setItem('baseUrl', value);
}

function getAuthHeaders() {
    const apiKey = document.getElementById('apiKey').value;
    return {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json'
    };
}

async function checkBackendState() {
    const button = document.getElementById('backendToggle');
    const status = document.getElementById('backendStatus');
    
    try {
        const response = await fetch('/api/backend/state', {
            headers: getAuthHeaders()
        });
        if (!response.ok) {
            throw new Error(`HTTP error! status: ${response.status}`);
        }
        
        const data = await response.json();
        
        if (data.status === 'running') {
            backendRunning = true;
            button.textContent = 'Stop Backend';
            button.classList.remove('bg-green-600', 'hover:bg-green-500');
            button.classList.add('bg-red-600', 'hover:bg-red-500');
            status.innerHTML = `<span class="text-green-600">${data.message || 'Backend is running'}</span>`;
        } else {
            backendRunning = false;
            button.textContent = 'Start Backend';
            button.classList.remove('bg-red-600', 'hover:bg-red-500');
            button.classList.add('bg-green-600', 'hover:bg-green-500');
            status.innerHTML = `<span class="text-gray-600">${data.message || 'Backend is stopped'}</span>`;
        }
    } catch (error) {
        console.error('Error checking backend state:', error);
        status.innerHTML = `<span class="text-red-600">Error checking backend state</span>`;
    }
}

async function toggleBackend() {
    const button = document.getElementById('backendToggle');
    const status = document.getElementById('backendStatus');
    
    try {
        button.disabled = true;
        const action = backendRunning ? 'stop' : 'start';
        status.innerHTML = `<span class="text-blue-600">${action === 'start' ? 'Starting' : 'Stopping'} backend...</span>`;
        
        const response = await fetch(`/api/backend/${action}`, {
            headers: getAuthHeaders()
        });
        if (!response.ok) {
            throw new Error(`HTTP error! status: ${response.status}`);
        }
        
        const data = await response.json();
        
        if (data.status === 'success') {
            backendRunning = !backendRunning;
            button.textContent = backendRunning ? 'Stop Backend' : 'Start Backend';
            button.classList.toggle('bg-green-600');
            button.classList.toggle('bg-red-600');
            button.classList.toggle('hover:bg-green-500');
            button.classList.toggle('hover:bg-red-500');
            status.innerHTML = `<span class="text-green-600">${data.message || (backendRunning ? 'Backend is running' : 'Backend is stopped')}</span>`;
        } else {
            status.innerHTML = `<span class="text-red-600">${data.message || 'Failed to toggle backend'}</span>`;
        }
    } catch (error) {
        console.error('Error:', error);
        status.innerHTML = `<span class="text-red-600">Error: ${error.message}</span>`;
    } finally {
        button.disabled = false;
    }
}
</file>

<file path="scripts/deno-ui/app/sections.js">
// All files under /deno-ui/app are compiled and combined into main.js (All functions are available globally)

// Section visibility state
const sectionStates = {
    backend: true,
    upload: true,
    list: true,
    query: true,
    logs: true
};

// Toggle section visibility
function toggleSection(sectionName) {
    const content = document.getElementById(`${sectionName}Content`);
    const toggle = document.getElementById(`${sectionName}Toggle`);
    
    if (!content || !toggle) return;
    
    sectionStates[sectionName] = !content.classList.contains('collapsed');
    content.classList.toggle('collapsed');
    toggle.classList.toggle('collapsed');
    
    // Special handling for logs section
    if (sectionName === 'logs') {
        if (sectionStates[sectionName]) {
            stopLogFetching();
        } else {
            startLogFetching(); 
        }
    }
}

// Initialize section states
function initializeSections() {
    Object.keys(sectionStates).forEach(section => {
        const content = document.getElementById(`${section}Content`);
        const toggle = document.getElementById(`${section}Toggle`);
        
        // Skip if elements don't exist
        if (!content || !toggle) return;
        
        if (sectionStates[section]) {
            content.classList.remove('collapsed');
            toggle.classList.remove('collapsed');
        } else {
            content.classList.add('collapsed');
            toggle.classList.add('collapsed');
        }
    });
}
</file>

<file path="scripts/deno-ui/ui/fileList.js">
export const template = `
<!-- File List Section -->
<div class="bg-white rounded shadow p-4">
        <div class="p-4">
            <button onclick="listFiles()" 
                    class="w-full bg-blue-600 text-white p-2 rounded hover:bg-blue-500 mb-4">
                Refresh File List
            </button>
            <div id="fileList" class="break-words"></div>
        </div>
</div>
`;
</file>

<file path="spec.md">
Create a Node.js application that implements a RAG (Retrieval-Augmented Generation) system with the following specifications:

Data Processing:
Accept CSV files with columns: code (unique ID), metadata_small, metadata_big_1, metadata_big_2, metadata_big_3
Process and validate CSV files to ensure proper formatting and data integrity
Handle JSON-stringified data in metadata_big columns
Database Integration:
Store document embeddings in Pinecone vector database
Store complete records in MongoDB with the following schema:
code: String (unique identifier)
metadata_small: String
metadata_big_1: JSON
metadata_big_2: JSON
metadata_big_3: JSON
timestamp: Date
API Endpoints:
POST /csv/upload: Upload and process new CSV files
GET /csv/list: Retrieve list of processed CSV files
PUT /csv/update/:id: Update existing CSV data
DELETE /csv/delete/:id: Remove CSV data
POST /query: Perform similarity search and LLM interaction
Environment Configuration:
OPENAI_API_KEY: OpenAI API authentication
OPENAI_MODEL: OpenAI API authentication
MONGODB_URI: MongoDB connection string
PINECONE_API_KEY: Pinecone API key
PINECONE_ENVIRONMENT: Pinecone environment
PINECONE_INDEX: Pinecone index name
LLM_SYSTEM_PROMPT: Customizable system prompt for LLM interactions
Error Handling:
Implement comprehensive error handling for API requests
Validate input data formats
Handle service interruptions gracefully
Security:
Use CORS *
The application should follow RESTful principles and include appropriate logging and monitoring capabilities
</file>

<file path="docs/openai-like-api.md">
# OpenAI-like Completion API

This document describes the design of a new API endpoint `/api/completion` that provides LLM completion functionality, mirroring the request/response format of the OpenAI completion API.

## Endpoint

`POST /api/completion`

## Authentication

This endpoint requires Bearer token authentication. You must include a valid API key in the `Authorization` header.

```
Authorization: Bearer YOUR_BACKEND_API_KEY
```

## Request Body

The request body should be a JSON object with the following parameters:

```json
{
  "model": "string", // Required. The model to use for completion (e.g., "openai/gpt-3.5-turbo-instruct", "google/gemini-pro"). This parameter is currently ignored; the system uses the configured LLM models.
  "prompt": "string", // Required. The prompt to generate completions for.
  "max_tokens": "integer", // Optional. The maximum number of tokens to generate.
  "temperature": "number"  // Optional. Sampling temperature, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
}
```

## Response Body (Success - 200 OK)

The response body is a JSON object with the following structure:

```json
{
  "id": "string",       // Unique ID for the completion.
  "object": "text_completion",
  "created": "integer",  // Timestamp of the completion creation (Unix timestamp).
  "model": "string",      // The model used for completion.
  "choices": [
    {
      "text": "string",      // The generated text.
      "index": "integer",    // The index of the choice in the list of choices.
      "logprobs": "object | null", // Not implemented yet.
      "finish_reason": "string" // The reason the completion finished, e.g., "stop" if the API hit a stop sequence, "length" if `max_tokens` was reached.
    }
  ],
  "usage": {
    "prompt_tokens": "integer",    // The number of tokens in your prompt.
    "completion_tokens": "integer", // The number of tokens in the completion.
    "total_tokens": "integer"      // The total number of tokens used in the request (prompt + completion).
  }
}
```

## Error Responses

The API will return standard HTTP error codes for various issues:

*   **400 Bad Request:**  Indicates missing or invalid parameters in the request body. The response body will include a JSON object with an `error` message.
*   **401 Unauthorized:**  Indicates that the request lacks valid authentication credentials. Ensure you are providing a valid Bearer token.
*   **500 Internal Server Error:**  Indicates an error on the server. The response body will include a JSON object with an `error` message.

## Usage Notes

- The `model` parameter should correspond to a valid model identifier supported by the underlying LLM service (e.g., OpenAI compatible API provider).
- The `prompt` parameter is the text that the model will use to generate the completion.
- If `max_tokens` is not specified, the model will use a default maximum.
- The `temperature` parameter controls the randomness of the output.
- The LLM completion uses the same logic as the current query route behind the scenes. Therefore, the `model` parameter in the request is ignored, and the system will use the configured LLM models as defined in the environment variables (e.g., `OPENAI_MODEL`, `OPENAI_MODEL_FALLBACK`).

## Example Request

```bash
curl -X POST \\
     -H "Content-Type: application/json" \\
     -H "Authorization: Bearer YOUR_BACKEND_API_KEY" \\
     -d '{
       "model": "openai/gpt-3.5-turbo-instruct",
       "prompt": "Write a tagline for an ice cream shop",
       "max_tokens": 50,
       "temperature": 0.7
     }' \\
     http://localhost:3000/api/completion
```

## Example Success Response

```json
{
  "id": "cmpl-xxxxxxxxxxxxx",
  "object": "text_completion",
  "created": 1699999999,
  "model": "openai/gpt-3.5-turbo-instruct",
  "choices": [
    {
      "text": "\\n\\nScoops of happiness in every bite!",
      "index": 0,
      "logprobs": null,
      "finish_reason": "length"
    }
  ],
  "usage": {
    "prompt_tokens": 7,
    "completion_tokens": 10,
    "total_tokens": 17
  }
}
</file>

<file path="scripts/deno-ui/app/logs.js">
// All files under /deno-ui/app are compiled and combined into main.js (All functions are available globally)

// Log management
let logs = [];
let lastFetchTime = Date.now() - 10000; // default to now - 10 seconds
let logsVisible = true;
let logFetchInterval = null;
const MAX_LOGS = 1000;
const LOG_FETCH_INTERVAL = 5000;

function getAuthHeaders() {
    const apiKey = document.getElementById('apiKey').value;
    return {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json'
    };
}

// Load logs from localStorage
function loadLogsFromStorage() {
    try {
        const storedLogs = localStorage.getItem('logs');
        if (storedLogs) {
            logs = JSON.parse(storedLogs);
        }
    } catch (error) {
        console.error('Error loading logs from storage:', error);
    }
}

// Save logs to localStorage
function saveLogsToStorage() {
    try {
        localStorage.setItem('logs', JSON.stringify(logs));
    } catch (error) {
        console.error('Error saving logs to storage:', error);
        if (error.message.includes('QuotaExceededError')) {
            console.warn('Local storage quota exceeded. Pruning logs.');
            logs = [];
            // Attempt to save again after pruning
            try {
                localStorage.setItem('logs', JSON.stringify(logs));
                console.log('Logs saved successfully after pruning.');
            } catch (retryError) {
                console.error('Error saving logs after pruning:', retryError);
            }
        }
    }
}

// Format log entry
function formatLogEntry(log) {
    const date = new Date(log.timestamp);
    const hours = date.getHours().toString().padStart(2, '0');
    const minutes = date.getMinutes().toString().padStart(2, '0');
    const seconds = date.getSeconds().toString().padStart(2, '0');
    const millis = date.getMilliseconds().toString().padStart(3, '0');
    const time = `${hours}:${minutes}:${seconds}.${millis}`;

    let levelClass = 'text-gray-800';
    let icon = '';
    
    switch (log.level.toLowerCase()) {
        case 'error':
            levelClass = 'text-red-600';
            icon = '❌';
            break;
        case 'warn':
            levelClass = 'text-yellow-600';
            icon = '⚠️';
            break;
        case 'info':
            levelClass = 'text-blue-600';
            icon = 'ℹ️';
            break;
        case 'debug':
            levelClass = 'text-gray-600';
            icon = '🔍';
            break;
    }
    
    return `<div class="log-entry mb-1">
        <span class="text-gray-500">[${time}]</span>
        <span class="${levelClass}">${icon} [${log.level.toUpperCase()}]</span>
        <span class="ml-2">${log.message}</span>
    </div>`;
}

// Check if user is at bottom of logs
function isUserAtBottom() {
    const logsContent = document.getElementById('logsContent');
    if (!logsContent) return false;
    return Math.abs(logsContent.scrollHeight - logsContent.scrollTop - logsContent.clientHeight) < 10;
}

// Display logs
function displayLogs(append = false) {
    const logsContent = document.getElementById('logsContent');
    if (!logsContent) return;
    
    const wasAtBottom = isUserAtBottom();
    const shouldAutoScroll = document.getElementById('autoScroll')?.checked && wasAtBottom;
    
    if (append) {
        const fragment = document.createDocumentFragment();
        const tempDiv = document.createElement('div');
        logs.slice(-10).forEach(log => {
            tempDiv.innerHTML = formatLogEntry(log);
            fragment.appendChild(tempDiv.firstChild);
        });
        logsContent.appendChild(fragment);
    } else {
        logsContent.innerHTML = logs.map(formatLogEntry).join('');
    }
    
    if (shouldAutoScroll) {
        logsContent.scrollTop = logsContent.scrollHeight;
    }
}

// Fetch new logs
async function fetchLogs() {
    const baseUrl = document.getElementById('baseUrl').value;
    
    try {
        const response = await fetch(`${baseUrl}/api/logs?timestamp=${lastFetchTime}`, {
            headers: getAuthHeaders()
        });
        
        if (!response.ok) {
            throw new Error(`HTTP error! status: ${response.status}`);
        }
        
        const data = await response.json();
        
        if (data.logs && data.logs.length > 0) {
            // Update lastFetchTime to the newest timestamp from the response
            lastFetchTime = data.newestTimestamp;
            
            // Add new logs and remove duplicates
            const newLogs = data.logs.filter(newLog => 
                !logs.some(existingLog => 
                    existingLog.timestamp === newLog.timestamp && 
                    existingLog.message === newLog.message
                )
            );

            if (newLogs.length > 0) {
                logs = [...newLogs, ...logs]
                    .sort((a, b) => b.timestamp - a.timestamp) // Sort by timestamp descending
                    .slice(0, MAX_LOGS); // Keep only the latest MAX_LOGS entries
                
                saveLogsToStorage();
                displayLogs();
            }
        }
    } catch (error) {
        console.error('Error fetching logs:', error);
    }
}

// Handle infinite scrolling and track user scroll
function handleLogsScroll(event) {
    // Save the user's scroll position for later
    localStorage.setItem('logsScrollPosition', event.target.scrollTop);
}

// Clear logs
function clearLogs() {
    logs = [];
    saveLogsToStorage();
    displayLogs();
}

// Start log fetching
function startLogFetching() {
    if (!logFetchInterval) {
        fetchLogs(); // Initial fetch
        logFetchInterval = setInterval(fetchLogs, LOG_FETCH_INTERVAL);
    }
}

// Stop log fetching
function stopLogFetching() {
    if (logFetchInterval) {
        clearInterval(logFetchInterval);
        logFetchInterval = null;
    }
}

function appendLog(message, level = 'info') {
    const log = {
        timestamp: Date.now(),
        level,
        message
    };
    logs = [log, ...logs].slice(0, MAX_LOGS);
    saveLogsToStorage();
    displayLogs();
}

startLogFetching()
</file>

<file path="scripts/deno-ui/ui/backend.js">
export const template = ()=>`
<!-- Backend Control Section -->
<div id="backendSection" class="bg-white rounded shadow p-4 hidden">
        <div class="space-y-4">
            <div>
                <label for="baseUrl" class="block text-sm font-medium text-gray-700 mb-1">Base URL:</label>
                <input type="text" id="baseUrl" value="${Deno.env.get('UI_BACKEND_URL')||"http://localhost:3000"}" 
                       placeholder="Enter base URL (e.g., http://localhost:3000)"
                       class="w-full p-2 border border-gray-300 rounded"
                       onchange="handleBaseUrlChange(this.value)">
            </div>
            <div>
                <label for="apiKey" class="block text-sm font-medium text-gray-700 mb-1">API Key:</label>
                <input type="password" id="apiKey" 
                       placeholder="Enter your API key"
                       class="w-full p-2 border border-gray-300 rounded"
                       value="${Deno.env.get('BACKEND_API_KEY')}"
                       onchange="handleApiKeyChange(this.value)">
            </div>
            <div class="flex items-center justify-between">
                <button id="backendToggle" 
                        onclick="toggleBackend()" 
                        class="px-4 py-2 bg-green-600 text-white rounded hover:bg-green-500">
                    Start Backend
                </button>
                <div id="backendStatus" class="text-gray-600">Backend is stopped</div>
            </div>
        </div>
</div>
`;
</file>

<file path="scripts/deno-ui/ui/query.js">
export const template = `
<!-- Query Section -->
<div class="bg-white rounded shadow p-4">
        <div class="p-4 space-y-3">
            <div class="mb-4">
                <label for="queryNamespace" class="block text-sm font-medium text-gray-700 mb-1">Namespace: <span class="text-red-600" title="Required">*</span></label>
                <div class="flex items-center">
                    <select id="queryNamespace" 
                            class="flex-grow p-2 border border-red-300 rounded" 
                            required>
                        <option value="" disabled selected>Select a namespace</option>
                        <!-- Namespaces will be populated dynamically -->
                    </select>
                    <button onclick="loadNamespaces()" 
                            class="ml-2 bg-gray-200 hover:bg-gray-300 text-gray-700 p-2 rounded">
                        <span>↻</span>
                    </button>
                </div>
                <div id="namespaceError" class="text-red-600 text-sm mt-1 hidden">Namespace is required</div>
            </div>
            <div>
                <textarea id="queryInput" 
                          placeholder="Enter your query here..."
                          class="w-full p-2 border border-gray-300 rounded"
                          rows="3"></textarea>
            </div>
            <div class="flex space-x-2">
                <button onclick="submitQuery()" 
                        class="flex-1 bg-blue-600 text-white p-2 rounded hover:bg-blue-500">
                    Submit Query
                </button>
                <button onclick="computeCurl()" 
                        class="bg-gray-600 text-white p-2 rounded hover:bg-gray-500 flex items-center">
                    <span>Compute cURL</span>
                </button>
            </div>
            <div id="curlCommand" class="hidden">
                <div class="bg-gray-100 p-3 rounded-lg text-sm font-mono relative">
                    <pre class="whitespace-pre-wrap break-all"></pre>
                    <button onclick="copyToClipboard(this.previousElementSibling.textContent)" 
                            class="absolute top-2 right-2 bg-gray-700 text-white px-2 py-1 rounded text-xs hover:bg-gray-600">
                        Copy
                    </button>
                </div>
            </div>
            <div id="queryResult" class="mt-4"></div>
        </div>
</div>`;
</file>

<file path="scripts/deno-ui/ui/styles.js">
export const styles = `
    /* DaisyUI tabs: .tab and .tab-active are used for tab navigation */
    .tab-content {
        display: block;
        transition: opacity 0.3s ease;
    }
    .tab-content.hidden {
        display: none;
        opacity: 0;
    }
    
    /* Legacy accordion styles (keeping for backward compatibility) */
    .section-content {
        transition: max-height 0.3s ease-out;
        overflow: hidden;
        max-height: 500px;
        overflow-y: auto;
    }
    .section-content.collapsed {
        max-height: 0 !important;
    }
    .section-toggle {
        transition: transform 0.3s ease;
    }
    .section-toggle.collapsed {
        transform: rotate(-90deg);
    }
    #logsSection {
        position: fixed;
        bottom: 0;
        left: 0;
        right: 0;
        background: white;
        z-index: 50;
        box-shadow: 0 -2px 10px rgba(0, 0, 0, 0.1);
        max-height: 500px;
    }
    body {
        padding-bottom: 550px; /* Space for fixed logs section + margin */
    }
    .main-content {
        max-width: 1200px;
        margin: 0 auto;
    }
`;
</file>

<file path="scripts/md-to-csv.js">
#!/usr/bin/env node

import fs from 'fs/promises';
import crypto from 'crypto';
import { Command } from 'commander';

const program = new Command();

program
  .option('-i, --input <file>', 'Input markdown file')
  .option('-o, --output <file>', 'Output CSV file', 'output.csv')
  .option('-d, --delimiter <delimiter>', 'CSV delimiter', ';')
  .option('-m, --media-dir <dir>', 'Directory to save media files', 'images')
  .option('--overlap-percentage <percentage>', 'Percentage of context overlap between chunks', '20')
  .option('--max-metadata-size <size>', 'Maximum metadata size in tokens', '4860')
  .option('--encode-metadata', 'Encode metadata fields in base64')
  .option('--format-codefetch', 'Convert markdown representing a codebase')
  .parse();

const options = program.opts();

if (!options.input) {
  console.error('Error: Input file is required');
  process.exit(1);
}

// Validate overlap percentage
const overlapPercentage = parseInt(options.overlapPercentage);
if (isNaN(overlapPercentage) || overlapPercentage < 0 || overlapPercentage > 100) {
  console.error('Error: Overlap percentage must be between 0 and 100');
  process.exit(1);
}

// Validate metadata size
const maxMetadataSize = parseInt(options.maxMetadataSize);
if (isNaN(maxMetadataSize) || maxMetadataSize <= 0) {
  console.error('Error: Maximum metadata size must be a positive number');
  process.exit(1);
}

function generateHumanReadableId(type, content) {
  const sanitized = content
    .toLowerCase()
    .replace(/[^a-z0-9\s]/g, ' ')
    .replace(/\s+/g, '_')
    .substring(0, 30);

  const hash = crypto
    .createHash('md5')
    .update(type + content)
    .digest('hex')
    .substring(0, 8);

  return `${type}_${sanitized}_${hash}`;
}

function parseTreeView(treeContent) {
  const fileMap = new Map();
  const lines = treeContent.split('\n');
  let currentPath = [];
  let currentDepth = 0;

  lines.forEach(line => {
    const trimmed = line.trim();
    if (!trimmed) return;

    const depth = line.search(/\S/);
    
    if (trimmed.startsWith('├──') || trimmed.startsWith('└──')) {
      const fileName = trimmed.replace(/^[├└]──\s*/, '').trim();
      const fullPath = [...currentPath, fileName].join('/');
      fileMap.set(fileName, fullPath);
    } else if (trimmed.startsWith('│')) {
      // Continue in current directory
    } else {
      // Directory change
      if (depth > currentDepth) {
        // Going deeper
        const dirName = trimmed;
        currentPath.push(dirName);
        currentDepth = depth;
      } else if (depth < currentDepth) {
        // Going up
        const levelsUp = (currentDepth - depth) / 4;
        currentPath = currentPath.slice(0, -levelsUp);
        currentDepth = depth;
      }
    }
  });

  return fileMap;
}

function parseMarkdownContent(content) {
  const lines = content.split('\n');
  const chunks = [];
  let currentChunk = '';
  let currentTitle = '';
  let inParagraph = false;
  let overlapSize = 0;
  let previousChunkEnd = '';

  // Calculate overlap size based on max metadata size
  const maxChunkSize = options.maxMetadataSize * 4; // Approximate token to character ratio
  overlapSize = Math.floor(maxChunkSize * (options.overlapPercentage / 100));

  for (const line of lines) {
    const trimmed = line.trim();
    
    // Skip empty lines
    if (!trimmed) continue;

    // Handle titles
    if (trimmed.startsWith('#')) {
      // Save current chunk if exists
      if (currentChunk) {
        const chunkContent = currentChunk.trim();
        chunks.push({
          title: currentTitle,
          content: previousChunkEnd + chunkContent
        });
        
        // Store the end of this chunk for overlap
        previousChunkEnd = chunkContent.slice(-overlapSize);
        currentChunk = '';
      }
      currentTitle = trimmed.replace(/^#+\s*/, '');
      continue;
    }

    // Handle paragraphs
    if (!inParagraph) {
      currentChunk += `${currentTitle ? `${currentTitle}\n` : ''}`;
      inParagraph = true;
    }
    currentChunk += `${line}\n`;

    // Check if we need to split the chunk
    if (currentChunk.length >= maxChunkSize) {
      const chunkContent = currentChunk.trim();
      chunks.push({
        title: currentTitle,
        content: previousChunkEnd + chunkContent
      });
      
      // Store the end of this chunk for overlap
      previousChunkEnd = chunkContent.slice(-overlapSize);
      currentChunk = '';
      inParagraph = false;
    }
  }

  // Add final chunk
  if (currentChunk) {
    chunks.push({
      title: currentTitle,
      content: previousChunkEnd + currentChunk.trim()
    });
  }

  return chunks;
}

async function parseMarkdownFile(inputPath) {
  const content = await fs.readFile(inputPath, 'utf-8');
  
  if (options.formatCodefetch) {
    const lines = content.split('\n');
    const csvRows = [];
    let state = 'start';
    let treeViewLines = [];
    let currentFilePath = '';
    let codeLines = [];
    let fileMap = new Map();

    for (const line of lines) {
      if (state === 'start' && line.startsWith('```')) {
        state = 'tree-view';
        continue;
      }

      if (state === 'tree-view') {
        if (line.startsWith('```')) {
          state = 'file-path';
          fileMap = parseTreeView(treeViewLines.join('\n'));
          continue;
        }
        treeViewLines.push(line);
        continue;
      }

      if (state === 'file-path') {
        if (!line.trim()) continue;
        
        if (line.startsWith('```')) {
          state = 'code-block';
          continue;
        }
        
        currentFilePath = line.trim();
        continue;
      }

      if (state === 'code-block') {
        if (line.startsWith('```')) {
          // End of code block
          const fullPath = fileMap.get(currentFilePath) || currentFilePath;
          const codeContent = codeLines.join('\n');
          
          const row = {
            code: generateHumanReadableId('file', fullPath),
            metadata_small: codeContent.split('\n').slice(0, 3).join('\n'),
            metadata_big_1: codeContent,
            metadata_big_2: currentFilePath,
            metadata_big_3: fullPath
          };
          
          // Skip row if only code field has value
          if (row.metadata_small || row.metadata_big_1 || row.metadata_big_2 || row.metadata_big_3) {
            csvRows.push(row);
          }
          
          // Reset for next file
          currentFilePath = '';
          codeLines = [];
          state = 'file-path';
          continue;
        }
        
        codeLines.push(line);
      }
    }

    return csvRows;
  }

  // Handle regular markdown content
  const chunks = parseMarkdownContent(content);
  return chunks
    .map(chunk => ({
      code: generateHumanReadableId('content', chunk.title || chunk.content),
      metadata_small: chunk.content.substring(0, options.maxMetadataSize),
      metadata_big_1: chunk.content,
      metadata_big_2: '',
      metadata_big_3: ''
    }))
    .filter(row => row.metadata_small || row.metadata_big_1 || row.metadata_big_2 || row.metadata_big_3);
}

async function writeCSV(data, outputPath, delimiter) {
  const headers = ['code', 'metadata_small', 'metadata_big_1', 'metadata_big_2', 'metadata_big_3'];
  
  const csvContent = [
    headers.join(delimiter),
    ...data.map(row => {
      return headers.map(header => {
        let value = row[header] || '';
        if (options.encodeMetadata && header.startsWith('metadata_')) {
          value = Buffer.from(value.toString()).toString('base64');
        }
        
        // Escape line breaks and quotes
        value = value.toString()
          .replace(/\n/g, '\\n')
          .replace(/"/g, '""');
          
        return value.includes(delimiter) || value.includes('"') ? `"${value}"` : value;
        return value.includes(delimiter) ? `"${value}"` : value;
      }).join(delimiter);
    })
  ].join('\n');

  await fs.writeFile(outputPath, csvContent, 'utf-8');
  console.log(`CSV file written to ${outputPath}`);
}

async function main() {
  try {
    const data = await parseMarkdownFile(options.input);
    await writeCSV(data, options.output, options.delimiter);
  } catch (error) {
    console.error('Error:', error);
    process.exit(1);
  }
}

main();
</file>

<file path="src/utils/logger.js">
import { logService } from '../services/log.service.js';
import fs from 'fs';
import { join } from 'path';

const __dirname = process.cwd();

console.log('Completion Log File:', join(__dirname, 'completions.log'));

class Logger {
    formatMessage(message, meta = {}) {
        const formattedMeta = Object.keys(meta).length > 0 
            ? ` | ${JSON.stringify(meta)}`
            : '';
        return `${message}${formattedMeta}`;
    }

    info(message, meta = {}) {
        const formattedMessage = this.formatMessage(message, meta);
        console.info(formattedMessage);
        logService.info(formattedMessage);
    }

    error(message, meta = {}) {
        const formattedMessage = this.formatMessage(message, meta);
        console.error(formattedMessage);
        logService.error(formattedMessage);
    }

    warn(message, meta = {}) {
        const formattedMessage = this.formatMessage(message, meta);
        console.warn(formattedMessage);
        logService.warn(formattedMessage);
    }

    debug(message, meta = {}) {
        const formattedMessage = this.formatMessage(message, meta);
        console.debug(formattedMessage);
        logService.debug(formattedMessage);
    }

    createCompletionLogger() {
        const logFile = join(__dirname, 'completions.log');
        const logStream = fs.createWriteStream(logFile, { flags: 'a' });

        return {
            info: (message, meta = {}) => {
                const formattedMessage = this.formatMessage(message, meta);
                logStream.write(`${formattedMessage}\n`);
            },
            error: (message, meta = {}) => {
                const formattedMessage = this.formatMessage(message, meta);
                logStream.write(`${formattedMessage}\n`);
            }
        };
    }
}

export const logger = new Logger();
export const completionLogger = logger.createCompletionLogger();
</file>

<file path="compose.yml">
version: '3.8'

services:
  backend:
    image: node:20.17.0-alpine
    working_dir: /app
    ports:
      - "3000:3000"
    volumes:
      - ./:/app
    command: ["npm","run", "dev:backend"]
    restart: unless-stopped

  deno-ui:
    image: denoland/deno:alpine
    working_dir: /app
    ports:
      - "3001:3001"
    environment:
      - PORT=3001
      - INTERNAL_BACKEND=0
    volumes:
      - ./:/app
    command: ["run", "--allow-net", "--allow-env", "--allow-read", "--allow-run","--watch", "scripts/deno-ui.js"]
    restart: unless-stopped
    depends_on:
      - backend
</file>

<file path="scripts/deno-ui/app/query.js">
// All files under /deno-ui/app are compiled and combined into main.js (All functions are available globally)

function getAuthHeaders() {
    const apiKey = document.getElementById('apiKey').value;
    return {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json'
    };
}

/**
 * Load available namespaces from the backend
 */
async function loadNamespaces() {
    // scripts/deno-ui/app/query.js loadNamespaces Loading available namespaces
    console.log('query.js loadNamespaces Loading available namespaces', {data: {}});
    
    const baseUrl = document.getElementById('baseUrl').value;
    const namespaceSelect = document.getElementById('queryNamespace');
    const error = document.getElementById('error');
    
    try {
        error.textContent = '';
        
        // Show loading state
        const currentSelection = namespaceSelect.value;
        namespaceSelect.innerHTML = '<option value="" disabled selected>Loading namespaces...</option>';
        
        const response = await fetch(`${baseUrl}/api/csv/namespaces`, {
            method: 'GET',
            headers: getAuthHeaders()
        });
        
        if (response.ok) {
            const data = await response.json();
            
            // Clear and repopulate the select
            namespaceSelect.innerHTML = '<option value="" disabled selected>Select a namespace</option>';
            
            if (data.namespaces && data.namespaces.length > 0) {
                data.namespaces.forEach(namespace => {
                    const option = document.createElement('option');
                    option.value = namespace;
                    option.textContent = namespace;
                    namespaceSelect.appendChild(option);
                });
                
                // Restore previous selection if it exists
                if (currentSelection && data.namespaces.includes(currentSelection)) {
                    namespaceSelect.value = currentSelection;
                }
                
                appendLog(`Loaded ${data.namespaces.length} namespaces`);
            } else {
                appendLog('No namespaces found. Upload a CSV file first.', 'warning');
            }
        } else {
            const errorText = await response.text();
            appendLog(`Failed to load namespaces: ${errorText}`, 'error');
            namespaceSelect.innerHTML = '<option value="" disabled selected>Failed to load namespaces</option>';
        }
    } catch (error) {
        // scripts/deno-ui/app/query.js loadNamespaces Error loading namespaces
        console.log('query.js loadNamespaces Error loading namespaces', {data: {message: error.message, stack: error.stack}});
        
        appendLog(`Error loading namespaces: ${error.message}`, 'error');
        namespaceSelect.innerHTML = '<option value="" disabled selected>Error loading namespaces</option>';
    }
}

async function submitQuery() {
    // scripts/deno-ui/app/query.js submitQuery Submitting query
    console.log('query.js submitQuery Submitting query', {data: {}});
    
    const queryInput = document.getElementById('queryInput');
    const namespaceSelect = document.getElementById('queryNamespace');
    const namespaceError = document.getElementById('namespaceError');
    const query = queryInput.value.trim();
    const namespace = namespaceSelect.value;
    
    // Validate inputs
    let isValid = true;
    
    if (!query) {
        appendLog('Please enter a query', 'error');
        isValid = false;
    }
    
    if (!namespace) {
        namespaceError.classList.remove('hidden');
        appendLog('Please select a namespace', 'error');
        isValid = false;
    } else {
        namespaceError.classList.add('hidden');
    }
    
    if (!isValid) {
        return;
    }
    
    const baseUrl = document.getElementById('baseUrl').value;
    const queryResult = document.getElementById('queryResult');
    const error = document.getElementById('error');
    
    try {
        error.textContent = '';
        queryResult.innerHTML = 'Processing query...';
        
        // scripts/deno-ui/app/query.js submitQuery Sending query to backend
        console.log('query.js submitQuery Sending query to backend', {data: {query, namespace}});
        
        const response = await fetch(`${baseUrl}/api/query`, {
            method: 'POST',
            headers: getAuthHeaders(),
            body: JSON.stringify({ query, namespace })
        });
        
        if (response.ok) {
            const result = await response.json();
            displayQueryResult(result);
            appendLog('Query executed successfully');
        } else {
            const errorText = await response.text();
            appendLog(`Query failed: ${errorText}`, 'error');
        }
    } catch (error) {
        // scripts/deno-ui/app/query.js submitQuery Error submitting query
        console.log('query.js submitQuery Error submitting query', {data: {message: error.message, stack: error.stack}});
        
        appendLog(`Query failed: ${error.message}`, 'error');
    }
}

function computeCurl() {
    // scripts/deno-ui/app/query.js computeCurl Computing curl command
    console.log('query.js computeCurl Computing curl command', {data: {}});
    
    const queryInput = document.getElementById('queryInput');
    const namespaceSelect = document.getElementById('queryNamespace');
    const namespaceError = document.getElementById('namespaceError');
    const query = queryInput.value.trim();
    const namespace = namespaceSelect.value;
    const baseUrl = document.getElementById('baseUrl').value;
    const apiKey = document.getElementById('apiKey').value;
    
    // Validate inputs
    let isValid = true;
    
    if (!query) {
        appendLog('Please enter a query first', 'error');
        isValid = false;
    }
    
    if (!namespace) {
        namespaceError.classList.remove('hidden');
        appendLog('Please select a namespace', 'error');
        isValid = false;
    } else {
        namespaceError.classList.add('hidden');
    }
    
    if (!isValid) {
        return;
    }
    
    // Use string concatenation instead of template literals to avoid escaping issues
    const curlCommand = 'curl -X POST "' + baseUrl + '/api/query" \\\n' +
        '     -H "Content-Type: application/json" \\\n' +
        '     -H "Authorization: Bearer ' + apiKey + '" \\\n' +
        '     -d \'{"query": "' + query.replace(/'/g, "\\'") + '", "namespace": "' + namespace + '"}\'';
    
    const curlDiv = document.getElementById('curlCommand');
    curlDiv.classList.remove('hidden');
    curlDiv.querySelector('pre').textContent = curlCommand;
    
    // scripts/deno-ui/app/query.js computeCurl Curl command generated
    console.log('query.js computeCurl Curl command generated', {data: {namespace}});
}

function displayQueryResult(result) {
    const queryResult = document.getElementById('queryResult');
    queryResult.innerHTML = '';
    
    if (Array.isArray(result)) {
        const table = document.createElement('table');
        table.className = 'min-w-full divide-y divide-gray-200 border border-gray-200 rounded-lg overflow-hidden';
        
        // Create table header
        const thead = document.createElement('thead');
        thead.className = 'bg-gray-100';
        const headerRow = document.createElement('tr');
        Object.keys(result[0] || {}).forEach(key => {
            const th = document.createElement('th');
            th.className = 'px-6 py-4 text-left text-sm font-semibold text-gray-600 uppercase tracking-wider border-b';
            th.textContent = key;
            headerRow.appendChild(th);
        });
        thead.appendChild(headerRow);
        table.appendChild(thead);
        
        // Create table body
        const tbody = document.createElement('tbody');
        result.forEach((row, i) => {
            const tr = document.createElement('tr');
            tr.className = i % 2 === 0 ? 'bg-white hover:bg-gray-50' : 'bg-gray-50 hover:bg-gray-100';
            
            Object.values(row).forEach(value => {
                const td = document.createElement('td');
                td.className = 'px-6 py-4 text-sm text-gray-900 border-t';
                
                // Format different types of values
                if (typeof value === 'object' && value !== null) {
                    td.innerHTML = `<pre class="whitespace-pre-wrap break-words">${JSON.stringify(value, null, 2)}</pre>`;
                } else if (typeof value === 'boolean') {
                    td.innerHTML = `<span class="px-2 py-1 rounded-full text-xs ${value ? 'bg-green-100 text-green-800' : 'bg-red-100 text-red-800'}">${value}</span>`;
                } else if (value === null || value === undefined) {
                    td.innerHTML = '<span class="text-gray-400 italic">null</span>';
                } else {
                    td.textContent = value;
                }
                
                tr.appendChild(td);
            });
            
            tbody.appendChild(tr);
        });
        table.appendChild(tbody);
        
        queryResult.appendChild(table);
    } else {
        // Create a formatted display for non-array results
        const resultContainer = document.createElement('div');
        resultContainer.className = 'bg-white rounded-lg border border-gray-200 p-4';
        
        if (typeof result === 'object' && result !== null) {
            const pre = document.createElement('pre');
            pre.className = 'whitespace-pre-wrap break-words text-sm text-gray-800 font-mono';
            pre.textContent = JSON.stringify(result, null, 2);
            resultContainer.appendChild(pre);
        } else {
            resultContainer.textContent = result?.toString() || 'No result';
        }
        
        queryResult.appendChild(resultContainer);
    }
}

function copyToClipboard(text) {
    navigator.clipboard.writeText(text).then(() => {
        const button = document.querySelector('#curlCommand button');
        const originalText = button.textContent;
        button.textContent = 'Copied!';
        setTimeout(() => {
            button.textContent = originalText;
        }, 2000);
    }).catch(err => {
        console.error('Failed to copy text:', err);
        appendLog('Failed to copy to clipboard', 'error');
    });
}
</file>

<file path="scripts/deno-ui/ui/template.js">
export const template = `<!DOCTYPE html>
<html lang="en" data-theme="cmyk">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CSV to RAG UI</title>
    <!-- DaisyUI and Tailwind CSS via CDN -->
    <link href="https://cdn.jsdelivr.net/npm/daisyui@5" rel="stylesheet" type="text/css" />
    <script src="https://cdn.jsdelivr.net/npm/@tailwindcss/browser@4"></script>
    <link href="https://cdn.jsdelivr.net/npm/daisyui@5/themes.css" rel="stylesheet" type="text/css" />
   
    
</head>
<body class="bg-base-200 min-h-screen">
    <div class="w-full max-w-full px-8 py-8 main-content">
        <h1 class="text-5xl text-primary mb-8 text-center">CSV to RAG UI</h1>
        
        <!-- Tab Navigation -->
        <div class="tabs tabs-boxed mb-6">
            <a id="tab-upload" class="tab tab-active" onclick="switchTab('upload')">Upload</a>
            <a id="tab-files" class="tab" onclick="switchTab('files')">Files</a>
            <a id="tab-query" class="tab" onclick="switchTab('query')">Query</a>
            <!-- <a id="tab-backend" class="tab ml-auto" onclick="switchTab('backend')">Backend</a> -->
        </div>
        
        <!-- Tab Content Container -->
        <div class="tab-content-container">
            <div id="error" class="text-red-600 mt-4 mb-4"></div>
            <!-- Modular tab content sections -->
            <div id="upload-tab" class="tab-content">[UPLOAD CONTENT]</div>
            <div id="files-tab" class="tab-content hidden">[FILES CONTENT]</div>
            <div id="query-tab" class="tab-content hidden">[QUERY CONTENT]</div>
            <div id="backend-tab" class="tab-content hidden">[BACKEND CONTENT]</div>
        </div>
    </div>

    <script src="/static/main.js"></script>
    <script>
    // Initialize on page load
    document.addEventListener('DOMContentLoaded', async () => {
        try {
            // Check if internal backend is available
            const response = await fetch('/api/backend/available');
            if (response.ok) {
                const data = await response.json();
                if (data.available) {
                    document.getElementById('backendSection').classList.remove('hidden');
                    setTimeout(checkBackendState, 2000);
                }
            }
        } catch (error) {
            console.error('Error checking backend availability:', error);
            appendLog('Error checking backend availability: ' + error.message, 'error');
        }
    });
    </script>
</body>
</html>`;
</file>

<file path="scripts/deno-ui/ui/upload.js">
export const template = `
<!-- Upload Section -->
<div class="bg-white rounded shadow p-4">
        <div class="p-4 space-y-3">
            <div>
                <input type="file" 
                       id="csvFile" 
                       accept=".csv"
                       class="block w-full text-sm text-gray-500
                              file:mr-4 file:py-2 file:px-4
                              file:rounded file:border-0
                              file:text-sm file:font-semibold
                              file:bg-blue-50 file:text-blue-700
                              hover:file:bg-blue-100">
            </div>
            <div>
                <label for="delimiter" class="block text-sm font-medium text-gray-700 mb-1">Delimiter:</label>
                <select id="delimiter" 
                        class="block w-full p-2 border border-gray-300 rounded">
                    <option value=",">Comma (,)</option>
                    <option value=";">Semicolon (;)</option>
                    <option value="|">Pipe (|)</option>
                    <option value="\\t">Tab</option>
                </select>
            </div>
            <div>
                <label for="namespace" class="block text-sm font-medium text-gray-700 mb-1">Namespace: <span class="text-red-600" title="Required">*</span></label>
<input type="text" id="namespace" name="namespace" placeholder="Enter namespace (required)" required class="block w-full p-2 border border-red-300 rounded" aria-required="true" />
<div id="namespaceError" class="text-red-600 text-sm mt-1" style="display:none"></div>
<small class="text-gray-500">Namespace is required. Choose a unique name for your data grouping.</small>
            </div>
            <button onclick="uploadFile()" 
                    class="w-full bg-blue-600 text-white p-2 rounded hover:bg-blue-500">
                Upload
            </button>
            <div id="uploadProgress" class="hidden">
                <div class="w-full bg-gray-200 rounded-full h-2.5">
                    <div id="uploadProgressBar" 
                         class="bg-blue-600 h-2.5 rounded-full" 
                         style="width: 0%"></div>
                </div>
                <div id="uploadStatus" class="text-sm text-gray-600 mt-1"></div>
            </div>
        </div>
</div>
`;
</file>

<file path="scripts/clear-db.js">
import inquirer from 'inquirer';
import { Pinecone } from '@pinecone-database/pinecone';
import mongoose from 'mongoose';
import dotenv from 'dotenv';
import { logger } from '../src/utils/logger.js';
import { Document } from '../src/models/document.model.js';

dotenv.config();

// Cache vector dimension
const VECTOR_DIM = parseInt(process.env.VECTOR_DIM || '1536', 10);

async function clearPineconeData(fileName = null, namespace = 'default') {
  try {
    const pinecone = new Pinecone({
      apiKey: process.env.PINECONE_API_KEY
    });

    const index = pinecone.index(process.env.PINECONE_INDEX);
    
    if (fileName) {
      // Use zero vector trick to fetch vectors by metadata
      const zeroVector = new Array(VECTOR_DIM).fill(0);
      
      // Query vectors by fileName
      const response = await index.namespace(namespace).query({
        vector: zeroVector,
        filter: { fileName },
        topK: 10000, // Adjust if needed
        includeMetadata: true
      });

      if (response.matches && response.matches.length > 0) {
        // Delete vectors in batches
        const BATCH_SIZE = 100;
        const vectorIds = response.matches.map(match => match.id);
        
        for (let i = 0; i < vectorIds.length; i += BATCH_SIZE) {
          const batch = vectorIds.slice(i, i + BATCH_SIZE);
          await index.namespace(namespace).deleteMany(batch);
          logger.info(`Deleted batch ${Math.floor(i/BATCH_SIZE) + 1}/${Math.ceil(vectorIds.length/BATCH_SIZE)} (${batch.length} vectors)`);
        }
        
        logger.info(`Successfully deleted ${vectorIds.length} vectors for fileName: ${fileName}`);
      } else {
        logger.info(`No vectors found for fileName: ${fileName}`);
      }
    } else if (namespace !== 'default') {
      // Use zero vector trick to fetch all vectors in namespace
      const zeroVector = new Array(VECTOR_DIM).fill(0);
      
      // Query all vectors in namespace
      const response = await index.namespace(namespace).query({
        vector: zeroVector,
        topK: 10000, // Adjust if needed
        includeMetadata: true
      });

      if (response.matches && response.matches.length > 0) {
        // Delete vectors in batches
        const BATCH_SIZE = 100;
        const vectorIds = response.matches.map(match => match.id);
        
        for (let i = 0; i < vectorIds.length; i += BATCH_SIZE) {
          const batch = vectorIds.slice(i, i + BATCH_SIZE);
          await index.namespace(namespace).deleteMany(batch);
          logger.info(`Deleted batch ${Math.floor(i/BATCH_SIZE) + 1}/${Math.ceil(vectorIds.length/BATCH_SIZE)} (${batch.length} vectors)`);
        }
        
        logger.info(`Successfully deleted ${vectorIds.length} vectors in namespace: ${namespace}`);
      } else {
        logger.info(`No vectors found in namespace: ${namespace}`);
      }
    } else {
      // Prompt user for confirmation before deleting all vectors
      const { confirm } = await inquirer.prompt([
        {
          type: 'confirm',
          name: 'confirm',
          message: 'WARNING: You are about to delete all vectors in the index. Do you want to proceed?',
          default: false
        }
      ]);

      if (confirm) {
        // Delete all vectors in the index
        await index.deleteAll();
        logger.info('Successfully cleared all data from Pinecone');
      } else {
        logger.info('Operation cancelled by the user.');
      }
    }
  } catch (error) {
    logger.error(`Error clearing Pinecone data: (${namespace})`, {
      message: error.message,
      stack: error.stack
    });
    throw error;
  }
}

async function clearMongoDBData(fileName = null, namespace = 'default') {
  try {
    await mongoose.connect(process.env.MONGODB_URI);
    
    if (fileName) {
      // Delete documents for specific fileName
      const result = await Document.deleteMany({ fileName, namespace });
      logger.info(`Deleted ${result.deletedCount} documents for fileName: ${fileName}`);
    } else if (namespace !== 'default') {
      // Delete documents for specific namespace
      const result = await Document.deleteMany({ namespace });
      logger.info(`Deleted ${result.deletedCount} documents in namespace: ${namespace}`);
    } else {
      logger.warn('Clearing all collections is currently disabled. No collections were dropped.');

      // Get all collections
      const collections = await mongoose.connection.db.collections();
      
      // Drop each collection
      for (const collection of collections) {
        logger.warn(`Skipping drop for collection: ${collection.collectionName}`);
      }
      
      logger.info('No data cleared from MongoDB due to disabled feature.');
    }
  } catch (error) {
    logger.error('Error clearing MongoDB data:', error);
    throw error;
  } finally {
    await mongoose.connection.close();
  }
}
async function main() {
  try {
    // Parse command line arguments
    const args = process.argv.slice(2);
    const fileNameArg = args.find(arg => arg.startsWith('--fileName='));
    const fileName = fileNameArg ? fileNameArg.split('=')[1] : null;
    
    const nsArg = args.find(arg => arg.startsWith('--ns=') || arg.startsWith('--namespace='));
    const namespace = nsArg ? nsArg.split('=')[1] : 'default';

    if (!nsArg) {
      const { confirmNamespace } = await inquirer.prompt([
        {
          type: 'confirm',
          name: 'confirmNamespace',
          message: 'WARNING: You have not provided a namespace. Do you want to proceed with the default namespace?',
          default: false
        }
      ]);

      if (!confirmNamespace) {
        logger.info('Operation cancelled due to missing namespace');
        return;
      }
    }

    if (fileName) {
      const { confirm } = await inquirer.prompt([
        {
          type: 'confirm',
          name: 'confirm',
          message: `Are you sure you want to clear all data for fileName: ${fileName} in namespace: ${namespace}?`,
          default: false
        }
      ]);

      if (!confirm) {
        logger.info('Operation cancelled');
        return;
      }

      logger.info(`Clearing data for fileName: ${fileName} in namespace: ${namespace}`);
      await clearMongoDBData(fileName, namespace);
      await clearPineconeData(fileName, namespace);
    } else {
      const { confirm } = await inquirer.prompt([
        {
          type: 'confirm',
          name: 'confirm',
          message: namespace !== 'default'
            ? `Are you sure you want to clear all data in namespace: ${namespace}?`
            : 'Are you sure you want to clear ALL data from both MongoDB and Pinecone?',
          default: false
        }
      ]);

      if (!confirm) {
        logger.info('Operation cancelled');
        return;
      }

      logger.info('Clearing all data...');

      if (namespace === 'default' || !namespace) {
        const { confirm } = await inquirer.prompt([
          {
            type: 'confirm',
            name: 'confirm',
            message: 'WARNING: You are about to clear all data in the default namespace. Do you want to proceed?',
            default: false
          }
        ]);

        if (!confirm) {
          logger.info('Operation cancelled');
          return;
        }
      }

      await clearMongoDBData(null, namespace);
      await clearPineconeData(null, namespace);
    }

    logger.info('Data clearing completed successfully');
  } catch (error) {
    logger.error('Error in main:', error);
    process.exit(1);
  }
}

// Run the script
main();
</file>

<file path="src/config/openai.js">
import OpenAI from 'openai';
import { logger } from '../utils/logger.js';

let openaiInstance = null;
let openaiEmbeddingInstance = null;

export function initOpenAIEmbedding(){
  if (!openaiEmbeddingInstance) {
    logger.info('Initializing OpenAI embedding client',{
      key: (process.env.EMBEDDING_OPENAI_API_KEY||process.env.OPENAI_API_KEY).slice(0, 15),
      baseURL: process.env.EMBEDDING_OPENAI_BASE_URL||process.env.OPENAI_BASE_URL
    });
    openaiEmbeddingInstance = new OpenAI({
      apiKey: process.env.EMBEDDING_OPENAI_API_KEY||process.env.OPENAI_API_KEY,
      baseURL: process.env.EMBEDDING_OPENAI_BASE_URL||process.env.OPENAI_BASE_URL
    });
  }
  return openaiEmbeddingInstance;
}

export function initOpenAI() {
  if (!openaiInstance) {

    if(!process.env.OPENAI_BASE_URL){
      logger.error('OPENAI_BASE_URL is not set');
      process.exit(1);
    }

    logger.info('Initializing OpenAI client',{key: (process.env.OPENAI_API_KEY).slice(0, 15)});
    openaiInstance = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY,
      baseURL: process.env.OPENAI_BASE_URL,
      defaultHeaders: {
        'HTTP-Referer': process.env.PUBLIC_DOMAIN || 'http://localhost:3000',
        'X-Title': 'CSV to RAG'
      }
    });
  }
  return openaiInstance;
}

export function getOpenAI() {
  return initOpenAI();
}

export function getOpenAIEmbedding() {
  return initOpenAIEmbedding();
}
</file>

<file path="src/config/pinecone.js">
import { Pinecone } from '@pinecone-database/pinecone';
import { logger } from '../utils/logger.js';

// Singleton Pinecone index instance
let pineconeIndexInstance = null;

export async function initPinecone() {
  if (pineconeIndexInstance) {
    console.log('src/config/pinecone.js initPinecone Returning cached Pinecone index singleton', { index: process.env.PINECONE_INDEX });
    return pineconeIndexInstance;
  }
  console.log('src/config/pinecone.js initPinecone Creating new Pinecone index singleton', { index: process.env.PINECONE_INDEX });
  try {
    const pinecone = new Pinecone({
      apiKey: process.env.PINECONE_API_KEY,
    });
    pineconeIndexInstance = pinecone.index(process.env.PINECONE_INDEX, process.env.PINECONE_HOST);
    return pineconeIndexInstance;
  } catch (err) {
    console.log('src/config/pinecone.js initPinecone Error initializing Pinecone client', { message: err.message, stack: err.stack });
    throw err;
  }
}
</file>

<file path="src/models/document.model.js">
import mongoose from 'mongoose';

const documentSchema = new mongoose.Schema({
  code: {
    type: String,
    required: true
  },
  fileName: {
    type: String,
    required: true,
    index: true
  },
  metadata_small: {
    type: String,
    required: true
  },
  metadata_big_1: {
    type: mongoose.Schema.Types.Mixed
  },
  metadata_big_2: {
    type: mongoose.Schema.Types.Mixed
  },
  metadata_big_3: {
    type: mongoose.Schema.Types.Mixed
  },
  namespace: {
    type: String,
    default: 'default',
    required:true
  },
  timestamp: {
    type: Date,
    default: Date.now
  }
});

// Remove single index on code since we'll use a compound index
documentSchema.index({ code: 1, namespace: 1 }, { unique: true });

// Compound index for efficient querying by fileName
documentSchema.index({ fileName: 1, code: 1 });

export const Document = mongoose.model('Document', documentSchema);
</file>

<file path="scripts/deno-ui/app/app.js">
// All files under /deno-ui/app are compiled and combined into main.js (All functions are available globally)

// Initialize everything when the page loads
window.addEventListener('load', () => {
    // scripts/deno-ui/app/app.js window load initializing
    console.log('app.js window load initializing', {data:{}});
    try {
        // Initialize tabs instead of sections
        initializeTabs();
        loadLogsFromStorage();
        displayLogs();
        checkBackendState();
        loadNamespaces();
        // scripts/deno-ui/app/app.js load UI initialized
        console.log('app.js load UI initialized', {data:{}});
    } catch (err) {
        // scripts/deno-ui/app/app.js window load try/catch
        console.log('app.js window load try/catch', {message: err?.message, stack: err?.stack});
    }
    
    // Set up log scroll handler
    const logsContent = document.getElementById('logsContent');
    if (logsContent) {
        logsContent.addEventListener('scroll', handleLogsScroll);
    }
    
    // Add keyboard shortcut for toggling logs section (ALT+L)
    document.addEventListener('keydown', (event) => {
        // Check if ALT+L was pressed
        if (event.altKey && event.key.toLowerCase() === 'l') {
            console.log('app.js keydown ALT+L detected', {data: {logsVisible}});
            event.preventDefault(); // Prevent default browser behavior
            toggleSection('logs');
        }
    });

    toggleSection('logs');
});

// Clean up on page unload
window.addEventListener('unload', () => {
    stopLogFetching();
});
</file>

<file path="scripts/deno-ui/ui.js">
import { template as mainTemplate } from './ui/template.js';
import { styles } from './ui/styles.js';
import { template as backendTemplate } from './ui/backend.js';
import { template as uploadTemplate } from './ui/upload.js';
import { template as fileListTemplate } from './ui/fileList.js';
import { template as queryTemplate } from './ui/query.js';
import { template as logsTemplate } from './ui/logs.js';

// Combine all templates
export const template = ()=>mainTemplate.replace(
    '</head>',
    `<style>${styles}</style>
</head>`
).replace(
    '<div id="error" class="text-red-600 mt-4 mb-4"></div>',
    `<div id="error" class="text-red-600 mt-4 mb-4"></div>
    
    <!-- Tab Content -->
    <div id="upload-tab" class="tab-content active">
        ${uploadTemplate}
    </div>
    <div id="files-tab" class="tab-content hidden">
        ${fileListTemplate}
    </div>
    <div id="query-tab" class="tab-content hidden">
        ${queryTemplate}
    </div>
    <div id="backend-tab" class="tab-content hidden">
        ${backendTemplate()}
    </div>
    </div>
    ${logsTemplate}`
);
</file>

<file path="src/middleware/validation.middleware.js">
import Joi from 'joi';
import { logger } from '../utils/logger.js';
import { parse } from 'csv-parse/sync';

// Validation schema for CSV file uploads
export const csvSchema = Joi.object({
  code: Joi.string().required(),
  metadata_small: Joi.string().required(),
  metadata_big_1: Joi.string().allow('').optional(),
  metadata_big_2: Joi.string().allow('').optional(),
  metadata_big_3: Joi.string().allow('').optional(),
});

// Validation middleware for CSV data
export const validateCsv = (req, res, next) => {
  try {
    if (!req.file) {
      logger.warn('No file uploaded');
      return res.status(400).json({ error: 'No file uploaded' });
    }

    const csvData = req.file.buffer.toString('utf-8');
    //logger.info('CSV Data:', csvData); // Log the raw CSV data

    const records = parse(csvData, {
      columns: true,
      delimiter: ';',
      skip_empty_lines: true
    });

    logger.info('Parsed Records:', records.length); // Log the parsed records

    if (records.length === 0) {
      logger.warn('CSV file is empty');
      return res.status(400).json({ error: 'CSV file is empty' });
    }

    // Validate first row to check structure
    const { error } = csvSchema.validate(records[0]);
    if (error) {
      logger.warn('CSV validation error:', { details: error.details });
      return res.status(400).json({ error: error.details[0].message });
    }

    // Store parsed records for later use
    req.csvRecords = records;
    next();
  } catch (err) {
    logger.error('CSV parsing error:', err);
    return res.status(400).json({ error: 'Failed to parse CSV file' });
  }
};

// Validation schema for query requests
const querySchema = Joi.object({
  query: Joi.string().required(),
  namespace: Joi.string().optional(),
  onlyContext: Joi.boolean().optional().allow('true', 'false').default(false)
});

// Validation middleware for query requests
export const validateQuery = (req, res, next) => {
  const { error } = querySchema.validate(req.body);
  if (error) {
    logger.warn('Query validation error:', { details: error.details });
    return res.status(400).json({ error: error.details[0].message });
  }
  next();
};

// Validation schema for completion requests
const completionSchema = Joi.object({
  prompt: Joi.string().required(),
  max_tokens: Joi.number().integer().min(1).optional(),
  temperature: Joi.number().min(0).max(2).optional(),
  model: Joi.string().optional()
});

// Validation middleware for completion requests
export const validateCompletion = (req, res, next) => {
  const { error } = completionSchema.validate(req.body);
  if (error) {
    logger.warn('Completion validation error:', { details: error.details });
    return res.status(400).json({ error: error.details[0].message });
  }
  next();
};
</file>

<file path="src/routes/query.routes.js">
import express from 'express';
import { QueryService } from '../services/query.service.js';
import { validateQuery, validateCompletion } from '../middleware/validation.middleware.js';
import { logger } from '../utils/logger.js';
import { v4 as uuidv4 } from 'uuid';

const router = express.Router();

/**
 * @route POST /api/query
 * @desc Perform a similarity search and generate a response
 * @access Public
 */
router.post('/query', validateQuery, async (req, res, next) => {
  try {
    const { query } = req.body;
    const namespace = req.query.namespace || req.body.namespace || 'default';
    const onlyContext = Boolean(req.query.onlyContext || req.body.onlyContext);

    console.debug('Query:', { query, namespace, onlyContext })
    
    const { documents } = await QueryService.performSimilaritySearch(query, 5, namespace);
    
    if (onlyContext) {
      // Return only the context without LLM completion
      res.json(documents.map(mapRemoveKeys(['_id','__v','timestamp','fileName','namespace'])));
    } else {
      // Generate LLM response as before
      const answer = await QueryService.generateResponse(query, documents);
      res.json({
        answer,
        sources: documents.map(doc => ({
          fileName: doc.fileName,
          namespace: doc.namespace,
          context: doc.metadata_small // Using metadata_small as context
        }))
      });
    }
  } catch (error) {
    logger.error('Error in query processing:', error);
    next(error);
  }
});

const mapRemoveKeys = keysToRemove => doc => {
  doc = doc.toJSON();
  return Object.fromEntries(
    Object.entries(doc).filter(([key]) => !keysToRemove.includes(key))
  );
}

/**
 * @route POST /api/completion
 * @desc Legacy completion endpoint (defaults to 'default' namespace)
 * @access Public
 */
router.post('/completion', validateCompletion, async (req, res, next) => {
  try {
    const { prompt, max_tokens, temperature } = req.body;
    const namespace = req.query.namespace || req.body.namespace || 'default';
    await handleCompletion(req, res, next, namespace);
  } catch (error) {
    logger.error('Error in completion:', error);
    next(error);
  }
});

/**
 * @route POST /api/ns/:namespace/completion
 * @desc Namespaced completion endpoint following OpenAI URL pattern
 * @access Public
 */
router.post('/ns/:namespace/completion', validateCompletion, async (req, res, next) => {
  try {
    const namespace = req.params.namespace;
    await handleCompletion(req, res, next, namespace);
  } catch (error) {
    logger.error('Error in namespaced completion:', error);
    next(error);
  }
});

/**
 * Handle completion request for both namespaced and non-namespaced routes
 */
async function handleCompletion(req, res, next, namespace) {
  const { prompt, max_tokens, temperature } = req.body;
  
  // Generate a unique ID for the completion
  const completionId = uuidv4();

  // Perform similarity search using the prompt as the query
  const { documents } = await QueryService.performSimilaritySearch(prompt, 5, namespace);
  
  // Generate response using the retrieved documents
  const completion = await QueryService.generateResponse(prompt, documents);

  // Format the response to match the OpenAI API format
  res.json({
    id: `cmpl-${completionId}`,
    object: 'text_completion',
    created: Math.floor(Date.now() / 1000),
    model: 'gpt-3.5-turbo',
    choices: [{
      text: completion,
      index: 0,
      logprobs: null,
      finish_reason: 'stop'
    }],
    usage: {
      prompt_tokens: prompt.length,
      completion_tokens: completion.length,
      total_tokens: prompt.length + completion.length
    }
  });
}

export const queryRoutes = router;
</file>

<file path="deno.lock">
{
  "version": "4",
  "redirects": {
    "https://deno.land/std/path/mod.ts": "https://deno.land/std@0.224.0/path/mod.ts",
    "https://deno.land/x/dotenv/mod.ts": "https://deno.land/x/dotenv@v3.2.2/mod.ts"
  },
  "remote": {
    "https://deno.land/std@0.210.0/assert/assert.ts": "e265ad50a9341f3b40e51dd4cb41ab253d976943ba78a977106950e52e0302ab",
    "https://deno.land/std@0.210.0/assert/assertion_error.ts": "26ed1863d905005f00785c89750c001c3522c5417e4f58f95044b8143cfc1593",
    "https://deno.land/std@0.210.0/async/delay.ts": "d414b6ff5b597a3b8a90b1b860b675062a106ad0e311816256f4696aa40ec1a1",
    "https://deno.land/std@0.210.0/http/server.ts": "59a47779624ff748a058c6959d75fc5ca9f334d32b327344a2eec20561880b58",
    "https://deno.land/std@0.210.0/path/_common/assert_path.ts": "061e4d093d4ba5aebceb2c4da3318bfe3289e868570e9d3a8e327d91c2958946",
    "https://deno.land/std@0.210.0/path/_common/basename.ts": "0d978ff818f339cd3b1d09dc914881f4d15617432ae519c1b8fdc09ff8d3789a",
    "https://deno.land/std@0.210.0/path/_common/common.ts": "9e4233b2eeb50f8b2ae10ecc2108f58583aea6fd3e8907827020282dc2b76143",
    "https://deno.land/std@0.210.0/path/_common/constants.ts": "e49961f6f4f48039c0dfed3c3f93e963ca3d92791c9d478ac5b43183413136e0",
    "https://deno.land/std@0.210.0/path/_common/dirname.ts": "2ba7fb4cc9fafb0f38028f434179579ce61d4d9e51296fad22b701c3d3cd7397",
    "https://deno.land/std@0.210.0/path/_common/format.ts": "11aa62e316dfbf22c126917f5e03ea5fe2ee707386555a8f513d27ad5756cf96",
    "https://deno.land/std@0.210.0/path/_common/from_file_url.ts": "ef1bf3197d2efbf0297a2bdbf3a61d804b18f2bcce45548ae112313ec5be3c22",
    "https://deno.land/std@0.210.0/path/_common/glob_to_reg_exp.ts": "50386887d6041f15741d0013a703ee63ef673983d465d3a0c9c190e95f8da4fe",
    "https://deno.land/std@0.210.0/path/_common/normalize.ts": "2ba7fb4cc9fafb0f38028f434179579ce61d4d9e51296fad22b701c3d3cd7397",
    "https://deno.land/std@0.210.0/path/_common/normalize_string.ts": "88c472f28ae49525f9fe82de8c8816d93442d46a30d6bb5063b07ff8a89ff589",
    "https://deno.land/std@0.210.0/path/_common/relative.ts": "1af19d787a2a84b8c534cc487424fe101f614982ae4851382c978ab2216186b4",
    "https://deno.land/std@0.210.0/path/_common/strip_trailing_separators.ts": "7ffc7c287e97bdeeee31b155828686967f222cd73f9e5780bfe7dfb1b58c6c65",
    "https://deno.land/std@0.210.0/path/_common/to_file_url.ts": "a8cdd1633bc9175b7eebd3613266d7c0b6ae0fb0cff24120b6092ac31662f9ae",
    "https://deno.land/std@0.210.0/path/_interface.ts": "6471159dfbbc357e03882c2266d21ef9afdb1e4aa771b0545e90db58a0ba314b",
    "https://deno.land/std@0.210.0/path/_os.ts": "30b0c2875f360c9296dbe6b7f2d528f0f9c741cecad2e97f803f5219e91b40a2",
    "https://deno.land/std@0.210.0/path/basename.ts": "04bb5ef3e86bba8a35603b8f3b69537112cdd19ce64b77f2522006da2977a5f3",
    "https://deno.land/std@0.210.0/path/common.ts": "f4d061c7d0b95a65c2a1a52439edec393e906b40f1caf4604c389fae7caa80f5",
    "https://deno.land/std@0.210.0/path/dirname.ts": "88a0a71c21debafc4da7a4cd44fd32e899462df458fbca152390887d41c40361",
    "https://deno.land/std@0.210.0/path/extname.ts": "8c6d6112bce335b4d3d5a07cb0451816d0c2094c147049874fca2db5f707044b",
    "https://deno.land/std@0.210.0/path/format.ts": "3457530cc85d1b4bab175f9ae73998b34fd456c830d01883169af0681b8894fb",
    "https://deno.land/std@0.210.0/path/from_file_url.ts": "e7fa233ea1dff9641e8d566153a24d95010110185a6f418dd2e32320926043f8",
    "https://deno.land/std@0.210.0/path/glob_to_regexp.ts": "74d7448c471e293d03f05ccb968df4365fed6aaa508506b6325a8efdc01d8271",
    "https://deno.land/std@0.210.0/path/is_absolute.ts": "67232b41b860571c5b7537f4954c88d86ae2ba45e883ee37d3dec27b74909d13",
    "https://deno.land/std@0.210.0/path/is_glob.ts": "567dce5c6656bdedfc6b3ee6c0833e1e4db2b8dff6e62148e94a917f289c06ad",
    "https://deno.land/std@0.210.0/path/join.ts": "3ee91038e3eaa966897eddda43d5207d7cae5c2de8a658bdbd722e8f8f29206a",
    "https://deno.land/std@0.210.0/path/join_globs.ts": "9b84d5103b63d3dbed4b2cf8b12477b2ad415c7d343f1488505162dc0e5f4db8",
    "https://deno.land/std@0.210.0/path/mod.ts": "eff1d7b0617293bd90254d379a7266887dc6fbf5a00e0f450eeb854959379294",
    "https://deno.land/std@0.210.0/path/normalize.ts": "aa95be9a92c7bd4f9dc0ba51e942a1973e2b93d266cd74f5ca751c136d520b66",
    "https://deno.land/std@0.210.0/path/normalize_glob.ts": "674baa82e1c00b6cb153bbca36e06f8e0337cb8062db6d905ab5de16076ca46b",
    "https://deno.land/std@0.210.0/path/parse.ts": "d87ff0deef3fb495bc0d862278ff96da5a06acf0625ca27769fc52ac0d3d6ece",
    "https://deno.land/std@0.210.0/path/posix/_util.ts": "ecf49560fedd7dd376c6156cc5565cad97c1abe9824f4417adebc7acc36c93e5",
    "https://deno.land/std@0.210.0/path/posix/basename.ts": "a630aeb8fd8e27356b1823b9dedd505e30085015407caa3396332752f6b8406a",
    "https://deno.land/std@0.210.0/path/posix/common.ts": "e781d395dc76f6282e3f7dd8de13194abb8b04a82d109593141abc6e95755c8b",
    "https://deno.land/std@0.210.0/path/posix/dirname.ts": "f48c9c42cc670803b505478b7ef162c7cfa9d8e751b59d278b2ec59470531472",
    "https://deno.land/std@0.210.0/path/posix/extname.ts": "ee7f6571a9c0a37f9218fbf510c440d1685a7c13082c348d701396cc795e0be0",
    "https://deno.land/std@0.210.0/path/posix/format.ts": "b94876f77e61bfe1f147d5ccb46a920636cd3cef8be43df330f0052b03875968",
    "https://deno.land/std@0.210.0/path/posix/from_file_url.ts": "b97287a83e6407ac27bdf3ab621db3fccbf1c27df0a1b1f20e1e1b5acf38a379",
    "https://deno.land/std@0.210.0/path/posix/glob_to_regexp.ts": "6ed00c71fbfe0ccc35977c35444f94e82200b721905a60bd1278b1b768d68b1a",
    "https://deno.land/std@0.210.0/path/posix/is_absolute.ts": "159900a3422d11069d48395568217eb7fc105ceda2683d03d9b7c0f0769e01b8",
    "https://deno.land/std@0.210.0/path/posix/is_glob.ts": "ec4fbc604b9db8487f7b56ab0e759b24a971ab6a45f7b0b698bc39b8b9f9680f",
    "https://deno.land/std@0.210.0/path/posix/join.ts": "0c0d84bdc344876930126640011ec1b888e6facf74153ffad9ef26813aa2a076",
    "https://deno.land/std@0.210.0/path/posix/join_globs.ts": "f4838d54b1f60a34a40625a3293f6e583135348be1b2974341ac04743cb26121",
    "https://deno.land/std@0.210.0/path/posix/mod.ts": "f1b08a7f64294b7de87fc37190d63b6ce5b02889af9290c9703afe01951360ae",
    "https://deno.land/std@0.210.0/path/posix/normalize.ts": "11de90a94ab7148cc46e5a288f7d732aade1d616bc8c862f5560fa18ff987b4b",
    "https://deno.land/std@0.210.0/path/posix/normalize_glob.ts": "10a1840c628ebbab679254d5fa1c20e59106102354fb648a1765aed72eb9f3f9",
    "https://deno.land/std@0.210.0/path/posix/parse.ts": "199208f373dd93a792e9c585352bfc73a6293411bed6da6d3bc4f4ef90b04c8e",
    "https://deno.land/std@0.210.0/path/posix/relative.ts": "e2f230608b0f083e6deaa06e063943e5accb3320c28aef8d87528fbb7fe6504c",
    "https://deno.land/std@0.210.0/path/posix/resolve.ts": "51579d83159d5c719518c9ae50812a63959bbcb7561d79acbdb2c3682236e285",
    "https://deno.land/std@0.210.0/path/posix/separator.ts": "0b6573b5f3269a3164d8edc9cefc33a02dd51003731c561008c8bb60220ebac1",
    "https://deno.land/std@0.210.0/path/posix/to_file_url.ts": "ac5499aa0c6e2c266019cba7d1f7e5a92b8e04983cd72be97f81adad185619a6",
    "https://deno.land/std@0.210.0/path/posix/to_namespaced_path.ts": "c9228a0e74fd37e76622cd7b142b8416663a9b87db643302fa0926b5a5c83bdc",
    "https://deno.land/std@0.210.0/path/relative.ts": "23d45ede8b7ac464a8299663a43488aad6b561414e7cbbe4790775590db6349c",
    "https://deno.land/std@0.210.0/path/resolve.ts": "5b184efc87155a0af9fa305ff68a109e28de9aee81fc3e77cd01380f19daf867",
    "https://deno.land/std@0.210.0/path/separator.ts": "1a21ffd408bfaa317bffff604e5a799f78a7a5571590bde6b9cdce7685953d74",
    "https://deno.land/std@0.210.0/path/to_file_url.ts": "edaafa089e0bce386e1b2d47afe7c72e379ff93b28a5829a5885e4b6c626d864",
    "https://deno.land/std@0.210.0/path/to_namespaced_path.ts": "cf8734848aac3c7527d1689d2adf82132b1618eff3cc523a775068847416b22a",
    "https://deno.land/std@0.210.0/path/windows/_util.ts": "f32b9444554c8863b9b4814025c700492a2b57ff2369d015360970a1b1099d54",
    "https://deno.land/std@0.210.0/path/windows/basename.ts": "8a9dbf7353d50afbc5b221af36c02a72c2d1b2b5b9f7c65bf6a5a2a0baf88ad3",
    "https://deno.land/std@0.210.0/path/windows/common.ts": "e781d395dc76f6282e3f7dd8de13194abb8b04a82d109593141abc6e95755c8b",
    "https://deno.land/std@0.210.0/path/windows/dirname.ts": "5c2aa541384bf0bd9aca821275d2a8690e8238fa846198ef5c7515ce31a01a94",
    "https://deno.land/std@0.210.0/path/windows/extname.ts": "07f4fa1b40d06a827446b3e3bcc8d619c5546b079b8ed0c77040bbef716c7614",
    "https://deno.land/std@0.210.0/path/windows/format.ts": "343019130d78f172a5c49fdc7e64686a7faf41553268961e7b6c92a6d6548edf",
    "https://deno.land/std@0.210.0/path/windows/from_file_url.ts": "d53335c12b0725893d768be3ac6bf0112cc5b639d2deb0171b35988493b46199",
    "https://deno.land/std@0.210.0/path/windows/glob_to_regexp.ts": "290755e18ec6c1a4f4d711c3390537358e8e3179581e66261a0cf348b1a13395",
    "https://deno.land/std@0.210.0/path/windows/is_absolute.ts": "245b56b5f355ede8664bd7f080c910a97e2169972d23075554ae14d73722c53c",
    "https://deno.land/std@0.210.0/path/windows/is_glob.ts": "ec4fbc604b9db8487f7b56ab0e759b24a971ab6a45f7b0b698bc39b8b9f9680f",
    "https://deno.land/std@0.210.0/path/windows/join.ts": "e6600bf88edeeef4e2276e155b8de1d5dec0435fd526ba2dc4d37986b2882f16",
    "https://deno.land/std@0.210.0/path/windows/join_globs.ts": "f4838d54b1f60a34a40625a3293f6e583135348be1b2974341ac04743cb26121",
    "https://deno.land/std@0.210.0/path/windows/mod.ts": "d7040f461465c2c21c1c68fc988ef0bdddd499912138cde3abf6ad60c7fb3814",
    "https://deno.land/std@0.210.0/path/windows/normalize.ts": "9deebbf40c81ef540b7b945d4ccd7a6a2c5a5992f791e6d3377043031e164e69",
    "https://deno.land/std@0.210.0/path/windows/normalize_glob.ts": "344ff5ed45430495b9a3d695567291e50e00b1b3b04ea56712a2acf07ab5c128",
    "https://deno.land/std@0.210.0/path/windows/parse.ts": "120faf778fe1f22056f33ded069b68e12447668fcfa19540c0129561428d3ae5",
    "https://deno.land/std@0.210.0/path/windows/relative.ts": "026855cd2c36c8f28f1df3c6fbd8f2449a2aa21f48797a74700c5d872b86d649",
    "https://deno.land/std@0.210.0/path/windows/resolve.ts": "5ff441ab18a2346abadf778121128ee71bda4d0898513d4639a6ca04edca366b",
    "https://deno.land/std@0.210.0/path/windows/separator.ts": "ae21f27015f10510ed1ac4a0ba9c4c9c967cbdd9d9e776a3e4967553c397bd5d",
    "https://deno.land/std@0.210.0/path/windows/to_file_url.ts": "8e9ea9e1ff364aa06fa72999204229952d0a279dbb876b7b838b2b2fea55cce3",
    "https://deno.land/std@0.210.0/path/windows/to_namespaced_path.ts": "e0f4d4a5e77f28a5708c1a33ff24360f35637ba6d8f103d19661255ef7bfd50d",
    "https://deno.land/std@0.224.0/assert/assert.ts": "09d30564c09de846855b7b071e62b5974b001bb72a4b797958fe0660e7849834",
    "https://deno.land/std@0.224.0/assert/assertion_error.ts": "ba8752bd27ebc51f723702fac2f54d3e94447598f54264a6653d6413738a8917",
    "https://deno.land/std@0.224.0/path/_common/assert_path.ts": "dbdd757a465b690b2cc72fc5fb7698c51507dec6bfafce4ca500c46b76ff7bd8",
    "https://deno.land/std@0.224.0/path/_common/basename.ts": "569744855bc8445f3a56087fd2aed56bdad39da971a8d92b138c9913aecc5fa2",
    "https://deno.land/std@0.224.0/path/_common/common.ts": "ef73c2860694775fe8ffcbcdd387f9f97c7a656febf0daa8c73b56f4d8a7bd4c",
    "https://deno.land/std@0.224.0/path/_common/constants.ts": "dc5f8057159f4b48cd304eb3027e42f1148cf4df1fb4240774d3492b5d12ac0c",
    "https://deno.land/std@0.224.0/path/_common/dirname.ts": "684df4aa71a04bbcc346c692c8485594fc8a90b9408dfbc26ff32cf3e0c98cc8",
    "https://deno.land/std@0.224.0/path/_common/format.ts": "92500e91ea5de21c97f5fe91e178bae62af524b72d5fcd246d6d60ae4bcada8b",
    "https://deno.land/std@0.224.0/path/_common/from_file_url.ts": "d672bdeebc11bf80e99bf266f886c70963107bdd31134c4e249eef51133ceccf",
    "https://deno.land/std@0.224.0/path/_common/glob_to_reg_exp.ts": "6cac16d5c2dc23af7d66348a7ce430e5de4e70b0eede074bdbcf4903f4374d8d",
    "https://deno.land/std@0.224.0/path/_common/normalize.ts": "684df4aa71a04bbcc346c692c8485594fc8a90b9408dfbc26ff32cf3e0c98cc8",
    "https://deno.land/std@0.224.0/path/_common/normalize_string.ts": "33edef773c2a8e242761f731adeb2bd6d683e9c69e4e3d0092985bede74f4ac3",
    "https://deno.land/std@0.224.0/path/_common/relative.ts": "faa2753d9b32320ed4ada0733261e3357c186e5705678d9dd08b97527deae607",
    "https://deno.land/std@0.224.0/path/_common/strip_trailing_separators.ts": "7024a93447efcdcfeaa9339a98fa63ef9d53de363f1fbe9858970f1bba02655a",
    "https://deno.land/std@0.224.0/path/_common/to_file_url.ts": "7f76adbc83ece1bba173e6e98a27c647712cab773d3f8cbe0398b74afc817883",
    "https://deno.land/std@0.224.0/path/_interface.ts": "8dfeb930ca4a772c458a8c7bbe1e33216fe91c253411338ad80c5b6fa93ddba0",
    "https://deno.land/std@0.224.0/path/_os.ts": "8fb9b90fb6b753bd8c77cfd8a33c2ff6c5f5bc185f50de8ca4ac6a05710b2c15",
    "https://deno.land/std@0.224.0/path/basename.ts": "7ee495c2d1ee516ffff48fb9a93267ba928b5a3486b550be73071bc14f8cc63e",
    "https://deno.land/std@0.224.0/path/common.ts": "03e52e22882402c986fe97ca3b5bb4263c2aa811c515ce84584b23bac4cc2643",
    "https://deno.land/std@0.224.0/path/constants.ts": "0c206169ca104938ede9da48ac952de288f23343304a1c3cb6ec7625e7325f36",
    "https://deno.land/std@0.224.0/path/dirname.ts": "85bd955bf31d62c9aafdd7ff561c4b5fb587d11a9a5a45e2b01aedffa4238a7c",
    "https://deno.land/std@0.224.0/path/extname.ts": "593303db8ae8c865cbd9ceec6e55d4b9ac5410c1e276bfd3131916591b954441",
    "https://deno.land/std@0.224.0/path/format.ts": "6ce1779b0980296cf2bc20d66436b12792102b831fd281ab9eb08fa8a3e6f6ac",
    "https://deno.land/std@0.224.0/path/from_file_url.ts": "911833ae4fd10a1c84f6271f36151ab785955849117dc48c6e43b929504ee069",
    "https://deno.land/std@0.224.0/path/glob_to_regexp.ts": "7f30f0a21439cadfdae1be1bf370880b415e676097fda584a63ce319053b5972",
    "https://deno.land/std@0.224.0/path/is_absolute.ts": "4791afc8bfd0c87f0526eaa616b0d16e7b3ab6a65b62942e50eac68de4ef67d7",
    "https://deno.land/std@0.224.0/path/is_glob.ts": "a65f6195d3058c3050ab905705891b412ff942a292bcbaa1a807a74439a14141",
    "https://deno.land/std@0.224.0/path/join.ts": "ae2ec5ca44c7e84a235fd532e4a0116bfb1f2368b394db1c4fb75e3c0f26a33a",
    "https://deno.land/std@0.224.0/path/join_globs.ts": "5b3bf248b93247194f94fa6947b612ab9d3abd571ca8386cf7789038545e54a0",
    "https://deno.land/std@0.224.0/path/mod.ts": "f6bd79cb08be0e604201bc9de41ac9248582699d1b2ee0ab6bc9190d472cf9cd",
    "https://deno.land/std@0.224.0/path/normalize.ts": "4155743ccceeed319b350c1e62e931600272fad8ad00c417b91df093867a8352",
    "https://deno.land/std@0.224.0/path/normalize_glob.ts": "cc89a77a7d3b1d01053b9dcd59462b75482b11e9068ae6c754b5cf5d794b374f",
    "https://deno.land/std@0.224.0/path/parse.ts": "77ad91dcb235a66c6f504df83087ce2a5471e67d79c402014f6e847389108d5a",
    "https://deno.land/std@0.224.0/path/posix/_util.ts": "1e3937da30f080bfc99fe45d7ed23c47dd8585c5e473b2d771380d3a6937cf9d",
    "https://deno.land/std@0.224.0/path/posix/basename.ts": "d2fa5fbbb1c5a3ab8b9326458a8d4ceac77580961b3739cd5bfd1d3541a3e5f0",
    "https://deno.land/std@0.224.0/path/posix/common.ts": "26f60ccc8b2cac3e1613000c23ac5a7d392715d479e5be413473a37903a2b5d4",
    "https://deno.land/std@0.224.0/path/posix/constants.ts": "93481efb98cdffa4c719c22a0182b994e5a6aed3047e1962f6c2c75b7592bef1",
    "https://deno.land/std@0.224.0/path/posix/dirname.ts": "76cd348ffe92345711409f88d4d8561d8645353ac215c8e9c80140069bf42f00",
    "https://deno.land/std@0.224.0/path/posix/extname.ts": "e398c1d9d1908d3756a7ed94199fcd169e79466dd88feffd2f47ce0abf9d61d2",
    "https://deno.land/std@0.224.0/path/posix/format.ts": "185e9ee2091a42dd39e2a3b8e4925370ee8407572cee1ae52838aed96310c5c1",
    "https://deno.land/std@0.224.0/path/posix/from_file_url.ts": "951aee3a2c46fd0ed488899d024c6352b59154c70552e90885ed0c2ab699bc40",
    "https://deno.land/std@0.224.0/path/posix/glob_to_regexp.ts": "76f012fcdb22c04b633f536c0b9644d100861bea36e9da56a94b9c589a742e8f",
    "https://deno.land/std@0.224.0/path/posix/is_absolute.ts": "cebe561ad0ae294f0ce0365a1879dcfca8abd872821519b4fcc8d8967f888ede",
    "https://deno.land/std@0.224.0/path/posix/is_glob.ts": "8a8b08c08bf731acf2c1232218f1f45a11131bc01de81e5f803450a5914434b9",
    "https://deno.land/std@0.224.0/path/posix/join.ts": "7fc2cb3716aa1b863e990baf30b101d768db479e70b7313b4866a088db016f63",
    "https://deno.land/std@0.224.0/path/posix/join_globs.ts": "a9475b44645feddceb484ee0498e456f4add112e181cb94042cdc6d47d1cdd25",
    "https://deno.land/std@0.224.0/path/posix/mod.ts": "2301fc1c54a28b349e20656f68a85f75befa0ee9b6cd75bfac3da5aca9c3f604",
    "https://deno.land/std@0.224.0/path/posix/normalize.ts": "baeb49816a8299f90a0237d214cef46f00ba3e95c0d2ceb74205a6a584b58a91",
    "https://deno.land/std@0.224.0/path/posix/normalize_glob.ts": "9c87a829b6c0f445d03b3ecadc14492e2864c3ebb966f4cea41e98326e4435c6",
    "https://deno.land/std@0.224.0/path/posix/parse.ts": "09dfad0cae530f93627202f28c1befa78ea6e751f92f478ca2cc3b56be2cbb6a",
    "https://deno.land/std@0.224.0/path/posix/relative.ts": "3907d6eda41f0ff723d336125a1ad4349112cd4d48f693859980314d5b9da31c",
    "https://deno.land/std@0.224.0/path/posix/resolve.ts": "08b699cfeee10cb6857ccab38fa4b2ec703b0ea33e8e69964f29d02a2d5257cf",
    "https://deno.land/std@0.224.0/path/posix/to_file_url.ts": "7aa752ba66a35049e0e4a4be5a0a31ac6b645257d2e031142abb1854de250aaf",
    "https://deno.land/std@0.224.0/path/posix/to_namespaced_path.ts": "28b216b3c76f892a4dca9734ff1cc0045d135532bfd9c435ae4858bfa5a2ebf0",
    "https://deno.land/std@0.224.0/path/relative.ts": "ab739d727180ed8727e34ed71d976912461d98e2b76de3d3de834c1066667add",
    "https://deno.land/std@0.224.0/path/resolve.ts": "a6f977bdb4272e79d8d0ed4333e3d71367cc3926acf15ac271f1d059c8494d8d",
    "https://deno.land/std@0.224.0/path/to_file_url.ts": "88f049b769bce411e2d2db5bd9e6fd9a185a5fbd6b9f5ad8f52bef517c4ece1b",
    "https://deno.land/std@0.224.0/path/to_namespaced_path.ts": "b706a4103b104cfadc09600a5f838c2ba94dbcdb642344557122dda444526e40",
    "https://deno.land/std@0.224.0/path/windows/_util.ts": "d5f47363e5293fced22c984550d5e70e98e266cc3f31769e1710511803d04808",
    "https://deno.land/std@0.224.0/path/windows/basename.ts": "6bbc57bac9df2cec43288c8c5334919418d784243a00bc10de67d392ab36d660",
    "https://deno.land/std@0.224.0/path/windows/common.ts": "26f60ccc8b2cac3e1613000c23ac5a7d392715d479e5be413473a37903a2b5d4",
    "https://deno.land/std@0.224.0/path/windows/constants.ts": "5afaac0a1f67b68b0a380a4ef391bf59feb55856aa8c60dfc01bd3b6abb813f5",
    "https://deno.land/std@0.224.0/path/windows/dirname.ts": "33e421be5a5558a1346a48e74c330b8e560be7424ed7684ea03c12c21b627bc9",
    "https://deno.land/std@0.224.0/path/windows/extname.ts": "165a61b00d781257fda1e9606a48c78b06815385e7d703232548dbfc95346bef",
    "https://deno.land/std@0.224.0/path/windows/format.ts": "bbb5ecf379305b472b1082cd2fdc010e44a0020030414974d6029be9ad52aeb6",
    "https://deno.land/std@0.224.0/path/windows/from_file_url.ts": "ced2d587b6dff18f963f269d745c4a599cf82b0c4007356bd957cb4cb52efc01",
    "https://deno.land/std@0.224.0/path/windows/glob_to_regexp.ts": "e45f1f89bf3fc36f94ab7b3b9d0026729829fabc486c77f414caebef3b7304f8",
    "https://deno.land/std@0.224.0/path/windows/is_absolute.ts": "4a8f6853f8598cf91a835f41abed42112cebab09478b072e4beb00ec81f8ca8a",
    "https://deno.land/std@0.224.0/path/windows/is_glob.ts": "8a8b08c08bf731acf2c1232218f1f45a11131bc01de81e5f803450a5914434b9",
    "https://deno.land/std@0.224.0/path/windows/join.ts": "8d03530ab89195185103b7da9dfc6327af13eabdcd44c7c63e42e27808f50ecf",
    "https://deno.land/std@0.224.0/path/windows/join_globs.ts": "a9475b44645feddceb484ee0498e456f4add112e181cb94042cdc6d47d1cdd25",
    "https://deno.land/std@0.224.0/path/windows/mod.ts": "2301fc1c54a28b349e20656f68a85f75befa0ee9b6cd75bfac3da5aca9c3f604",
    "https://deno.land/std@0.224.0/path/windows/normalize.ts": "78126170ab917f0ca355a9af9e65ad6bfa5be14d574c5fb09bb1920f52577780",
    "https://deno.land/std@0.224.0/path/windows/normalize_glob.ts": "9c87a829b6c0f445d03b3ecadc14492e2864c3ebb966f4cea41e98326e4435c6",
    "https://deno.land/std@0.224.0/path/windows/parse.ts": "08804327b0484d18ab4d6781742bf374976de662f8642e62a67e93346e759707",
    "https://deno.land/std@0.224.0/path/windows/relative.ts": "3e1abc7977ee6cc0db2730d1f9cb38be87b0ce4806759d271a70e4997fc638d7",
    "https://deno.land/std@0.224.0/path/windows/resolve.ts": "8dae1dadfed9d46ff46cc337c9525c0c7d959fb400a6308f34595c45bdca1972",
    "https://deno.land/std@0.224.0/path/windows/to_file_url.ts": "40e560ee4854fe5a3d4d12976cef2f4e8914125c81b11f1108e127934ced502e",
    "https://deno.land/std@0.224.0/path/windows/to_namespaced_path.ts": "4ffa4fb6fae321448d5fe810b3ca741d84df4d7897e61ee29be961a6aac89a4c",
    "https://deno.land/x/dotenv@v3.2.2/mod.ts": "077b48773de9205266a0b44c3c3a3c3083449ed64bb0b6cc461b95720678d38e",
    "https://deno.land/x/dotenv@v3.2.2/util.ts": "693730877b13f8ead2b79b2aa31e2a0652862f7dc0c5f6d2f313f4d39c7b7670"
  },
  "workspace": {
    "packageJson": {
      "dependencies": [
        "npm:@pinecone-database/pinecone@4",
        "npm:axios@^1.8.4",
        "npm:commander@13",
        "npm:cors@^2.8.5",
        "npm:csv-parse@^5.5.3",
        "npm:dotenv@^16.3.1",
        "npm:express-validator@^7.0.1",
        "npm:express@^4.18.2",
        "npm:inquirer@^12.3.0",
        "npm:jest@^29.7.0",
        "npm:joi@^17.13.3",
        "npm:marked@^11.2.0",
        "npm:mongoose@^8.0.3",
        "npm:multer@^1.4.5-lts.1",
        "npm:mysql2@^3.12.0",
        "npm:nodemon@^3.0.2",
        "npm:npm-run-all@^4.1.5",
        "npm:openai@^4.24.1",
        "npm:tree-kill@^1.2.2",
        "npm:uuid@^11.0.3",
        "npm:winston@^3.11.0"
      ]
    }
  }
}
</file>

<file path="scripts/deno-ui.js">
#!/usr/bin/env -S deno run --allow-net --allow-env --allow-read --allow-run

// Steps to run in development mode:
// 1. Set the desired port in the environment variable: PORT=3001
// 2. deno run --allow-net --allow-env --allow-read --allow-run --watch scripts/deno-ui.js

/**
 * Steps to create a standalone executable for Linux:
 * 
 * 1. Install Deno if not already installed:
 *    curl -fsSL https://deno.land/x/install/install.sh | sh
 * 
 * 2. Make the script executable:
 *    chmod +x deno-ui.js
 * 
 * 3. Compile to standalone executable:
 *    deno compile --allow-net --allow-env --allow-read --allow-run --target x86_64-unknown-linux-gnu --output csv-to-rag-ui deno-ui.js
 * 
 * 4. (Optional) Move to system bin for global access:
 *    sudo mv csv-to-rag-ui /usr/local/bin/
 * 
 * The executable will be created as 'csv-to-rag-ui' and can be run directly:
 * ./csv-to-rag-ui
 */

import { config } from "https://deno.land/x/dotenv/mod.ts";

// Load environment variables from .env file
const env = config();

// Set Deno.env variables
Deno.env.set('UI_BACKEND_URL', env.UI_BACKEND_URL || 'http://localhost:3000');
Deno.env.set('UI_USERNAME', env.CSVTORAG_BASIC_AUTH_USER||env.BASIC_AUTH_USER||env.UI_USERNAME || 'admin');
Deno.env.set('UI_PASSWORD', env.CSVTORAG_BASIC_AUTH_PASSWORD||env.BASIC_AUTH_PASSWORD||env.UI_PASSWORD || 'admin');
Deno.env.set('BACKEND_API_KEY', env.BACKEND_API_KEY || 'secret');


// Import required Deno modules
import { serve } from "https://deno.land/std@0.210.0/http/server.ts";
import { template } from "./deno-ui/ui.js";
import { join } from "https://deno.land/std@0.210.0/path/mod.ts";

console.log('Backend URL:', Deno.env.get('UI_BACKEND_URL'));

// Get port from environment variable or use default 3001
const port = parseInt(Deno.env.get('PORT') || "3001");

// Store backend process
let backendProcess = null;
let backendPid = null;

// Check if internal backend is available
async function checkBackendAvailable() {
  if (Deno.env.get("INTERNAL_BACKEND") === "0") {
    return false;
  }

  try {
    // Check if package.json exists and contains the dev script
    const packageJsonPath = join(Deno.cwd(), "package.json");
    try {
      const packageJson = JSON.parse(await Deno.readTextFile(packageJsonPath));
      return packageJson.scripts && packageJson.scripts.dev;
    } catch {
      return false;
    }
  } catch {
    return false;
  }

}

// Handle backend process management
async function startBackend() {
  if (backendProcess) {
    return { status: "error", message: "Backend is already running" };
  }

  try {
    const command = new Deno.Command("npm", {
      args: ["run", "dev"],
      cwd: Deno.cwd(),
      stdout: "piped",
      stderr: "piped"
    });

    backendProcess = command.spawn();
    backendPid = backendProcess.pid;

    // Handle process output
    (async () => {
      const decoder = new TextDecoder();
      for await (const chunk of backendProcess.stdout) {
        console.log(decoder.decode(chunk));
      }
    })();

    (async () => {
      const decoder = new TextDecoder();
      for await (const chunk of backendProcess.stderr) {
        console.error(decoder.decode(chunk));
      }
    })();

    // Wait a bit to check if process started successfully
    await new Promise(resolve => setTimeout(resolve, 2000));

    if (backendPid) {
      return { status: "success", message: "Backend started successfully" };
    } else {
      backendProcess = null;
      backendPid = null;
      return { status: "error", message: "Failed to start backend" };
    }
  } catch (error) {
    backendProcess = null;
    backendPid = null;
    return { status: "error", message: `Error starting backend: ${error.message}` };
  }
}

async function stopBackend() {
  if (!backendProcess || !backendPid) {
    return { status: "error", message: "Backend is not running" };
  }

  try {
    // Use tree-kill via node to kill the process tree
    const killCommand = new Deno.Command("node", {
      args: ["-e", `require('tree-kill')(${backendPid}, 'SIGTERM', err => process.exit(err ? 1 : 0))`],
    });
    await killCommand.output();

    backendProcess = null;
    backendPid = null;
    return { status: "success", message: "Backend stopped successfully" };
  } catch (error) {
    // Cleanup state even if there was an error
    backendProcess = null;
    backendPid = null;
    return { status: "error", message: `Error stopping backend: ${error.message}` };
  }
}

async function handler(req) {


  const url = new URL(req.url);

  // Decode and verify credentials for root route
  if (url.pathname === "/") {
    // Check basic authentication
    const authorization = req.headers.get('Authorization');
    if (!authorization) {
      return new Response('Unauthorized', {
        status: 401,
        headers: {
          'WWW-Authenticate': 'Basic realm="Restricted Access"',
        },
      });
    }
    const [scheme, encoded] = authorization.split(' ');
    if (!encoded || scheme !== 'Basic') {
      return new Response('Invalid authentication', { status: 401 });
    }

    const decoded = atob(encoded);
    const [username, password] = decoded.split(':');

    if (username !== Deno.env.get('UI_USERNAME') || password !== Deno.env.get('UI_PASSWORD')) {
      return new Response('Invalid credentials', { status: 401 });
    }
  }



  if (url.pathname === "/") {
    return new Response(template(), {
      headers: { "Content-Type": "text/html" },
    });
  }

  if (url.pathname === "/static/main.js") {
    try {
      let jsContent;
      //if (Deno.env.get("DEV") === "true") {
        // Development mode - load files individually
        const files = [];
        const appDir = join(Deno.cwd(), "scripts", "deno-ui", "app");
        for await (const entry of Deno.readDir(appDir)) {
          if (entry.isFile && entry.name.endsWith(".js")) {
            files.push(Deno.readTextFile(join(appDir, entry.name)));
          }
        }
        jsContent = (await Promise.all(files)).join("\n");
      /* } else {
        // Production mode - use embedded bundle
        jsContent = await Deno.readTextFile(
          new URL("./deno-ui/app-bundle.js", import.meta.url)
        );
      } */

      return new Response(jsContent, {
        headers: { "Content-Type": "application/javascript" },
      });
    } catch (error) {
      console.error(`Error loading JavaScript: ${error}`);
      return new Response("Error loading application files", { status: 500 });
    }
  }

  if (url.pathname === "/api/backend/state") {
    return new Response(JSON.stringify({
      status: backendProcess ? 'running' : 'stopped',
      message: backendProcess ? 'Backend is running' : 'Backend is stopped'
    }), {
      headers: { "Content-Type": "application/json" },
    });
  }

  if (url.pathname === "/api/backend/available") {
    const available = await checkBackendAvailable();
    return new Response(JSON.stringify({ available }), {
      headers: { "Content-Type": "application/json" }
    });
  }

  if (url.pathname === "/api/backend/start") {
    const result = await startBackend();
    return new Response(JSON.stringify(result), {
      headers: { "Content-Type": "application/json" },
    });
  }

  if (url.pathname === "/api/backend/stop") {
    const result = await stopBackend();
    return new Response(JSON.stringify(result), {
      headers: { "Content-Type": "application/json" },
    });
  }

  return new Response("Not Found", { status: 404 });
}

// Start the server
console.log(`UI Server running at http://localhost:${port}`);
await serve(handler, { port });
</file>

<file path=".gitignore">
tmp

# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
lerna-debug.log*
csv-to-rag-ui
./build/*
./scripts/deno-ui/*bundle*
.pnpm-debug.log*
*.csv
citipav*
# Diagnostic reports (https://nodejs.org/api/report.html)
report.[0-9]*.[0-9]*.[0-9]*.[0-9]*.json
.env*
!.env.example
# Runtime data
pids
*.pid
*.seed
*.pid.lock

# Directory for instrumented libs generated by jscoverage/JSCover
lib-cov

# Coverage directory used by tools like istanbul
coverage
*.lcov

# nyc test coverage
.nyc_output

# Grunt intermediate storage (https://gruntjs.com/creating-plugins#storing-task-files)
.grunt

# Bower dependency directory (https://bower.io/)
bower_components

# node-waf configuration
.lock-wscript

# Compiled binary addons (https://nodejs.org/api/addons.html)
build/Release

# Dependency directories
node_modules/
jspm_packages/

# Snowpack dependency directory (https://snowpack.dev/)
web_modules/

# TypeScript cache
*.tsbuildinfo

# Optional npm cache directory
.npm

# Optional eslint cache
.eslintcache

# Optional stylelint cache
.stylelintcache

# Microbundle cache
.rpt2_cache/
.rts2_cache_cjs/
.rts2_cache_es/
.rts2_cache_umd/

# Optional REPL history
.node_repl_history

# Output of 'npm pack'
*.tgz

# Yarn Integrity file
.yarn-integrity

# dotenv environment variable files
.env
.env.development.local
.env.test.local
.env.production.local
.env.local

# parcel-bundler cache (https://parceljs.org/)
.cache
.parcel-cache

# Next.js build output
.next
out

# Nuxt.js build / generate output
.nuxt
dist

# Gatsby files
.cache/
# Comment in the public line in if your project uses Gatsby and not Next.js
# https://nextjs.org/blog/next-9-1#public-directory-support
# public

# vuepress build output
.vuepress/dist

# vuepress v2.x temp and cache directory
.temp
.cache

# Docusaurus cache and generated files
.docusaurus

# Serverless directories
.serverless/

# FuseBox cache
.fusebox/

# DynamoDB Local files
.dynamodb/

# TernJS port file
.tern-port

# Stores VSCode versions used for testing VSCode extensions
.vscode-test

# yarn v2
.yarn/cache
.yarn/unplugged
.yarn/build-state.yml
.yarn/install-state.gz
.pnp.*
</file>

<file path="README.md">
# csv-to-rag

## Project Overview
This Node.js application implements a Retrieval-Augmented Generation (RAG) system that processes CSV files and integrates with Pinecone and MongoDB for data storage and retrieval.

## Data Processing
- Accepts CSV files with the following columns:
  - `code`: Unique identifier
  - `metadata_small`: Small metadata string
  - `metadata_big_1`: JSON-stringified data
  - `metadata_big_2`: JSON-stringified data
  - `metadata_big_3`: JSON-stringified data

## Database Integration
- Stores document embeddings in Pinecone vector database.
- Stores complete records in MongoDB with the following schema:
  - `code`: String (unique identifier)
  - `metadata_small`: String
  - `metadata_big_1`: JSON
  - `metadata_big_2`: JSON
  - `metadata_big_3`: JSON
  - `timestamp`: Date

## API Endpoints

### Upload CSV File
- **POST** `/csv/upload`
- **Description**: Upload and process new CSV files.
- **Curl Example**:
  ```bash
  curl -X POST http://localhost:3000/csv/upload \
    -F "file=@/home/jarancibia/Documents/repos/csv-to-rag/mysql-schemas.csv"
  ```

### List Processed CSV Files
- **GET** `/csv/list`
- **Description**: Retrieve a list of processed CSV files.
- **Curl Example**:
  ```bash
  curl -X GET http://localhost:3000/csv/list
  ```

### Update CSV Data 
- **PUT** `/csv/update/:id`
- **Description**: Update existing CSV data.
- **Curl Example**:
  ```bash
  curl -X PUT http://localhost:3000/csv/update/12345 \
    -H "Content-Type: application/json" \
    -d '{"metadata_small": "Updated value"}'
  ```

### Delete CSV Data
- **DELETE** `/csv/delete/:id`
- **Description**: Remove CSV data.
- **Curl Example**:
  ```bash
  curl -X DELETE http://localhost:3000/csv/delete/12345
  ```

### Perform Similarity Search
- **POST** `/query`
- **Description**: Perform similarity search and LLM interaction.
- **Curl Example**:
  ```bash
  curl -X POST http://localhost:3000/query \
    -H "Content-Type: application/json" \
    -d '{"query": "Your search query here"}'
  ```

### OpenAI-like Completion
- **POST** `/completion`
- **Description**: Perform Retrieval-Augmented Generation (RAG) with OpenAI-like API format.
- **Curl Example**:
  ```bash
  curl -X POST \
     -H "Content-Type: application/json" \
     -H "Authorization: Bearer api-key" \
     -d '{
       "prompt": "What fields for an user ?",
       "max_tokens": 60,
       "temperature": 0.8
     }' \
     http://localhost:3000/api/completion
  ```

## Environment Configuration
- `OPENAI_API_KEY`: OpenAI compatible API authentication
- `OPENAI_MODEL`: available model
- `OPENAI_MODEL_FALLBACK`: fallback model
- `MONGODB_URI`: MongoDB connection string
- `PINECONE_API_KEY`: Pinecone API key
- `PINECONE_INDEX`: Pinecone index name
- `LLM_SYSTEM_PROMPT`: Customizable system prompt for LLM interactions

## Error Handling
Comprehensive error handling is implemented for all API requests to ensure proper feedback and logging.

## UI

There is a standalone UI that can be run with the following command:

```bash
deno run --allow-net --allow-env --allow-read --allow-run --watch scripts/deno-ui.js 
```

Note: Requires Deno to be installed.

## Docker image 

```bash
docker build -t javimosch/csv-to-rag-backend:1.2 -f Dockerfile.backend .
```

## Namespaces (NEW)

The recommended way to upload data is to use namespaces in the Upload route or by using CLI (db-health.js)

## Upload data using CLI

```bash
node ./scripts/db-health.js --repair --file fileName.csv --auto --ns fileName
```

---

## Documentation Index

For more details and advanced usage, see the following documentation in the `/docs` folder:

- [Database Health Check and Repair Tool](docs/db-health.md): Health checks, repair, and synchronization between MongoDB and Pinecone.
- [CSV Upload Process Flow](docs/process-flow.md): Step-by-step flow of CSV upload, validation, and embedding.
- [Generic CSV Format](docs/generic-csv-format.md): CSV schema and formatting requirements.
- [List Route](docs/list-route.md): Details on the `/csv/list` API route and its behavior.
- [Markdown to CSV Conversion](docs/md-to-csv.md): How to convert markdown files to CSV for ingestion.
- [OpenAI-like API](docs/openai-like-api.md): Using the OpenAI-compatible API endpoints.
- [Chunking Strategy for Docs](docs/chunking-strategy-for-docs.md): How documents are chunked for embedding and search.
- [UI Namespace Requirement](docs/ui-namespace-required.md): UI changes and requirements for namespace during file upload.

For troubleshooting, best practices, and more, consult the relevant doc above.
</file>

<file path="scripts/db-health.js">
import mongoose from 'mongoose';
import { config } from 'dotenv';
import { fileURLToPath } from 'url';
import { dirname, join } from 'path';
import { Pinecone } from '@pinecone-database/pinecone';
import { Document } from '../src/models/document.model.js';
import { logger } from '../src/utils/logger.js';
import fs from 'fs';
import { parse } from 'csv-parse';
import readline from 'readline';
import { embedDocument } from '../src/services/embedding.service.js';

function isBase64(str) {
    try {
        // Check if the string matches base64 pattern
        if (!/^[A-Za-z0-9+/=]+$/.test(str)) return false;
        
        // Try to decode and check if it's valid UTF-8
        const decoded = Buffer.from(str, 'base64').toString('utf-8');
        return true;
    } catch (e) {
        return false;
    }
}

function decodeBase64IfNeeded(value) {
    if (!value) return '';
    if (isBase64(value)) {
        try {
            return Buffer.from(value, 'base64').toString('utf-8');
        } catch (e) {
            logger.warn('Failed to decode base64 value:', { value, error: e.message });
            return value;
        }
    }
    return value;
}

// Load environment variables
const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);
const envPath = join(__dirname, '../.env');
config({ path: envPath });

async function parseCsvFile(filePath) {
    try {
        const fileContent = fs.readFileSync(filePath, 'utf-8');

        return new Promise((resolve, reject) => {
            // Using csv-parse library: https://csv.js.org/parse/
            parse(fileContent, {
                delimiter: ';',
                columns: true,
                skip_empty_lines: true,
                trim: true,
                relax_column_count: true,  // Allow flexible column count for JSON
                quote: false,              // Disable quote parsing to handle JSON
                comment: '#'               // Allow comments starting with #
            }, (err, records) => {
                if (err) {
                    reject(err);
                    return;
                }
                
                // Decode base64 metadata fields
                const parsedRecords = records.map(record => ({
                    ...record,
                    metadata_small: decodeBase64IfNeeded(record.metadata_small || ""),
                    metadata_big_1: decodeBase64IfNeeded(record.metadata_big_1 || ""),
                    metadata_big_2: decodeBase64IfNeeded(record.metadata_big_2 || ""),
                    metadata_big_3: decodeBase64IfNeeded(record.metadata_big_3 || "")
                }));

                resolve(parsedRecords);
            });
        });
    } catch (error) {
        logger.error('Error parsing CSV file:', {
            message: error.message,
            stack: error.stack
        });
        throw error;
    }
}

async function connectToMongoDB() {
    try {
        const mongoUri = process.env.MONGODB_URI;
        if (!mongoUri) {
            throw new Error('MONGODB_URI environment variable is not set');
        }

        logger.info('Connecting to MongoDB...');
        await mongoose.connect(mongoUri, {
            useNewUrlParser: true,
            useUnifiedTopology: true,
            serverSelectionTimeoutMS: 5000, // Timeout after 5 seconds
            socketTimeoutMS: 45000, // Close sockets after 45 seconds of inactivity
        });
        logger.info('Connected to MongoDB');
    } catch (error) {
        logger.error('Error connecting to MongoDB:', {
            message: error.message,
            stack: error.stack
        });
        throw error;
    }
}

async function initPinecone() {
    try {
        const pinecone = new Pinecone({
            apiKey: process.env.PINECONE_API_KEY
        });

        const index = pinecone.index(process.env.PINECONE_INDEX);
        logger.info('Initialized Pinecone client', 'initPinecone');

        return index;
    } catch (error) {
        logger.error('Error initializing Pinecone:', error);
        throw error;
    }
}


// Cache vector dimension after first embedding
const VECTOR_DIM = parseInt(process.env.VECTOR_DIM || '1536', 10);

async function getVectorDimension() {
    return VECTOR_DIM;
}

async function upsertVectors(pineconeIndex, vectors) {
    try {
        logger.debug(`Upserting ${vectors.length} vectors`);

        // Ensure we have valid vectors
        const validVectors = vectors.filter(vector => {
            if (!vector.id || !vector.values || !vector.metadata) {
                logger.warn(`Invalid vector format for id: ${vector.id}`);
                return false;
            }
            // Log metadata
            //logger.debug(`Vector ${vector.id} metadata:`, vector.metadata);
            return true;
        });

        if (validVectors.length === 0) {
            logger.warn('No valid vectors to upsert');
            return false;
        }

        // Format vectors for Pinecone API
        const formattedVectors = validVectors.map(vector => ({
            id: vector.id,
            values: Array.from(vector.values), // Ensure it's a proper array
            metadata: vector.metadata
        }));

        // Log first vector for debugging
        //logger.debug('First formatted vector:', formattedVectors[0]);

        // Batch upsert to Pinecone
        await pineconeIndex.upsert(formattedVectors);

        return true;
    } catch (error) {
        logger.error('Error upserting vectors:', {
            message: error.message,
            stack: error.stack
        });
        throw error; // Re-throw to abort the process
    }
}

async function fetchVectorByCode(pineconeIndex, code, namespace = 'default') {
    try {
        // Get vector dimension for zero vector
        const dim = await getVectorDimension();
        const zeroVector = new Array(dim).fill(0);

        // Query by code with zero vector
        const response = await pineconeIndex.namespace(namespace).query({
            vector: zeroVector,
            filter: { code },
            topK: 1,
            includeMetadata: true
        });

        if (!response.matches || response.matches.length === 0) {
            return null;
        }

        return response.matches[0];
    } catch (error) {
        logger.error('Error fetching vector by code:', {
            code,
            namespace,
            message: error.message,
            stack: error.stack
        });
        throw error;
    }
}

async function promptUser(message) {
    const rl = readline.createInterface({
        input: process.stdin,
        output: process.stdout
    });

    return new Promise(resolve => {
        rl.question(message + ' (y/N): ', answer => {
            rl.close();
            resolve(answer.toLowerCase() === 'y');
        });
    });
}

async function repairMetadata(pineconeIndex, csvPath, options = {
    auto: false,
    code: null,
    namespace: 'default',
    repairMongoOnly: false
}) {
    try {
        // Extract fileName from csvPath
        const fileName = csvPath.split('/').pop();
        logger.info(`Using fileName: ${fileName}`);

        // Parse CSV file
        const records = await parseCsvFile(csvPath);
        if (!records || records.length === 0) {
            throw new Error('No records found in CSV file');
        }
        logger.info(`Found ${records.length} records in CSV file`);

        // If code is provided, filter records to only repair the specified row
        let recordsToProcess = records;
        if (options.code) {
            recordsToProcess = records.filter(record => record.code.toLowerCase().includes(options.code.toLowerCase()));
            if (recordsToProcess.length === 0) {
                logger.error(`No records found with code containing: ${options.code} in CSV file`);
                return;
            }
            logger.info(`Found ${recordsToProcess.length} records with code containing: ${options.code} in CSV file`);
        }

        // Calculate dynamic batch size for max 5 batches
        const batchSize = options.code ? 1 : Math.max(10, Math.ceil(records.length / 5));
        logger.info(`Using batch size: ${batchSize}`);

        // Process records in batches
        const batches = [];
        for (let i = 0; i < recordsToProcess.length; i += batchSize) {
            batches.push(recordsToProcess.slice(i, i + batchSize));
        }

        logger.info(`Processing ${batches.length} batches`);

        for (const [index, batch] of batches.entries()) {
            try {
                logger.info(`Processing batch ${index + 1}/${batches.length} (${batch.length} records)`);
                await processBatch(pineconeIndex, batch.map(record => ({ ...record, fileName })), options.auto, options.namespace, options.repairMongoOnly);
            } catch (error) {
                logger.error(`Error processing batch ${index + 1}:`, error);
                throw error;
            }
        }

        logger.info('Metadata repair completed successfully');
    } catch (error) {
        logger.error('Error repairing metadata:', {
            message: error.message,
            stack: error.stack
        });
        throw error;
    }
}



async function processBatch(pineconeIndex, batch, isAuto = false, namespace = 'default', repairMongoOnly = false) {
    try {
        // Display the batch
        console.log('Repair', batch.length, 'records?');
        console.log('Sample:', JSON.stringify(batch[0], null, 2));

        let shouldProceed = isAuto;
        if (!isAuto) {
            const answer = await promptUser('Proceed with repair? (y/n): ');
            shouldProceed = answer.toLowerCase() === 'y';
        }

        if (!shouldProceed) {
            console.log('Skipping batch');
            return;
        }

        logger.info(`Processing batch of ${batch.length} records`);

        // Process in smaller parallel batches to avoid overwhelming the system
        const PARALLEL_BATCH_SIZE = 10;
        for (let i = 0; i < batch.length; i += PARALLEL_BATCH_SIZE) {
            const currentBatch = batch.slice(i, i + PARALLEL_BATCH_SIZE);
            logger.info(`Processing sub-batch ${Math.floor(i/PARALLEL_BATCH_SIZE) + 1}/${Math.ceil(batch.length/PARALLEL_BATCH_SIZE)} (${currentBatch.length} records)`);

            // Process each record in the current batch in parallel
            for (let j = 0; j < currentBatch.length; j += PARALLEL_BATCH_SIZE) {
                const parallelBatch = currentBatch.slice(j, j + PARALLEL_BATCH_SIZE);

                // Fetch vectors for current batch
                const vectorPromises = parallelBatch.map(record => fetchVectorByCode(pineconeIndex, record.code, namespace));
                const existingVectors = await Promise.all(vectorPromises);

                const parallelVectors = existingVectors.slice(j, j + PARALLEL_BATCH_SIZE);

                // Process each record in parallel
                await Promise.all(parallelBatch.map(async (record, index) => {
                    const existingVector = parallelVectors[index];
                    try {
                        // Prepare document data
                        const documentData = {
                            code: record.code,
                            namespace: namespace,
                            fileName: record.fileName,
                            metadata_small: record.metadata_small,
                            metadata_big_1: record.metadata_big_1,
                            metadata_big_2: record.metadata_big_2,
                            metadata_big_3: record.metadata_big_3
                        };

                        // Try to find and update/create document
                        let doc;
                        try {
                            doc = await Document.findOneAndUpdate(
                                { code: record.code, namespace: namespace },
                                documentData,
                                { upsert: true, new: true }
                            );
                        } catch (error) {
                            if (error.code === 11000) {
                                logger.warn(`Duplicate key detected for code: ${record.code}. Attempting to resolve...`);
                                
                                try {
                                    // Find the existing document
                                    const existingDoc = await Document.findOne({ code: record.code });
                                    
                                    if (existingDoc) {
                                        if (existingDoc.namespace === namespace) {
                                            // Update the existing document
                                            doc = await Document.findOneAndUpdate(
                                                { _id: existingDoc._id },
                                                documentData,
                                                { new: true }
                                            );
                                            logger.info(`Updated existing document for code: ${record.code} in namespace: ${namespace}`);
                                        } else {
                                            // Document exists in a different namespace, create new one
                                            await Document.deleteOne({ code: record.code, namespace: namespace });
                                            doc = await Document.create(documentData);
                                            logger.info(`Created new document for code: ${record.code} in namespace: ${namespace}`);
                                        }
                                    } else {
                                        // No document found, try creating again
                                        doc = await Document.create(documentData);
                                        logger.info(`Created new document for code: ${record.code} in namespace: ${namespace}`);
                                    }
                                } catch (retryError) {
                                    logger.error(`Failed to resolve duplicate key for code: ${record.code}`, {
                                        error: retryError.message,
                                        stack: retryError.stack
                                    });
                                    throw retryError;
                                }
                            } else {
                                throw error;
                            }
                        }

                        // Skip Pinecone operations if repairMongoOnly is true
                        if (!repairMongoOnly) {
                            // Get vector for the document
                            const embedding = await embedDocument(record.metadata_small);
                            if (!embedding) {
                                logger.error('Failed to get embedding for document:', {
                                    code: record.code,
                                    metadata_small: record.metadata_small
                                });
                                return;
                            }

                            // Prepare metadata
                            const metadata = {
                                fileName: record.fileName,
                                code: record.code,
                                metadata_small: record.metadata_small
                            };

                            // Update or create vector
                            if (existingVector) {
                                logger.info('Updating existing vector:', {
                                    id: existingVector.id,
                                    code: record.code,
                                    oldMetadata: existingVector.metadata,
                                    newMetadata: metadata
                                });

                                await pineconeIndex.namespace(namespace).update({
                                    id: existingVector.id,
                                    metadata
                                });
                            } else {
                                logger.info('Creating new vector:', {
                                    code: record.code,
                                    metadata
                                });

                                await pineconeIndex.namespace(namespace).upsert([{
                                    id: record.code,
                                    values: embedding,
                                    metadata
                                }]);
                            }
                        }
                    } catch (error) {
                        logger.error(`Error processing record:`, {
                            record,
                            error: error.message,
                            stack: error.stack
                        });
                        throw error;
                    }
                }));

                // Add small delay between parallel batches to avoid rate limiting
                await new Promise(resolve => setTimeout(resolve, 100));
            }
        }

        logger.info('Batch processing completed successfully');
    } catch (error) {
        logger.error('Error processing batch:', {
            error: error.message,
            stack: error.stack,
            batchSize: batch.length
        });
        throw error;
    }
}

async function getMongoHealth(namespace = 'default') {
    try {
        // Assumes MongoDB connection is already established

        // Get document counts by fileName
        const documentCounts = await Document.aggregate([
            {
                $group: {
                    _id: '$fileName',
                    count: { $sum: 1 }
                }
            }
        ]);

        // Create a map for easier access
        const countMap = new Map();
        documentCounts.forEach(doc => {
            countMap.set(doc._id, doc.count);
        });

        return countMap;
    } catch (error) {
        logger.error('Error getting MongoDB health:', error);
        throw error;
    }
}

async function getPineconeHealth(pineconeIndex) {
    try {
        // Using existing Pinecone client

        // Get overall stats for all namespaces
        const stats = await pineconeIndex.describeIndexStats();
        logger.debug('Pinecone stats:', stats);

        // Ensure we have namespaces data
        if (!stats?.namespaces) {
            logger.warn('No namespaces found in Pinecone stats');
            return {};
        }

        const namespaceStats = {};
        
        // Process each namespace
        for (const [namespace, nsStats] of Object.entries(stats.namespaces)) {
            try {
                const vectorsByFile = new Map();
                const uniqueFileNames = (await Document.distinct('fileName', { namespace })) || [];

                // Process each file in the namespace
                for (const fileName of uniqueFileNames) {
                    try {
                        const codes = await Document.find({ fileName, namespace }).distinct('code');
                        const dim = await getVectorDimension();
                        const zeroVector = new Array(dim).fill(0);

                        const response = await pineconeIndex.namespace(namespace).query({
                            vector: zeroVector,
                            filter: { fileName },
                            topK: codes.length,
                            includeMetadata: true
                        });

                        const foundVectors = response.matches?.length || 0;
                        vectorsByFile.set(fileName, foundVectors);
                    } catch (error) {
                        logger.error(`Error processing file ${fileName} in namespace ${namespace}:`, error);
                        continue;
                    }
                }

                namespaceStats[namespace] = {
                    totalVectors: nsStats.vectorCount || 0,
                    vectorsWithFileName: Array.from(vectorsByFile.values()).reduce((a, b) => a + b, 0),
                    orphanedVectors: (nsStats.vectorCount || 0) - Array.from(vectorsByFile.values()).reduce((a, b) => a + b, 0),
                    vectorsByFile: Object.fromEntries(vectorsByFile)
                };
            } catch (error) {
                logger.error(`Error processing namespace ${namespace}:`, error);
                continue;
            }
        }

        return namespaceStats || {};
    } catch (error) {
        logger.error('Error getting Pinecone health:', error);
        throw error;
    }
} 


async function logExtraMongoDocuments(pineconeIndex, fileName, namespace = 'default') {
    try {
        // Get all documents for this file
        const mongoDocs = await Document.find({ fileName, namespace }).lean();
        
        // Get vector dimension for zero vector
        const dim = await getVectorDimension();
        const zeroVector = new Array(dim).fill(0);
        
        // Using existing Pinecone client
        
        // Create a log file for extra documents
        const logStream = fs.createWriteStream('repair.log', { flags: 'a' });
        logStream.write(`\n=== Extra MongoDB Documents for ${fileName} ===\n\n`);
        
        // Check each MongoDB document
        for (const doc of mongoDocs) {
            // Query Pinecone for this document
            const response = await pineconeIndex.namespace(namespace).query({
                vector: zeroVector,
                filter: { code: doc.code },
                topK: 1,
                includeMetadata: true
            });
            
            // If no matching vector in Pinecone, log it
            if (!response.matches || response.matches.length === 0) {
                logStream.write(JSON.stringify({
                    code: doc.code,
                    fileName: doc.fileName,
                    metadata_small: doc.metadata_small
                }, null, 2) + '\n');
            }
            
            // Add small delay to avoid rate limiting
            await new Promise(resolve => setTimeout(resolve, 100));
        }
        
        logStream.end();
        logger.info(`Extra MongoDB documents logged to mongo_extras.log`);
    } catch (error) {
        logger.error('Error logging extra MongoDB documents:', error);
        throw error;
    }
}

async function runHealthCheck(pineconeIndex, namespace = 'default') {
    try {
        logger.info('Running health check...');
        // Get health information from both systems
        const [mongoHealth, pineconeHealth] = await Promise.all([
            getMongoHealth(namespace),
            getPineconeHealth(pineconeIndex)
        ]);

        // 📊 Health Check Results 📊
        console.log('\n📊  Health Check Results  📊\n');

        // 🗄️  MongoDB Document Counts
        console.log('🗄️  MongoDB Document Counts:');
        mongoHealth.forEach((count, fileName) => {
            console.log(`   • ${fileName}: ${count.toLocaleString()} documents`);
        });

        // 🤖  Pinecone Vector Counts by File
        console.log('\n🤖  Pinecone Vector Counts by File:');
        // Aggregate counts and namespaces per file
        const fileNsMap = new Map();
        Object.entries(pineconeHealth).forEach(([ns, data]) => {
            Object.entries(data.vectorsByFile || {}).forEach(([file, cnt]) => {
                if (!fileNsMap.has(file)) fileNsMap.set(file, {});
                fileNsMap.get(file)[ns] = cnt;
            });
        });
        // Print per file summary
        fileNsMap.forEach((nsCounts, file) => {
            const total = Object.values(nsCounts).reduce((a, b) => a + b, 0);
            const details = Object.entries(nsCounts)
                .map(([ns, cnt]) => `${ns}: ${cnt}`)
                .join(', ');
            console.log(`   • ${file}: ${total.toLocaleString()} vectors  [${details}]`);
        });

        if (!pineconeHealth || typeof pineconeHealth !== 'object') {
            logger.error('Pinecone health data is not valid:', pineconeHealth);
            return;
        }


        // Raw per-namespace summary omitted; see above per-file summary

        // Check for discrepancies
        // ⚠️  Discrepancies ⚠️
        console.log('\n⚠️  Discrepancies  ⚠️');
        let hasDiscrepancies = false;
        const discrepancyFiles = [];

        // Aggregate Pinecone vector counts by file across all namespaces
        const pineconeCountMap = new Map();
        Object.values(pineconeHealth).forEach(healthData => {
            Object.entries(healthData.vectorsByFile || {}).forEach(([fileName, count]) => {
                pineconeCountMap.set(fileName, (pineconeCountMap.get(fileName) || 0) + count);
            });
        });

        const mongoCountMap = mongoHealth;
        // Compare counts per file
        mongoCountMap.forEach((mongoCount, fileName) => {
            const pineconeCount = pineconeCountMap.get(fileName) || 0;
            if (mongoCount !== pineconeCount) {
                hasDiscrepancies = true;
                const diff = mongoCount - pineconeCount;
                console.log(`   • ${fileName}: MongoDB=${mongoCount.toLocaleString()}, Pinecone=${pineconeCount.toLocaleString()} -> Δ=${diff.toLocaleString()}`);
                if (diff > 0) discrepancyFiles.push(fileName);
            }
        });

        if (!hasDiscrepancies) {
            console.log('   ✅ No discrepancies found. All counts match!');
        }

        // 🏁  Overall Health Status 🏁
        console.log('\n🏁  Overall Health Status  🏁');

        const totalMongo = Array.from(mongoCountMap.values()).reduce((a, b) => a + b, 0);
        const totalPinecone = Array.from(pineconeCountMap.values()).reduce((a, b) => a + b, 0);

        if (totalMongo === totalPinecone && !hasDiscrepancies) {
            console.log('   ✅ System is healthy! All document counts match.');
        } else {
            console.log('   ❌ System needs attention!');
            if (totalMongo !== totalPinecone) {
                console.log(`     • Total mismatch: MongoDB=${totalMongo.toLocaleString()} vs Pinecone=${totalPinecone.toLocaleString()}`);
            }
            if (hasDiscrepancies) {
                console.log('     • File-level discrepancies detected above.');
            }
        }

        // Log extra documents for all files with discrepancies
        if (discrepancyFiles.length > 0) {
            logger.info('Logging extra MongoDB documents...');
            for (const fileName of discrepancyFiles) {
                await logExtraMongoDocuments(pineconeIndex, fileName, namespace);
            }
        }

    } catch (error) {
        logger.error('Error running health check:', error);
        throw error;
    }
}

async function initServices() {
    // Connect to MongoDB first
    await connectToMongoDB();
    logger.info('MongoDB connection ready');

    // Initialize Pinecone client
    const pineconeIndex = await initPinecone();
    if (!pineconeIndex) {
        throw new Error('Failed to initialize Pinecone client');
    }
    logger.info('Pinecone client ready');

    return pineconeIndex;
}

async function closeServices() {
    // Clean up MongoDB connection
    await mongoose.disconnect();
    logger.info('MongoDB connection closed');
}

async function getOrphanVectors(pineconeIndex, namespace = 'default') {
    try {
        logger.info('Finding orphan vectors...');
        
        // Get vector dimension for zero vector
        const dim = await getVectorDimension();
        const zeroVector = new Array(dim).fill(0);
        
        // Query all vectors
        const response = await pineconeIndex.namespace(namespace).query({
            vector: zeroVector,
            topK: 10000, // Adjust this value based on your needs
            includeMetadata: true
        });

        if (!response.matches) {
            logger.warn('No vectors found in Pinecone');
            return [];
        }

        // Filter vectors that don't have fileName in metadata
        return response.matches.filter(vector => !vector.metadata?.fileName);
    } catch (error) {
        logger.error('Error getting orphan vectors:', {
            namespace,
            message: error.message,
            stack: error.stack
        });
        throw error;
    }
}

async function removeOrphanVectors(pineconeIndex, namespace = 'default') {
    try {
        logger.info('Starting orphan vector removal...');

        // Get orphan vectors
        const orphanVectors = await getOrphanVectors(pineconeIndex, namespace);
        if (orphanVectors.length === 0) {
            logger.info('No orphan vectors found');
            return;
        }

        // Confirm deletion
        console.log(`Found ${orphanVectors.length} orphan vectors to remove:`);
        for (const vector of orphanVectors.slice(0, 5)) {
            console.log(`- ${vector.id}`);
        }
        if (orphanVectors.length > 5) {
            console.log(`... and ${orphanVectors.length - 5} more`);
        }

        const answer = await promptUser('Remove these vectors? (y/N): ');
        if (answer.toLowerCase() !== 'y') {
            logger.info('Aborting vector removal');
            return;
        }

        // Delete vectors in batches
        const BATCH_SIZE = 100;
        for (let i = 0; i < orphanVectors.length; i += BATCH_SIZE) {
            const batch = orphanVectors.slice(i, i + BATCH_SIZE);
            const ids = batch.map(v => v.id);

            logger.info(`Deleting batch ${Math.floor(i/BATCH_SIZE) + 1}/${Math.ceil(orphanVectors.length/BATCH_SIZE)} (${ids.length} vectors)`);
            
            try {
                await pineconeIndex.namespace(namespace).deleteMany(ids);
                logger.info(`Successfully deleted ${ids.length} vectors`);
            } catch (error) {
                logger.error(`Error deleting batch:`, {
                    message: error.message,
                    stack: error.stack
                });
                // Continue with next batch
            }

            // Add small delay between batches
            await new Promise(resolve => setTimeout(resolve, 100));
        }

        logger.info('Orphan vector removal completed');
    } catch (error) {
        logger.error('Error removing orphan vectors:', {
            message: error.message,
            stack: error.stack
        });
        throw error;
    }
}

async function checkSingleRowHealth(pineconeIndex, code, namespace = 'default') {
    try {
        logger.info(`Checking health for document with code: ${code}`);

        // Get document from MongoDB
        const mongoDoc = await Document.findOne({ code, namespace });
        
        // Get vector from Pinecone
        const pineconeVector = await fetchVectorByCode(pineconeIndex, code, namespace);

        console.log('\n=== Single Row Health Check Results ===\n');
        
        if (!mongoDoc && !pineconeVector) {
            console.log(`❌ Document with code '${code}' not found in either MongoDB or Pinecone`);
            return;
        }

        // Check MongoDB
        console.log('MongoDB Status:');
        if (mongoDoc) {
            console.log('  ✅ Document exists');
            console.log(`  File Name: ${mongoDoc.fileName}`);
            console.log(`  Code: ${mongoDoc.code}`);
        } else {
            console.log('  ❌ Document not found');
        }

        // Check Pinecone
        console.log('\nPinecone Status:');
        if (pineconeVector) {
            console.log('  ✅ Vector exists');
            console.log(`  File Name: ${pineconeVector.metadata?.fileName || 'Not set'}`);
            console.log(`  Code: ${pineconeVector.metadata?.code || 'Not set'}`);
        } else {
            console.log('  ❌ Vector not found');
        }

        // Overall health assessment
        console.log('\nHealth Assessment:');
        if (mongoDoc && pineconeVector) {
            const metadataMatch = mongoDoc.fileName === pineconeVector.metadata?.fileName;
            if (metadataMatch) {
                console.log('✅ Row is healthy - exists in both systems with matching metadata');
            } else {
                console.log('⚠️  Row exists in both systems but metadata differs:');
                console.log(`  MongoDB fileName: ${mongoDoc.fileName}`);
                console.log(`  Pinecone fileName: ${pineconeVector.metadata?.fileName}`);
            }
        } else {
            console.log('❌ Row is unhealthy - missing from one system:');
            if (mongoDoc) console.log('  - Exists in MongoDB but missing from Pinecone');
            if (pineconeVector) console.log('  - Exists in Pinecone but missing from MongoDB');
        }

    } catch (error) {
        logger.error('Error checking single row health:', {
            message: error.message,
            stack: error.stack,
            code
        });
        throw error;
    }
}

async function main() {
    try {
        // Parse command line arguments
        const args = process.argv.slice(2);
        const shouldRepair = args.includes('--repair');
        const isAuto = args.includes('--auto');
        const removeOrphans = args.includes('--remove-orphan-vectors');
        const repairMongoOnly = args.includes('--repair-mongo-only');

        // Find file path - support both --file=path and --file path formats
        const fileArg = args.find(arg => arg.startsWith('--file='));
        const filePath = fileArg ? 
            fileArg.split('=')[1] : 
            args[args.indexOf('--file') + 1];

        // Find code for single row check
        const codeArg = args.find(arg => arg.startsWith('--code='));
        const code = codeArg ? codeArg.split('=')[1] : null;

        // Find namespace - support both --ns=namespace and --namespace=namespace formats
        const nsArg = args.find(arg => arg.startsWith('--ns=') || arg.startsWith('--namespace='));
        const namespace = nsArg ? nsArg.split('=')[1] : 'default';

        logger.info('Starting with options:', {
            repair: shouldRepair,
            auto: isAuto,
            removeOrphans,
            file: filePath || 'none',
            code: code || 'none',
            namespace
        });

        // Initialize services
        const pineconeIndex = await initServices();

        if (shouldRepair) {
            if (!filePath) {
                logger.error('--file argument is required when using --repair');
                console.log('Usage: node db-health.js [--repair --file=<path/to/csv> [--auto]] [--remove-orphan-vectors] [--code=<document-code>] [--ns=<namespace>]');
                return;
            }
            logger.info(`Starting repair with auto mode: ${isAuto}, namespace: ${namespace}`);
            await repairMetadata(pineconeIndex, filePath, { auto: isAuto, code, namespace, repairMongoOnly });
        } else if (code) {
            // If code is provided without repair, do single row health check
            await checkSingleRowHealth(pineconeIndex, code, namespace);
        } else if (removeOrphans) {
            await removeOrphanVectors(pineconeIndex, namespace);
        } else {
            await runHealthCheck(pineconeIndex, namespace);
        }
    } catch (error) {
        logger.error('Error in main:', {
            message: error.message,
            stack: error.stack
        });
        process.exit(1);
    } finally {
        await closeServices();
    }
}

// Run the script
main();
</file>

<file path="src/routes/csv.routes.js">
import express from 'express';
import multer from 'multer';
import { CSVService } from '../services/csv.service.js';
import { validateCsv } from '../middleware/validation.middleware.js';
import { logger } from '../utils/logger.js';
import { Document } from '../models/document.model.js';

const router = express.Router();
const upload = multer({ 
  storage: multer.memoryStorage(),
  limits: {
    fileSize: 10 * 1024 * 1024, // 10 MB limit
    files: 1, // 1 file
  },
});

router.post('/upload', upload.single('csvFile'), validateCsv, async (req, res, next) => {
  try {
    if (!req.file) {
      return res.status(400).json({ error: 'No file uploaded' });
    }

    const namespace = req.query.namespace || 'default';
    // Prevent uploading if a CSV with the same filename and namespace already exists
    const existing = await Document.findOne({ fileName: req.file.originalname, namespace });
    if (existing) {
      return res.status(409).json({ error: 'File already uploaded, remove first' });
    }
    
    // Start async processing
    const result = await CSVService.processFileAsync(
      req.file.buffer, 
      req.file.originalname,
      namespace
    );
    
    res.status(202).json({
      message: 'CSV file upload started',
      jobId: result.jobId,
      fileName: result.fileName,
      namespace: result.namespace
    });
  } catch (error) {
    logger.error('Error in CSV upload:', error);
    next(error);
  }
});

router.get('/list', async (req, res, next) => {
  try {
    // scripts/deno-ui/app/csv.routes.js GET /list Listing CSV files
    console.log('csv.routes.js GET /list Listing CSV files', {data: {}});
    
    const result = await CSVService.listCsvFiles();
    if (!result || !result.files) {
      return res.status(404).json({ error: 'No files found' });
    }
    res.json(result);
  } catch (error) {
    logger.error('Error listing CSV files:', error);
    next(error);
  }
});

// Get all available namespaces
router.get('/namespaces', async (req, res, next) => {
  try {
    // scripts/deno-ui/app/csv.routes.js GET /namespaces Retrieving available namespaces
    console.log('csv.routes.js GET /namespaces Retrieving available namespaces', {data: {}});
    
    const namespaces = await CSVService.getNamespaces();
    res.json({ namespaces });
  } catch (error) {
    logger.error('Error retrieving namespaces:', { 
      message: error.message,
      stack: error.stack 
    });
    next(error);
  }
});

router.put('/update/:id', async (req, res, next) => {
  try {
    const { id } = req.params;
    const updatedData = req.body;
    const updatedRecord = await CSVService.updateCsvRecord(id, updatedData);
    if (!updatedRecord) {
      return res.status(404).json({ error: 'Record not found' });
    }
    res.json(updatedRecord);
  } catch (error) {
    logger.error('Error updating CSV record:', error);
    next(error);
  }
});

// Delete entire file
router.delete('/file/:fileName', async (req, res, next) => {
  try {
    const { fileName } = req.params;
    if (!fileName) {
      return res.status(400).json({ 
        success: false,
        error: 'File name is required' 
      });
    }

    const result = await CSVService.deleteFile(fileName);
    res.json(result);
  } catch (error) {
    logger.error('Error deleting CSV file:', { 
      fileName: req.params.fileName,
      error: error.message,
      stack: error.stack
    });
    
    res.status(500).json({ 
      success: false,
      error: error.message || 'Internal server error while deleting file'
    });
  }
});


export const csvRoutes = router;
</file>

<file path="src/services/embedding.service.js">
import { getOpenAIEmbedding } from '../config/openai.js';
import { initPinecone } from '../config/pinecone.js';
import { logger } from '../utils/logger.js';
import { Document } from '../models/document.model.js';

// Get batch configuration from environment variables
const BATCH_SIZE = ()=>parseInt(process.env.PINECONE_BATCH_SIZE || '100', 10);
const BATCH_DELAY = ()=>parseInt(process.env.PINECONE_BATCH_DELAY || '100', 10);
const EMBEDDING_BATCH_SIZE = ()=>parseInt(process.env.EMBEDDING_BATCH_SIZE || '20', 10);

// Utility function to chunk array into batches
function chunkArray(array, size) {
  const chunks = [];
  for (let i = 0; i < array.length; i += size) {
    chunks.push(array.slice(i, i + size));
  }
  return chunks;
}

// Utility function to add delay between operations
const delay = ms => new Promise(resolve => setTimeout(resolve, ms));


async function embedDocument(code, metadata_small) {
  try {

      console.debug('Embedding document:', {
        model: process.env.EMBEDDING_OPENAI_MODEL||"text-embedding-ada-002"
      });

      const openai = getOpenAIEmbedding();
      const response = await openai.embeddings.create({
          input: `${code}\n${metadata_small}`,
          model: process.env.EMBEDDING_OPENAI_MODEL||"text-embedding-ada-002"
      });
      
      if (!response || !response.data || !response.data[0]) {
          logger.error('Invalid embedding response from OpenAI:', response);
          return null;
      }
      
      return response.data[0].embedding;
  } catch (error) {
      logger.error(`Error generating embedding for ${code}:`, {
          message: error.message,
          stack: error.stack
      });
      return null;
  }
}

async function generateEmbeddingBatch(records, openai) {
  const embeddingPromises = records.map(async (record) => {
    try {
      // Only use code and metadata_small for embeddings
      const text = `${record.code}\n${record.metadata_small}`;
      
      console.debug('Embedding document:', {
        model: process.env.EMBEDDING_OPENAI_MODEL||"text-embedding-ada-002"
      });

      const embedding = await openai.embeddings.create({
        input: text,
        model: process.env.EMBEDDING_OPENAI_MODEL||"text-embedding-ada-002"
      });

      return {
        record,
        embedding: {
          id: record.code,
          values: embedding.data[0].embedding,
          metadata: { 
            code: record.code,
            metadata_small: record.metadata_small
          }
        }
      };
    } catch (error) {
      logger.error('Error generating embedding for record:', { 
        code: record.code,
        error: error.message 
      });
      // Return null for failed embeddings, will be filtered out later
      return null;
    }
  });

  const results = await Promise.all(embeddingPromises);
  return results.filter(result => result !== null);
}

async function saveBatchToStorage(embeddings, pineconeIndex) {
  // src/services/embedding.service.js saveBatchToStorage Called
  console.log('src/services/embedding.service.js saveBatchToStorage Called', { embeddingsCount: embeddings.length });
  try {
    // Save records to MongoDB
    const documents = embeddings.map(item => item.record);
    console.log('src/services/embedding.service.js saveBatchToStorage Before insertMany', { documentsCount: documents.length });
    await Document.insertMany(documents);
    console.log('src/services/embedding.service.js saveBatchToStorage After insertMany', { documentsCount: documents.length });
    logger.info('Saved batch to MongoDB', { count: documents.length });

    // Save embeddings to Pinecone
    const vectors = embeddings.map(item => item.embedding);
    if (vectors.length > 0) {
      console.log('src/services/embedding.service.js saveBatchToStorage Before pineconeIndex.upsert', { vectorsCount: vectors.length });
      await pineconeIndex.upsert({
        vectors: vectors
      });
      console.log('src/services/embedding.service.js saveBatchToStorage After pineconeIndex.upsert', { vectorsCount: vectors.length });
      logger.info('Saved batch to Pinecone', { count: vectors.length });
    }
  } catch (err) {
    console.log('src/services/embedding.service.js saveBatchToStorage Error', { message: err.message, stack: err.stack, axiosData: err?.response?.data });
    logger.error('Error saving batch:', err);
    throw err;
  }
}

async function generateEmbeddings(records) {
  // src/services/embedding.service.js generateEmbeddings Starting embedding generation
  console.log('src/services/embedding.service.js generateEmbeddings Starting embedding generation', { totalRecords: records.length });
  try {
    const pineconeIndex = await initPinecone();
    const openai = getOpenAIEmbedding();
    // src/services/embedding.service.js generateEmbeddings Pinecone and OpenAI clients initialized
    console.log('src/services/embedding.service.js generateEmbeddings Pinecone and OpenAI clients initialized', {});
    // Split records into smaller batches for embedding generation
    const recordBatches = chunkArray(records, EMBEDDING_BATCH_SIZE());
    let processedCount = 0;
    let successCount = 0;
    let errorCount = 0;
    
    logger.info('Processing embeddings in batches', { 
      totalRecords: records.length,
      batchCount: recordBatches.length,
      batchSize: EMBEDDING_BATCH_SIZE()
    });

    // Process each batch and save immediately
    for (const [index, batch] of recordBatches.entries()) {
      try {
        // src/services/embedding.service.js generateEmbeddings Processing batch
        console.log('src/services/embedding.service.js generateEmbeddings Processing batch', { batchIndex: index+1, batchSize: batch.length });
        logger.info(`Processing batch ${index + 1}/${recordBatches.length}`);
        
        // Generate embeddings for the batch
        console.log('src/services/embedding.service.js generateEmbeddings Before generateEmbeddingBatch', { batchIndex: index+1 });
        const batchResults = await generateEmbeddingBatch(batch, openai);
        console.log('src/services/embedding.service.js generateEmbeddings After generateEmbeddingBatch', { batchIndex: index+1, batchResultsCount: batchResults.length });
        
        // Save successful embeddings immediately
        if (batchResults.length > 0) {
          await saveBatchToStorage(batchResults, pineconeIndex);
          successCount += batchResults.length;
        }
        
        errorCount += batch.length - batchResults.length;
        processedCount += batch.length;
        
        // Log progress
        logger.info('Batch processing progress', {
          batch: index + 1,
          totalBatches: recordBatches.length,
          processedCount,
          successCount,
          errorCount,
          progress: `${Math.round((processedCount / records.length) * 100)}%`
        });
        
        // Add a small delay between batches to avoid rate limits
        if (index < recordBatches.length - 1) {
          await delay(BATCH_DELAY());
        }
      } catch (err) {
        console.log('src/services/embedding.service.js generateEmbeddings Error in batch', { batchIndex: index+1, message: err.message, stack: err.stack, axiosData: err?.response?.data });
        logger.error(`Error processing batch ${index + 1}:`, err);
        errorCount += batch.length;
        processedCount += batch.length;
        // Continue with next batch even if this one failed
      }
    }

    logger.info('Embedding generation completed', {
      totalProcessed: processedCount,
      successful: successCount,
      failed: errorCount
    });

    return {
      totalProcessed: processedCount,
      successful: successCount,
      failed: errorCount
    };
  } catch (err) {
    console.log('src/services/embedding.service.js generateEmbeddings Fatal error', { message: err.message, stack: err.stack, axiosData: err?.response?.data });
    logger.error('Error in generateEmbeddings:', err);
    throw err;
  }
}

async function deleteVectors(codes) {
  try {
    if (!codes || codes.length === 0) {
      logger.warn('No codes provided for vector deletion');
      return;
    }

    const pineconeIndex = await initPinecone();
    
    // Delete in batches to avoid overwhelming Pinecone
    const batches = chunkArray(codes, BATCH_SIZE());
    
    logger.info('Starting vector deletion', { 
      totalVectors: codes.length,
      batchCount: batches.length 
    });

    for (const batch of batches) {
      try {
        await pineconeIndex.deleteMany({
          ids: batch
        });
        
        logger.info('Deleted vector batch', { 
          batchSize: batch.length 
        });
        
        // Add delay between batches
        await delay(BATCH_DELAY());
      } catch (error) {
        logger.error('Error deleting vector batch:', { 
          error: error.message,
          batch 
        });
        throw error;
      }
    }

    logger.info('Vector deletion completed', { 
      totalVectorsDeleted: codes.length 
    });
  } catch (error) {
    logger.error('Error in deleteVectors:', error);
    throw error;
  }
}

/**
 * Get vector counts for given CSV file names optionally scoped to a Pinecone namespace.
 * @param {string[]} fileNames - Array of CSV file names (metadata.fileName).
 * @param {string} [namespace] - Pinecone namespace to query. If omitted, queries default.
 * @returns {Promise<Map<string, number>>} Map of fileName to vector count.
 */
async function getVectorCountsByFileName(fileNames, namespace) {
  try {
    if (!fileNames || fileNames.length === 0) {
      return new Map();
    }

    const pineconeIndex = await initPinecone();
    const counts = new Map();
    const dim = parseInt(process.env.VECTOR_DIM || '1536', 10);

    for (const fileName of fileNames) {
      try {
        const zeroVector = new Array(dim).fill(0);
        // Query within namespace if provided
        const queryOptions = {
          vector: zeroVector,
          filter: { fileName },
          topK: 10000,
          includeMetadata: false
        };
        let queryResponse;
        if (namespace) {
          queryResponse = await pineconeIndex.namespace(namespace).query(queryOptions);
        } else {
          queryResponse = await pineconeIndex.query(queryOptions);
        }

        counts.set(fileName, queryResponse.matches?.length || 0);
        // Throttle between queries
        await new Promise(res => setTimeout(res, 100));
      } catch (error) {
        logger.error('Error getting vector count for file:', { fileName, namespace, error: error.message });
        counts.set(fileName, 0);
      }
    }

    return counts;
  } catch (error) {
    logger.error('Error getting vector counts:', error);
    throw error;
  }
}

export {
  embedDocument,
  generateEmbeddings,
  deleteVectors,
  getVectorCountsByFileName
};
</file>

<file path="src/server.js">
import dotenv from 'dotenv';

dotenv.config();

import { logger } from './utils/logger.js';
import express from 'express';
import cors from 'cors';

logger.info('Dotenv loaded'); //It doesnt print

//print inside csv.routes is called first
import { csvRoutes } from './routes/csv.routes.js';
import { queryRoutes } from './routes/query.routes.js';
import { logRoutes } from './routes/log.routes.js';
import { errorHandler } from './middleware/error.middleware.js';
import { setupMongoDB } from './config/mongodb.js';
import { initPinecone } from './config/pinecone.js';
import { initOpenAI, initOpenAIEmbedding } from './config/openai.js';

// Validate required environment variables
const requiredEnvVars = [
  'OPENAI_API_KEY',
  'PINECONE_API_KEY',
  'PINECONE_INDEX', //ENVIRONMENT is not required
  'MONGODB_URI',
  'PORT',
  'BACKEND_API_KEY'
];

function validateEnv() {
  const missingVars = requiredEnvVars.filter(varName => !process.env[varName]);
  
  if (missingVars.length > 0) {
    logger.error(`Missing required environment variables: ${missingVars.join(', ')}`);
    process.exit(1);
  }
  
  logger.info('All required environment variables are set');
}

// Validate environment variables before starting the server
validateEnv();


const app = express();
const PORT = process.env.CSVTORAG_PORT||process.env.PORT || 3000;

app.use(cors());
app.use(express.json());

// Bearer token authentication middleware
const authenticateApiKey = (req, res, next) => {
  const authHeader = req.headers.authorization;
  
  if (!authHeader || !authHeader.startsWith('Bearer ')) {
    return res.status(401).json({ error: 'Missing or invalid authorization header' });
  }

  const token = authHeader.split(' ')[1];
  
  if (token !== process.env.BACKEND_API_KEY) {
    console.log('Invalid API key',{
      token,
      expected: process.env.BACKEND_API_KEY
    });
    return res.status(401).json({ error: 'Invalid API key' });
  }

  next();
};

// Apply authentication middleware to all /api routes
app.use('/api', authenticateApiKey);

// Routes
app.use('/api/csv', csvRoutes);
app.use('/api', queryRoutes); // Now includes /completion
app.use('/api/logs', logRoutes);

app.get('/', (req, res) => {
  res.send(`CSV to RAG API running on port ${PORT}`);
});

// Error handling
app.use(errorHandler);

// Initialize databases and start server
async function startServer() {
  try {
    await setupMongoDB();
    await initPinecone();
    await initOpenAI();
    await initOpenAIEmbedding();
    
    app.listen(PORT, () => {
      logger.info(`Server running on port http://localhost:${PORT}`);
    });
  } catch (error) {
    logger.error('Failed to start server:', error);
    process.exit(1);
  }
}

startServer();
</file>

<file path="package.json">
{
  "name": "csv-to-rag",
  "version": "1.0.0",
  "private": true,
  "type": "module",
  "bin": {
    "csvtorag": "src/cli/index.js"
  },
  "scripts": {
    "start:backend": "node src/server.js",
    "start:ui": "deno run --allow-net --allow-env --allow-read --allow-run scripts/deno-ui.js",
    "dev": "run-p dev:backend dev:ui",
    "dev:backend": "nodemon src/server.js",
    "dev:ui": "DEV=true deno run --allow-net --allow-env --allow-read --allow-run --watch scripts/deno-ui.js",
    "test": "jest",
    "clear-db": "node scripts/clear-db.js",
    "desktop:build:bundle": "deno run --allow-read --allow-write scripts/bundle.js",
    "desktop:build": "npm run desktop:build:bundle && deno compile --allow-net --allow-env --allow-read --allow-run --include=./scripts/deno-ui/app-bundle.js --target x86_64-unknown-linux-gnu --output build/csv-to-rag-ui ./scripts/deno-ui.js",
    "health": "node scripts/db-health.js"
  },
  "dependencies": {
    "@pinecone-database/pinecone": "^4.0.0",
    "axios": "^1.8.4",
    "commander": "^13.0.0",
    "cors": "^2.8.5",
    "csv-parse": "^5.5.3",
    "dotenv": "^16.3.1",
    "express": "^4.18.2",
    "express-validator": "^7.0.1",
    "inquirer": "^12.3.0",
    "joi": "^17.13.3",
    "marked": "^11.2.0",
    "mongoose": "^8.0.3",
    "multer": "^1.4.5-lts.1",
    "mysql2": "^3.12.0",
    "npm-run-all": "^4.1.5",
    "openai": "^4.24.1",
    "tree-kill": "^1.2.2",
    "uuid": "^11.0.3",
    "winston": "^3.11.0"
  },
  "devDependencies": {
    "jest": "^29.7.0",
    "nodemon": "^3.0.2"
  }
}
</file>

<file path="src/services/query.service.js">
import { initPinecone } from '../config/pinecone.js';
import { getOpenAI, getOpenAIEmbedding } from '../config/openai.js';
import { Document } from '../models/document.model.js';
import { logger, completionLogger } from '../utils/logger.js';

export class QueryService {
  static async performSimilaritySearch(query, limit = 5, namespace = 'default') {
    try {
      logger.info('Starting similarity search for query:', { query, namespace });
      const pineconeIndex = await initPinecone();
      const openai = getOpenAIEmbedding();
      
      logger.info('Generating query embedding',{
        model: process.env.EMBEDDING_OPENAI_MODEL||"text-embedding-ada-002"
      });
      const queryEmbedding = await openai.embeddings.create({
        input: query,
        model: process.env.EMBEDDING_OPENAI_MODEL||"text-embedding-ada-002"
      });
      logger.info('Query embedding generated successfully');

      logger.info('Performing Pinecone vector search');
      const searchResults = await pineconeIndex.namespace(namespace).query({
        vector: queryEmbedding.data[0].embedding,
        topK: limit,
        includeMetadata: true
      });
      logger.info('Pinecone search completed', { 
        matchCount: searchResults.matches?.length || 0,
        namespace 
      });

      logger.info('Fetching documents from MongoDB');
      const documents = await Document.find({
        code: { $in: searchResults.matches.map(match => match.metadata.code) },
        namespace
      });
      logger.info('MongoDB documents retrieved', { 
        documentCount: documents.length,
        namespace 
      });

      return { searchResults, documents };
    } catch (error) {
      logger.error('Error in similarity search:', error);
      throw error;
    }
  }

  static async generateResponse(query, context) {
    try {
      logger.info('Starting response generation', { 
        contextSize: context?.length || 0 
      });
      
      const openaiInstance = getOpenAI();
      const primaryModel = process.env.CSVTORAG_OPENAI_MODEL||process.env.OPENAI_MODEL || 'google/gemini-2.0-flash-exp:free';
      const fallbackModel = process.env.CSVTORAG_OPENAI_MODEL_FALLBACK||process.env.OPENAI_MODEL_FALLBACK || 'openai/gpt-4o-mini-2024-07-18';
      
      try {
        logger.info('Making completion request to openaiInstance', {
          model: primaryModel,
          timestamp: new Date().toISOString()
        });
        
        const messages = [
          { role: "system", content: process.env.LLM_SYSTEM_PROMPT },
          { role: "user", content: `Context: ${JSON.stringify(context)}\n\nQuery: ${query}` }
        ];

        completionLogger.info('Sending messages to completions:', {
          messages: [
            { role: 'system', content: process.env.LLM_SYSTEM_PROMPT },
            { role: 'user', content: `Context: ${JSON.stringify(context)}\n\nQuery: ${query}` }
          ],
          timestamp: new Date().toISOString()
        });

        const completion = await openaiInstance.chat.completions.create({
          model: primaryModel,
          messages
        });
        
        logger.info('openaiInstance response received', {
          hasChoices: !!completion?.choices,
          choicesLength: completion?.choices?.length
        });

        if (!completion?.choices?.[0]?.message?.content) {
          logger.error('Invalid completion response', { completion });
          throw new Error('Invalid completion response from openaiInstance Code: '+completion?.error?.code);
        }

        return completion.choices[0].message.content;
        
      } catch (error) {
        logger.error('Initial model error:', {
          error: error?.message,
          code: error?.error?.code || error?.code,
          metadata: error?.metadata,
          timestamp: new Date().toISOString()
        });

        // Check for various rate limit and resource exhaustion scenarios
        const isRateLimitError = 
          error?.status === 429 || 
          error?.error?.code === 429 ||
          error?.code === 429 ||
          (error?.metadata?.raw && JSON.parse(error.metadata.raw)?.error?.code === 429) ||
          (error?.message && error.message.includes('429'));

        if (isRateLimitError) {
          logger.warn('Rate limit or resource exhaustion detected - Activating fallback model', {
            primaryModel,
            fallbackModel,
            errorDetails: {
              message: error?.message,
              code: error?.error?.code,
              raw: error?.metadata?.raw,
              provider: error?.metadata?.provider_name
            },
            timestamp: new Date().toISOString()
          });
          
          try {
            logger.info('Attempting fallback model request', {
              model: fallbackModel,
              timestamp: new Date().toISOString()
            });

            const fallbackMessages = [
              { role: "system", content: process.env.LLM_SYSTEM_PROMPT },
              { role: "user", content: `Context: ${JSON.stringify(context)}\n\nQuery: ${query}` }
            ];

            completionLogger.info('Sending messages to completions:', {
              messages: fallbackMessages,
              timestamp: new Date().toISOString()
            });

            const fallbackCompletion = await openaiInstance.chat.completions.create({
              model: fallbackModel,
              messages: fallbackMessages
            });
            
            logger.info('Fallback model response received', {
              hasChoices: !!fallbackCompletion?.choices,
              choicesLength: fallbackCompletion?.choices?.length
            });

            if (!fallbackCompletion?.choices?.[0]?.message?.content) {
              logger.error('Invalid fallback completion response', { fallbackCompletion });
              throw new Error('Invalid completion response from fallback model');
            }

            return fallbackCompletion.choices[0].message.content;
          } catch (error) {
            logger.error('Fallback model error:', {
              error: error?.message,
              code: error?.error?.code,
              metadata: error?.metadata,
              timestamp: new Date().toISOString()
            });
            throw error;
          }
        }
        
        // If it's not a rate limit error, rethrow
        throw error;
      }
    } catch (error) {
      logger.error('Error generating response:', error);
      throw error;
    }
  }
}
</file>

<file path=".env.example">
# Final LLM completion (See onlyContext)
OPENAI_MODEL=gsk_xxxx
OPENAI_MODEL=meta-llama/llama-4-scout-17b-16e-instruct
OPENAI_MODEL_FALLBACK=meta-llama/llama-4-scout-17b-16e-instruct
OPENAI_BASE_URL=https://api.groq.com/openai/v1

LLM_SYSTEM_PROMPT="You are a helpful assistant. Use the provided context to answer questions accurately."

# Embedding
OPENAI_API_KEY=your_openai_api_key
## Priority over EMBEDDING_OPENAI_API_KEY
EMBEDDING_OPENAI_API_KEY=your_openai_api_key

## Chunking related

CSV_DELIMITER=';'


# Persistance
MONGODB_URI=your_mongodb_uri

# Vector DB
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_ENVIRONMENT=your_pinecone_environment
PINECONE_INDEX=your_pinecone_index
PINECONE_HOST=your_pinecone_host
## Batch Processing Configuration
### Number of vectors to process in each batch
PINECONE_BATCH_SIZE=100  
### Delay in milliseconds between batch processing (rate limiting)
PINECONE_BATCH_DELAY=100 

EMBEDDING_OPENAI_MODEL=text-embedding-ada-002-v2
EMBEDDING_OPENAI_BASE_URL=https://api.openai.com/v1

# Server related

PORT=3000

## Basic authentication
UI_USERNAME=admin
UI_PASSWORD=admin

## Backend authentication
BACKEND_API_KEY=secret_api_key
</file>

<file path="scripts/deno-ui/app/files.js">
// All files under /deno-ui/app are compiled and combined into main.js (All functions are available globally)

// Toast utility
function showToast(message, type = 'info', duration = 3000) {
    let toast = document.getElementById('toast-notification');
    if (!toast) {
        toast = document.createElement('div');
        toast.id = 'toast-notification';
        toast.style.position = 'fixed';
        toast.style.top = '24px';
        toast.style.right = '24px';
        toast.style.zIndex = '9999';
        toast.style.minWidth = '220px';
        toast.style.padding = '12px 20px';
        toast.style.borderRadius = '6px';
        toast.style.fontSize = '16px';
        toast.style.boxShadow = '0 2px 8px rgba(0,0,0,0.08)';
        toast.style.transition = 'opacity 0.3s';
        document.body.appendChild(toast);
    }
    toast.textContent = message;
    toast.style.opacity = '1';
    toast.style.backgroundColor = type === 'warn' ? '#fbbf24' : '#4b5563';
    toast.style.color = type === 'warn' ? '#7c4700' : '#fff';
    toast.style.border = type === 'warn' ? '1px solid #f59e42' : 'none';
    toast.style.display = 'block';
    setTimeout(() => {
        toast.style.opacity = '0';
        setTimeout(() => { toast.style.display = 'none'; }, 300);
    }, duration);
}

function getAuthHeaders(contentType = 'application/json') {
    const apiKey = document.getElementById('apiKey').value;
    return {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': contentType
    };
}

async function listFiles() {
    const baseUrl = document.getElementById('baseUrl').value;
    const fileList = document.getElementById('fileList');
    const error = document.getElementById('error');
    
    try {
        error.textContent = '';
        fileList.innerHTML = 'Loading...';
        
        const response = await fetch(`${baseUrl}/api/csv/list`, {
            headers: getAuthHeaders()
        });
        
        if (!response.ok) {
            throw new Error(`HTTP error! status: ${response.status}`);
        }
        
        const data = await response.json();
        
        if (data && data.totalFiles !== undefined && Array.isArray(data.files)) {
            fileList.innerHTML = data.files.length > 0 
                ? data.files.map(file => `
                <div class="card bg-base-100 shadow mb-4 p-4 w-full">
                    <div class="flex flex-col md:flex-row justify-between items-start gap-4">
                        <div class="w-full md:w-3/4 break-words">
                            <strong>${file.fileName}</strong>
                            <span class="ml-2 text-xs bg-blue-100 text-blue-800 px-1 py-0.5 rounded">${file.namespace}</span>
                            <div class="flex flex-wrap items-center gap-2 mt-1">
                                <span>MongoDB: ${file.rowCount} rows</span>
                                <span class="hidden md:inline">|</span>
                                <span>Pinecone: ${file.vectorCount} vectors</span>
                                <span class="px-2 py-0.5 rounded text-sm ${file.isInSync 
                                    ? 'bg-green-100 text-green-800' 
                                    : 'bg-red-100 text-red-800'}"
                                >
                                    ${file.isInSync ? 'In Sync' : 'Out of Sync'}
                                </span>
                            </div>
                            <div class="mt-1">Last Updated: ${new Date(file.lastUpdated).toLocaleString()}</div>
                            <div class="mt-1 overflow-hidden text-ellipsis">Sample Code: ${file.sampleMetadata.code}</div>
                            <div class="mt-1 overflow-hidden text-ellipsis">Sample Metadata: ${file.sampleMetadata.metadata_small}</div>
                        </div>
                        <div class="flex flex-col gap-2 mt-2 md:mt-0">
                            <button 
                                onclick="deleteFile('${file.fileName}')"
                                class="text-white bg-red-600 hover:bg-red-700 px-3 py-1 rounded text-sm"
                            >
                                Delete
                            </button>
                        </div>
                    </div>
                </div>`).join('')
                : '<div class="alert alert-info shadow-lg"><div><span>No files found</span></div></div>';
        } else {
            fileList.innerHTML = '<div class="alert alert-error shadow-lg"><div><span>Invalid response format</span></div></div>';
        }
    } catch (err) {
        error.textContent = `Error: ${err.message}`;
        fileList.innerHTML = '';
    }
}

async function deleteFile(fileName) {
    if (!confirm(`Are you sure you want to delete ${fileName}?`)) {
        return;
    }

    const baseUrl = document.getElementById('baseUrl').value;
    const error = document.getElementById('error');
    
    try {
        error.textContent = '';
        
        const response = await fetch(`${baseUrl}/api/csv/file/${encodeURIComponent(fileName)}`, {
            method: 'DELETE',
            headers: getAuthHeaders()
        });
        
        if (!response.ok) {
            const errorData = await response.json();
            throw new Error(errorData.error || `HTTP error! status: ${response.status}`);
        }
        
        const result = await response.json();
        if (result.success) {
            // Refresh the file list
            await listFiles();
        } else {
            throw new Error(result.error || 'Delete operation failed');
        }
    } catch (err) {
        error.textContent = `Error deleting file: ${err.message}`;
    }
}

async function uploadFile() {
    // Get all UI elements upfront to avoid reference errors
    const baseUrl = document.getElementById('baseUrl').value;
    const fileInput = document.getElementById('csvFile');
    const namespaceInput = document.getElementById('namespace');
    const error = document.getElementById('error');
    const progress = document.getElementById('uploadProgress');
    const progressBar = document.getElementById('uploadProgressBar');
    const status = document.getElementById('uploadStatus');
    
    try {
        // scripts/deno-ui/app/files.js uploadFile Upload attempt
        console.log('files.js uploadFile Upload attempt',{data:{}});
        
        error.textContent = '';
        
        // Validate namespace is provided
        if (!namespaceInput.value || namespaceInput.value.trim() === '') {
            // scripts/deno-ui/app/files.js uploadFile no namespace
            console.log('files.js uploadFile no namespace',{data:{}});
            error.textContent = 'Please specify a namespace for the upload';
            error.style.display = '';
            error.classList.add('bg-red-100');
            setTimeout(() => error.classList.remove('bg-red-100'), 800);
            showToast('Namespace is required', 'warn');
            return;
        }
        
        if (!fileInput.files || fileInput.files.length === 0) {
            // scripts/deno-ui/app/files.js uploadFile no file selected
            console.log('files.js uploadFile no file selected',{data:{}});
            error.textContent = 'Please select a CSV file';
            error.style.display = '';
            error.classList.add('bg-red-100');
            setTimeout(() => error.classList.remove('bg-red-100'), 800);
            showToast('Please select a CSV file', 'warn');
            return;
        }

        const file = fileInput.files[0];

        // Validate file type
        if (!file.name.toLowerCase().endsWith('.csv')) {
            // scripts/deno-ui/app/files.js uploadFile invalid file type
            console.log('files.js uploadFile invalid file type',{data:{fileName: file.name}});
            error.textContent = 'File must have a .csv extension.';
            error.style.display = '';
            error.classList.add('bg-red-100');
            setTimeout(() => error.classList.remove('bg-red-100'), 800);
            showToast('File must have a .csv extension.', 'warn');
            return;
        }
        
        // Validate CSV header by reading the file and checking its structure
        let isValid = false;
        let validationDetails = {};
        
        try {
            // Read the file to check header format
            const text = await file.text();
            const firstLine = text.split(/\r?\n/)[0].trim();
            
            // Detect the delimiter used in the file
            let detectedDelimiter = ',';
            if (firstLine.includes(';')) detectedDelimiter = ';';
            else if (firstLine.includes('\t')) detectedDelimiter = '\t';
            else if (firstLine.includes('|')) detectedDelimiter = '|';
            
            // Get the column names regardless of delimiter
            const columns = firstLine.split(detectedDelimiter).map(col => col.trim());
            
            // Expected column structure (regardless of delimiter)
            const expectedColumns = ['code', 'metadata_small', 'metadata_big_1', 'metadata_big_2'];
            const expectedColumnsWithBig3 = [...expectedColumns, 'metadata_big_3'];
            
            // Check if columns match either expected structure
            isValid = JSON.stringify(columns) === JSON.stringify(expectedColumns) || 
                      JSON.stringify(columns) === JSON.stringify(expectedColumnsWithBig3);
            
            if (!isValid) {
                // Store validation details for logging if invalid
                validationDetails = {
                    foundHeader: firstLine,
                    foundColumns: columns,
                    expectedColumns: [expectedColumns, expectedColumnsWithBig3],
                    detectedDelimiter: detectedDelimiter === ';' ? 'semicolon (;)' : 
                                      detectedDelimiter === '\t' ? 'tab' : 
                                      detectedDelimiter === '|' ? 'pipe (|)' : 'comma (,)',
                    possibleIssue: columns.includes('code') ? 
                                  'wrong column structure' : 
                                  'missing required columns'
                };
            }
        } catch (err) {
            console.log('files.js uploadFile CSV validation error',{message:err.message,stack:err.stack});
            isValid = false;
            validationDetails = { possibleIssue: 'Error reading file: ' + err.message };
        }
        
        if (!isValid) {
            // scripts/deno-ui/app/files.js uploadFile invalid csv header
            // scripts/deno-ui/app/files.js uploadFile invalid csv header with details
            console.log('files.js uploadFile invalid csv header', {
                data: {
                    fileName: file.name,
                    foundHeader: validationDetails.foundHeader || 'unknown',
                    foundColumns: validationDetails.foundColumns || [],
                    expectedColumns: validationDetails.expectedColumns || [['code', 'metadata_small', 'metadata_big_1', 'metadata_big_2'], ['code', 'metadata_small', 'metadata_big_1', 'metadata_big_2', 'metadata_big_3']],
                    detectedDelimiter: validationDetails.detectedDelimiter || 'unknown',
                    possibleIssue: validationDetails.possibleIssue || 'unknown format'
                }
            });
            error.textContent = `CSV does not match required format (${validationDetails.detectedDelimiter} delimiter detected). See documentation.`;
            error.style.display = '';
            error.classList.add('bg-red-100');
            setTimeout(() => error.classList.remove('bg-red-100'), 800);
            showToast('CSV does not match required format. See documentation.', 'warn');
            return;
        }
        const formData = new FormData();
        // Use 'csvFile' as the field name to match backend expectation
        formData.append('csvFile', file);
        formData.append('delimiter', document.getElementById('delimiter').value);
        formData.append('namespace', namespaceInput.value.trim());
        
        // scripts/deno-ui/app/files.js uploadFile form data prepared
        console.log('files.js uploadFile form data prepared',{data:{fileName: file.name, namespace: namespaceInput.value.trim()}});

        progress.classList.remove('hidden');
        status.textContent = 'Uploading...';
        progressBar.style.width = '0%';

        const response = await fetch(`${baseUrl}/api/csv/upload`, {
            method: 'POST',
            headers: {
                'Authorization': getAuthHeaders().Authorization
                // Don't set Content-Type for FormData, browser will set it with boundary
            },
            body: formData
        });

        if (!response.ok) {
            throw new Error(`HTTP error! status: ${response.status}`);
        }

        const result = await response.json();
        progressBar.style.width = '100%';
        status.textContent = 'Upload complete!';
        
        // Refresh the file list
        await listFiles();
        
        // Clear the file input
        fileInput.value = '';
    } catch (err) {
        // scripts/deno-ui/app/files.js uploadFile upload try/catch
        console.log('files.js uploadFile upload try/catch',{message:err.message,stack:err.stack});
        error.textContent = `Error: ${err.message}`;
        status.textContent = 'Upload failed';
        progressBar.style.width = '0%';
    }
}
</file>

<file path="src/services/csv.service.js">
import { parse } from 'csv-parse';
import { Document } from '../models/document.model.js';
import { generateEmbeddings, deleteVectors, getVectorCountsByFileName } from './embedding.service.js';
import { logger } from '../utils/logger.js';
import mongoose from 'mongoose';

export class CSVService {
  static async processCSV(fileBuffer, fileName, namespace = 'default') {
    const startTime = Date.now();
    logger.info('Starting CSV processing', { fileSize: fileBuffer.length, fileName, namespace });

    // Convert buffer to string and normalize line endings
    const content = fileBuffer.toString('utf-8');
    const lines = content.split('\n');
    
    // Detect and normalize delimiter
    const headerLine = lines[0];
    const firstDataLine = lines[1] || '';
    
    // If header uses different delimiter than data, normalize it
    const headerDelimiter = headerLine.includes(',') ? ',' : ';';
    const dataDelimiter = process.env.CSV_DELIMITER || ';';
    
    // Normalize the header if needed
    const normalizedContent = headerDelimiter !== dataDelimiter
      ? [headerLine.replace(/,/g, dataDelimiter), ...lines.slice(1)].join('\n')
      : content;

    logger.info('CSV Content Preview:', {
      totalLines: lines.length,
      headerLine,
      //firstDataLine,
      headerDelimiter,
      dataDelimiter,
      normalizedHeaderLine: headerLine.replace(/,/g, dataDelimiter)
    });

    return new Promise((resolve, reject) => {
      parse(Buffer.from(normalizedContent), {
        columns: true,
        skip_empty_lines: true,
        delimiter: dataDelimiter,
        quote: false,
        escape: false,
        relax: true,
        trim: true,
        skip_records_with_error: true,
      }, (err, parsedRecords) => {
        if (err) {
          logger.error('CSV parsing error:', err);
          return reject(err);
        }
        let errorCount = 0;
        let recordCount = 0;
        const records = [];
        for (const record of parsedRecords) {
          recordCount++;
          if (recordCount % 100 === 0) {
            logger.info(`Processing record ${recordCount}...`);
          } else {
            logger.debug(`Processing record ${recordCount}...`);
          }
          try {
            const processedRecord = this.validateAndProcessRecord({
              ...record,
              metadata_big_1: String(record.metadata_big_1 || '').replace(/"/g, '\"'),
              metadata_big_2: String(record.metadata_big_2 || '').replace(/"/g, '\"'),
              metadata_big_3: String(record.metadata_big_3 || '').replace(/"/g, '\"')
            }, fileName, namespace);
            if (processedRecord) {
              records.push(processedRecord);
            } else {
              errorCount++;
              logger.warn('Record validation failed:', {
                recordNum: recordCount,
                record: JSON.stringify(record)
              });
            }
          } catch (error) {
            errorCount++;
            logger.error('Error processing record:', {
              recordNum: recordCount,
              error: error.message,
              record: JSON.stringify(record)
            });
          }
        }
        const duration = Date.now() - startTime;
        logger.info('CSV processing completed', {
          fileName,
          namespace,
          totalRecords: recordCount,
          validRecords: records.length,
          errorCount,
          duration: `${duration}ms`
        });
        console.log('src/services/csv.service.js processCSV CSV parsing ended, resolving promise', { fileName, namespace, recordCount, validRecords: records.length });
        if (records.length === 0 && recordCount > 0) {
          logger.error('No valid records found despite having input rows', {
            totalRecords: recordCount,
            errorCount
          });
        }
        resolve(records);
      });
    });

  }

  static async processFileAsync(fileBuffer, fileName, namespace = 'default') {
    const jobId = new mongoose.Types.ObjectId().toString();
    logger.info('Starting async CSV processing', { jobId, fileName, namespace, fileSize: fileBuffer.length });

    // Start processing in the background
    logger.info('Enqueuing background processing', { jobId, fileName, namespace });
    this.processInBackground(fileBuffer, fileName, namespace, jobId)
      .then(() => logger.info('Background processing promise resolved', { jobId }))
      .catch(error => {
        logger.error('Background processing failed', { jobId, fileName, namespace, error });
      });

    return {
      jobId,
      fileName,
      namespace,
      message: 'File upload successful. Processing started.',
      estimatedDuration: 'Processing time depends on file size. Check logs for progress.'
    };
  }

  static async cleanupExistingData(fileName, namespace) {
    logger.info('Checking for existing data', { fileName, namespace });
    
    // Find existing documents for this file
    const existingDocs = await Document.find({ fileName, namespace }, { code: 1 });
    
    if (existingDocs.length > 0) {
      logger.info('Found existing documents to clean up', { 
        fileName, 
        namespace, 
        count: existingDocs.length 
      });

      // Get codes for Pinecone cleanup
      const existingCodes = existingDocs.map(doc => doc.code);

      // Delete from MongoDB
      await Document.deleteMany({ fileName, namespace });
      
      // Delete from Pinecone
      await this.deletePineconeVectors(existingCodes);

      logger.info('Cleaned up existing data', { 
        fileName, 
        namespace, 
        deletedCount: existingDocs.length 
      });
    }
  }

  static async processInBackground(fileBuffer, fileName, namespace, jobId) {
    try {
      const startTime = Date.now();
      logger.info('Background processing started', { jobId, fileName, namespace });

      // Clean up existing data first
      logger.debug('About to cleanup existing data', { jobId, fileName, namespace });
      await this.cleanupExistingData(fileName, namespace);
      logger.info('cleanupExistingData completed', { jobId, fileName, namespace });

      // Step 1: Parse CSV
      logger.debug('About to parse CSV file', { jobId, fileName, namespace });
      const records = await this.processCSV(fileBuffer, fileName, namespace);
      console.log('src/services/csv.service.js processInBackground CSV parsing promise resolved', { jobId, fileName, namespace, recordCount: records.length });
      logger.info('CSV parsing and validation completed', { jobId, fileName, namespace, recordCount: records.length });
      if (!records || records.length === 0) {
        throw new Error('No valid records found in CSV');
      }

      logger.info('CSV parsing completed', { 
        jobId,
        fileName,
        namespace,
        recordCount: records.length 
      });

      // Step 2: Generate embeddings and save to Pinecone/MongoDB
      logger.debug('About to generate embeddings', { jobId, fileName, namespace });
      console.log('src/services/csv.service.js processInBackground Before generateEmbeddings', { jobId, fileName, namespace, recordCount: records.length });
      const embeddingResult = await generateEmbeddings(records, namespace);
      console.log('src/services/csv.service.js processInBackground After generateEmbeddings', { jobId, fileName, namespace, embeddingResult });
      logger.info('Embedding generation completed', { jobId, fileName, namespace, ...embeddingResult });
      // embeddingResult should include successful & failed counts
      if (!embeddingResult || embeddingResult.successful === 0) {
        throw new Error('No embeddings were successfully generated');
      }
      // For downstream Pinecone upload, regenerate detailed embeddings if needed
      const embeddings = Array.isArray(embeddingResult.embeddings) ? embeddingResult.embeddings : records;

      // Step 3: Save to MongoDB
      logger.info('Saving to MongoDB...', { jobId, fileName, namespace });
      const savedRecords = await Document.insertMany(records.map(record => ({ ...record, namespace })));
      logger.info('MongoDB save completed', { 
        jobId,
        fileName,
        namespace,
        savedCount: savedRecords.length 
      });

      // Step 4: Save to Pinecone
      logger.info('Saving to Pinecone metadata...', { jobId, fileName, namespace });
      const savedVectors = await this.saveToPinecone(embeddings, fileName, namespace);
      logger.info('Pinecone save completed', { jobId, fileName, namespace, vectorCount: Array.isArray(savedVectors) ? savedVectors.length : null });
      
      const duration = Date.now() - startTime;
      logger.info('Background processing completed successfully', {
        jobId,
        fileName,
        namespace,
        duration: `${duration}ms`,
        recordsProcessed: records.length,
        vectorsSaved: Array.isArray(savedVectors) ? savedVectors.length : undefined
      });

    } catch (err) {
      console.log('src/services/csv.service.js processInBackground Error', { jobId, fileName, namespace, message: err.message, stack: err.stack, axiosData: err?.response?.data });
      logger.error('Background processing failed', { 
        jobId,
        fileName,
        namespace,
        error: {
          message: err.message,
          stack: err.stack
        }
      });
      // If we failed after saving to MongoDB but before Pinecone,
      // clean up the MongoDB records
      if (err.message && err.message.includes('Pinecone')) {
        logger.info('Rolling back MongoDB changes...', { jobId, fileName, namespace });
        await Document.deleteMany({ fileName, namespace });
      }
      throw err;
    }
  }

  static isBase64(str) {
    try {
      // Check if the string matches base64 pattern
      if (!/^[A-Za-z0-9+/=]+$/.test(str)) return false;
      
      // Try to decode and check if it's valid UTF-8
      const decoded = Buffer.from(str, 'base64').toString('utf-8');
      return true;
    } catch (e) {
      return false;
    }
  }

  static decodeBase64IfNeeded(value) {
    if (!value) return '';
    if (this.isBase64(value)) {
      try {
        return Buffer.from(value, 'base64').toString('utf-8');
      } catch (e) {
        logger.warn('Failed to decode base64 value:', { value, error: e.message });
        return value;
      }
    }
    return value;
  }

  static validateAndProcessRecord(record, fileName, namespace) {
    const { code, metadata_small, metadata_big_1, metadata_big_2, metadata_big_3 } = record;
    
    if (!code || code.trim() === '') {
      logger.warn('Missing or empty code field:', { record });
      return null;
    }

    if (!metadata_small) {
      logger.warn('Missing metadata_small field:', { record });
      return null;
    }

    // Decode base64 metadata if needed
    return {
      fileName,
      namespace,
      code: code.trim(),
      metadata_small: this.decodeBase64IfNeeded(metadata_small),
      metadata_big_1: this.decodeBase64IfNeeded(metadata_big_1),
      metadata_big_2: this.decodeBase64IfNeeded(metadata_big_2),
      metadata_big_3: this.decodeBase64IfNeeded(metadata_big_3)
    };
  }

  static async saveToPinecone(embeddings, fileName, namespace) {
    if (!embeddings || embeddings.length === 0) {
      throw new Error('No embeddings to save to Pinecone');
    }

    // Add fileName and namespace to metadata for each vector
    const vectorsWithMetadata = embeddings.map(embedding => ({
      ...embedding,
      metadata: {
        ...embedding.metadata,
        fileName,
        namespace
      }
    }));

    // Implementation of Pinecone save
    // This should be implemented in the embedding.service.js
    return vectorsWithMetadata;
  }

  static async deletePineconeVectors(codes) {
    // Implementation of Pinecone vector deletion
    logger.info('Deleting vectors from Pinecone', { codes });
    // This should be implemented in the embedding.service.js
  }

  static async cleanupPinecone(jobId, fileName, namespace) {
    // Implement cleanup logic for Pinecone if needed
    logger.info('Cleaning up Pinecone data', { jobId, fileName, namespace });
  }

  static async listCsvFiles() {
    try {
      // Get unique file names, their document counts, and a sample document
      const files = await Document.aggregate([
        {
          $sort: { timestamp: -1 }  // Sort by newest first
        },
        {
          $group: {
            _id: { fileName: '$fileName', namespace: '$namespace' },
            rowCount: { $sum: 1 },
            lastUpdated: { $max: '$timestamp' },  // Use timestamp field
            sampleDoc: { $first: '$$ROOT' }  // Get the most recent document as sample
          }
        },
        {
          $project: {
            fileName: '$_id.fileName',
            namespace: '$_id.namespace',
            rowCount: 1,
            lastUpdated: 1,
            sampleMetadata: {
              code: '$sampleDoc.code',
              metadata_small: '$sampleDoc.metadata_small'
            },
            _id: 0
          }
        },
        {
          $sort: { lastUpdated: -1 }
        }
      ]);

      // Compute vector counts per namespace
      const namespaces = [...new Set(files.map(f => f.namespace))];
      const vectorCountsByNs = {};
      for (const ns of namespaces) {
        const fileNamesForNs = files
          .filter(f => f.namespace === ns)
          .map(f => f.fileName);
        vectorCountsByNs[ns] = await getVectorCountsByFileName(fileNamesForNs, ns);
      }

      // Add vector counts and sync status to the response
      const filesWithVectorCounts = files.map(file => {
        const countsMap = vectorCountsByNs[file.namespace] || new Map();
        const vectorCount = countsMap.get(file.fileName) || 0;
        return {
          ...file,
          vectorCount,
          isInSync: vectorCount === file.rowCount
        };
      });

      return {
        totalFiles: files.length,
        files: filesWithVectorCounts
      };
    } catch (error) {
      logger.error('Error in listCsvFiles:', error);
      throw error;
    }
  }

  static async deleteFile(fileName, namespace) {
    try {
      logger.info('Starting file deletion process', { fileName, namespace });

      // Get all document codes for Pinecone cleanup
      const documents = await Document.find({ fileName, namespace }, { code: 1 });
      const codes = documents.map(doc => doc.code);

      // Delete from MongoDB
      const deleteResult = await Document.deleteMany({ fileName, namespace });
      logger.info('Deleted documents from MongoDB', { 
        fileName, 
        namespace, 
        deletedCount: deleteResult.deletedCount 
      });

      // Delete from Pinecone
      if (codes.length > 0) {
        await deleteVectors(codes);
        logger.info('Deleted vectors from Pinecone', { 
          fileName, 
          namespace, 
          vectorCount: codes.length 
        });
      }

      return {
        success: true,
        deletedCount: deleteResult.deletedCount,
        vectorsDeleted: codes.length
      };
    } catch (error) {
      logger.error('Error deleting file:', { 
        fileName, 
        namespace, 
        error: error.message,
        stack: error.stack 
      });
      throw error;
    }
  }

  /**
   * Get a list of all available namespaces in the system
   * @returns {Promise<Array<string>>} Array of unique namespace names
   */
  static async getNamespaces() {
    try {
      // scripts/deno-ui/app/csv.service.js getNamespaces Retrieving available namespaces
      logger.info('Retrieving available namespaces', {data: {}});
      
      // Use MongoDB aggregation to get unique namespaces
      const result = await Document.aggregate([
        // Group by namespace
        { $group: { _id: '$namespace' } },
        // Sort alphabetically
        { $sort: { _id: 1 } },
        // Project to get a clean array
        { $project: { namespace: '$_id', _id: 0 } }
      ]);
      
      // Extract namespace values from result
      const namespaces = result.map(item => item.namespace);
      
      logger.info('Retrieved namespaces', {data: {count: namespaces.length, namespaces}});
      return namespaces;
    } catch (error) {
      logger.error('Error retrieving namespaces:', { 
        error: error.message,
        stack: error.stack 
      });
      throw error;
    }
  }

}
</file>

</files>
